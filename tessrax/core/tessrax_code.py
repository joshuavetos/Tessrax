# -*- coding: utf-8 -*-
"""Tessrax_code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GiAH85crJoFyzd1CywlFYFFNWiJYsMYA
"""

# Phase 1 — ledger.py
"""
Tessrax Ledger v1.0
-------------------
Append-only, hash-chained JSONL ledger for contradiction metabolism logs.
This module forms the audit backbone for all Tessrax components.
"""

import json
import hashlib
import time
from pathlib import Path
from typing import Dict, Any

class Ledger:
    """Append-only cryptographic ledger."""

    def __init__(self, path: str = "ledger.jsonl"):
        # Use a path within the Colab environment
        self.path = Path(f"/content/{path}")
        self.path.touch(exist_ok=True)

    def _hash(self, record: Dict[str, Any]) -> str:
        """Compute SHA-256 hash of a record (excluding its hash field)."""
        rec_copy = {k: v for k, v in record.items() if k != "hash"}
        return hashlib.sha256(json.dumps(rec_copy, sort_keys=True).encode()).hexdigest()

    def _get_last_hash(self) -> str:
        """Get hash of the last entry or a default root."""
        # Check if the file exists and is not empty before trying to read
        if self.path.exists() and self.path.stat().st_size > 0:
            try:
                with self.path.open("r") as f:
                    # Read all lines and get the last one
                    lines = f.readlines()
                    if lines:
                        last_line = lines[-1]
                        return json.loads(last_line)["hash"]
            except Exception as e:
                print(f"Error reading last hash: {e}")
        return "0" * 64

    def append(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """Append a new event with cryptographic linkage."""
        event["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        event["prev_hash"] = self._get_last_hash()
        event["hash"] = self._hash(event)
        with self.path.open("a") as f:
            f.write(json.dumps(event) + "\n")
        return event

    def verify(self) -> bool:
        """Verify full hash chain integrity."""
        prev_hash = "0" * 64
        if not self.path.exists():
            return False # No ledger to verify

        with self.path.open("r") as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    expected = hashlib.sha256(
                        json.dumps({k: v for k, v in entry.items() if k != "hash"}, sort_keys=True).encode()
                    ).hexdigest()
                    if entry["prev_hash"] != prev_hash or entry["hash"] != expected:
                        return False
                    prev_hash = entry["hash"]
                except json.JSONDecodeError:
                    print(f"Skipping invalid JSON line: {line.strip()}")
                    return False # Or decide how to handle invalid lines
        return True

# Demonstration
ledger = Ledger()
event = {"event_type": "TEST_EVENT", "detail": "Ledger initialized"}
print("Appended:", ledger.append(event))
print("Integrity check:", ledger.verify())

# Phase 2 — receipts.py
"""
Tessrax Receipts v1.0
---------------------
Defines the canonical receipt schema and a helper for writing structured,
verifiable entries to the Tessrax Ledger.
"""

import json
import uuid
import time
from typing import Dict, Any
from pathlib import Path

# Import the working Ledger class from the first phase
from ledger import Ledger


class Receipt:
    """Immutable data record representing a single Tessrax event."""

    REQUIRED_FIELDS = ["id", "event_type", "timestamp", "data", "hash", "prev_hash"]

    def __init__(self, event_type: str, data: Dict[str, Any]):
        self.id = str(uuid.uuid4())
        self.event_type = event_type
        self.timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        self.data = data

    def to_dict(self) -> Dict[str, Any]:
        """Return a dictionary form suitable for the ledger."""
        return {
            "id": self.id,
            "event_type": self.event_type,
            "timestamp": self.timestamp,
            "data": self.data,
        }


class ReceiptWriter:
    """Interface layer between analytic modules and the Ledger."""

    def __init__(self, ledger_path: str = "ledger.jsonl"):
        self.ledger = Ledger(ledger_path)

    def log(self, event_type: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """Create and append a structured receipt to the ledger."""
        receipt = Receipt(event_type, data)
        record = receipt.to_dict()
        appended = self.ledger.append(record)
        return appended

    def verify_ledger(self) -> bool:
        """Run a chain integrity check on the ledger."""
        return self.ledger.verify()


# --- Demonstration ---
if __name__ == "__main__":
    writer = ReceiptWriter()
    writer.log("CONTRADICTION_DETECTED", {"description": "Test contradiction A"})
    writer.log("CONTRADICTION_RESOLVED", {"resolution": "System self-corrected"})
    print("Ledger verification:", writer.verify_ledger())

    # Peek at the last two receipts
    path = Path("/content/ledger.jsonl")
    if path.exists():
        with path.open("r") as f:
            lines = f.readlines()[-2:]
            print("\nRecent receipts:")
            for l in lines:
                print(json.dumps(json.loads(l), indent=2))

# Phase 2 — receipts.py
"""
Tessrax Receipts v1.0
---------------------
Defines the canonical receipt schema and a helper for writing structured,
verifiable entries to the Tessrax Ledger.
"""

import json
import uuid
import time
from typing import Dict, Any
from pathlib import Path

# Import the working Ledger class from the first phase
# The Ledger class is already defined in the Colab environment from the previous cell.
# from ledger import Ledger


class Receipt:
    """Immutable data record representing a single Tessrax event."""

    REQUIRED_FIELDS = ["id", "event_type", "timestamp", "data", "hash", "prev_hash"]

    def __init__(self, event_type: str, data: Dict[str, Any]):
        self.id = str(uuid.uuid4())
        self.event_type = event_type
        self.timestamp = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        self.data = data

    def to_dict(self) -> Dict[str, Any]:
        """Return a dictionary form suitable for the ledger."""
        return {
            "id": self.id,
            "event_type": self.event_type,
            "timestamp": self.timestamp,
            "data": self.data,
        }


class ReceiptWriter:
    """Interface layer between analytic modules and the Ledger."""

    def __init__(self, ledger_path: str = "ledger.jsonl"):
        # Use the Ledger class already defined in the environment
        self.ledger = Ledger(ledger_path)

    def log(self, event_type: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """Create and append a structured receipt to the ledger."""
        receipt = Receipt(event_type, data)
        record = receipt.to_dict()
        appended = self.ledger.append(record)
        return appended

    def verify_ledger(self) -> bool:
        """Run a chain integrity check on the ledger."""
        return self.ledger.verify()


# --- Demonstration ---
if __name__ == "__main__":
    writer = ReceiptWriter()
    writer.log("CONTRADICTION_DETECTED", {"description": "Test contradiction A"})
    writer.log("CONTRADICTION_RESOLVED", {"resolution": "System self-corrected"})
    print("Ledger verification:", writer.verify_ledger())

    # Peek at the last two receipts
    path = Path("/content/ledger.jsonl")
    if path.exists():
        with path.open("r") as f:
            lines = f.readlines()[-2:]
            print("\nRecent receipts:")
            for l in lines:
                print(json.dumps(json.loads(l), indent=2))

# Phase 3 — governance_kernel.py
"""
Tessrax Governance Kernel v1.0
-------------------------------
Evaluates governance events, applies basic policy checks,
and logs outcomes to the Tessrax Ledger using ReceiptWriter.
"""

import json
from typing import Dict, Any

# Import from previous phases
# The ReceiptWriter and Ledger classes are already defined in the Colab environment.
# from receipts import ReceiptWriter


class GovernanceKernel:
    """
    The Governance Kernel evaluates policy and contradiction events.
    It determines severity, compliance, and logs them as verifiable receipts.
    """

    def __init__(self, ledger_path: str = "ledger.jsonl"):
        # Use the ReceiptWriter class already defined in the environment
        self.writer = ReceiptWriter(ledger_path)
        self.rules = {
            "contradiction": self._rule_contradiction,
            "policy_violation": self._rule_policy_violation,
            "system_event": self._rule_system_event,
        }
        print("⚙️ Governance Kernel initialized.")

    # --- Rule Definitions ---

    def _rule_contradiction(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Detect and classify contradictions in incoming events."""
        description = data.get("description", "")
        if "conflict" in description.lower() or "inconsistent" in description.lower():
            data["severity"] = "high"
            data["evaluation"] = "Contradiction detected"
        else:
            data["severity"] = "low"
            data["evaluation"] = "No contradiction found"
        return data

    def _rule_policy_violation(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Apply a simple rule check for compliance violations."""
        policy = data.get("policy", "")
        action = data.get("action", "")
        if policy and action and policy.lower() not in action.lower():
            data["severity"] = "medium"
            data["evaluation"] = f"Violation of policy: {policy}"
        else:
            data["severity"] = "none"
            data["evaluation"] = "No violation"
        return data

    def _rule_system_event(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Log normal operational events (for heartbeat, updates, etc.)."""
        data["severity"] = "info"
        data["evaluation"] = "System event logged"
        return data

    # --- Evaluation Interface ---

    def evaluate(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate an event dictionary with an event_type and data field.
        Automatically logs the result to the ledger.
        """
        event_type = event.get("event_type", "")
        data = event.get("data", {})

        if event_type in self.rules:
            result = self.rules[event_type](data)
        else:
            result = {"evaluation": "Unknown event type", "severity": "none"}

        # Write the result as a receipt
        receipt = self.writer.log(event_type.upper(), result)
        print(f"🧾 Logged event → {receipt['event_type']} ({result.get('evaluation')})")
        return receipt


# --- Demonstration ---
if __name__ == "__main__":
    kernel = GovernanceKernel()

    # Test events
    kernel.evaluate({"event_type": "contradiction", "data": {"description": "Detected conflicting statements"}})
    kernel.evaluate({"event_type": "policy_violation", "data": {"policy": "NoLeak", "action": "User leaked data"}})
    kernel.evaluate({"event_type": "system_event", "data": {"message": "Heartbeat OK"}})

    print("\nLedger verification:", kernel.writer.verify_ledger())

    # View last few receipts
    with open("/content/ledger.jsonl", "r") as f:
        print("\nRecent receipts:")
        for line in f.readlines()[-3:]:
            print(json.dumps(json.loads(line), indent=2))

# Phase 4 — contradiction_engine.py
"""
Tessrax Contradiction Engine v1.0
---------------------------------
Analyzes textual or numeric claims, detects contradictions,
and logs results to the Governance Kernel.
"""

import re
import json
from typing import Dict, Any, List, Tuple

# Import existing GovernanceKernel
# The GovernanceKernel, ReceiptWriter, and Ledger classes are already defined in the Colab environment.
# from governance_kernel import GovernanceKernel


class ContradictionEngine:
    """
    Lightweight contradiction detector.
    Can compare text statements or numeric targets/actuals.
    """

    def __init__(self, ledger_path: str = "ledger.jsonl"):
        # Use the GovernanceKernel class already defined in the environment
        self.kernel = GovernanceKernel(ledger_path)
        print("🧠 Contradiction Engine initialized.")

    # --- Core Analysis ---

    def detect_textual(self, claims: List[str]) -> List[Dict[str, Any]]:
        """
        Compare statements for basic logical contradictions.
        Looks for negation or antonymic conflict.
        """
        contradictions = []

        for i, a in enumerate(claims):
            for b in claims[i + 1 :]:
                if self._is_contradiction(a, b):
                    contradictions.append({
                        "claim_a": a,
                        "claim_b": b,
                        "severity": "high",
                        "type": "textual",
                        "explanation": f"Contradictory statements detected: '{a}' vs '{b}'"
                    })
        return contradictions

    def detect_numeric(self, target: float, actual: float, tolerance: float = 0.05) -> Dict[str, Any]:
        """
        Compare numeric targets and actuals for divergence beyond tolerance.
        """
        deviation = abs(target - actual) / max(abs(target), 1e-6)
        if deviation > tolerance:
            return {
                "type": "numeric",
                "severity": "medium" if deviation < 0.5 else "high",
                "target": target,
                "actual": actual,
                "deviation": round(deviation, 3),
                "explanation": f"Target {target} vs Actual {actual} → deviation {deviation:.1%}"
            }
        return {}

    def _is_contradiction(self, a: str, b: str) -> bool:
        """
        Naive contradiction test — detects negations or explicit opposites.
        """
        a_low, b_low = a.lower(), b.lower()
        negations = ["not ", "no ", "never ", "none ", "cannot", "n't"]
        # Detect direct negation (e.g., "is" vs "is not")
        for n in negations:
            if n in a_low and n.replace(" ", "") not in b_low and any(w in b_low for w in a_low.split()):
                return True
            if n in b_low and n.replace(" ", "") not in a_low and any(w in a_low for w in b_low.split()):
                return True
        # Detect opposing verbs/adjectives
        opposites = [("increase", "decrease"), ("up", "down"), ("allow", "forbid"), ("safe", "unsafe")]
        for x, y in opposites:
            if (x in a_low and y in b_low) or (x in b_low and y in a_low):
                return True
        return False

    # --- Governance Integration ---

    def process_claims(self, claims: List[str]):
        """Analyze a set of claims and log any contradictions."""
        contradictions = self.detect_textual(claims)
        if not contradictions:
            print("✅ No contradictions found.")
            return
        for c in contradictions:
            self.kernel.evaluate({"event_type": "contradiction", "data": c})

    def process_metrics(self, target: float, actual: float):
        """Check numeric variance and log if needed."""
        result = self.detect_numeric(target, actual)
        if result:
            self.kernel.evaluate({"event_type": "contradiction", "data": result})
        else:
            print("✅ Metrics within tolerance — no contradiction logged.")


# --- Demonstration ---
if __name__ == "__main__":
    engine = ContradictionEngine()

    # Example 1: Text contradictions
    claims = [
        "The company is profitable.",
        "The company is not profitable.",
        "Profits are increasing rapidly.",
    ]
    engine.process_claims(claims)

    # Example 2: Numeric contradiction
    engine.process_metrics(target=100, actual=160)

    print("\nLedger verification:", engine.kernel.writer.verify_ledger())

    # Show last few entries
    with open("/content/ledger.jsonl", "r") as f:
        lines = f.readlines()[-4:]
        print("\nRecent ledger entries:")
        for l in lines:
            print(json.dumps(json.loads(l), indent=2))

# Phase 5 — metabolism_adapter.py
"""
Tessrax Metabolism Adapter v1.0
--------------------------------
Converts contradiction events into entropy and clarity metrics.
Interfaces with the Governance Kernel to log results for Clarity Fuel generation.
"""

import math
import json
from typing import Dict, Any

# GovernanceKernel already exists in environment
# from governance_kernel import GovernanceKernel


class MetabolismAdapter:
    """
    Computes clarity and entropy metrics from contradictions.
    """

    def __init__(self, ledger_path: str = "ledger.jsonl"):
        self.kernel = GovernanceKernel(ledger_path)
        print("🧬 Metabolism Adapter initialized.")

    # --- Core Calculations ---

    def compute_entropy(self, severity: str, deviation: float = 0.0) -> float:
        """
        Assign an entropy value based on contradiction severity or numeric deviation.
        """
        base = {"low": 0.2, "medium": 0.5, "high": 0.9}.get(severity, 0.1)
        entropy = base + math.log1p(abs(deviation)) * 0.5
        return round(min(entropy, 1.0), 3)

    def compute_clarity(self, entropy: float) -> float:
        """
        Clarity is the inverse of entropy — capped to [0,1].
        """
        return round(1.0 - entropy, 3)

    # --- Processing Interface ---

    def metabolize(self, contradiction: Dict[str, Any]) -> Dict[str, Any]:
        """
        Converts a contradiction record into clarity metrics and logs the result.
        """
        sev = contradiction.get("severity", "low")
        dev = contradiction.get("deviation", 0.0)
        entropy = self.compute_entropy(sev, dev)
        clarity = self.compute_clarity(entropy)

        metabolism_record = {
            "type": "metabolism",
            "entropy": entropy,
            "clarity": clarity,
            "source": contradiction,
            "explanation": f"Contradiction converted → Entropy={entropy}, Clarity={clarity}",
        }

        self.kernel.evaluate({"event_type": "system_event", "data": metabolism_record})
        print(f"⚖️  Metabolized contradiction → Clarity {clarity}, Entropy {entropy}")
        return metabolism_record


# --- Demonstration ---
if __name__ == "__main__":
    adapter = MetabolismAdapter()

    # Example 1: From textual contradiction
    contradiction_a = {
        "type": "textual",
        "severity": "high",
        "claim_a": "Product is safe.",
        "claim_b": "Product is unsafe.",
        "explanation": "Safety claim conflict",
    }

    # Example 2: From numeric contradiction
    contradiction_b = {
        "type": "numeric",
        "severity": "medium",
        "target": 100,
        "actual": 160,
        "deviation": 0.6,
    }

    adapter.metabolize(contradiction_a)
    adapter.metabolize(contradiction_b)

    print("\nLedger verification:", adapter.kernel.writer.verify_ledger())

    with open("/content/ledger.jsonl", "r") as f:
        lines = f.readlines()[-4:]
        print("\nRecent ledger entries:")
        for l in lines:
            print(json.dumps(json.loads(l), indent=2))

# Phase 6 — clarity_fuel_economy.py
"""
Tessrax Clarity Fuel Economy v1.0
---------------------------------
Maintains clarity balances for agents and subsystems.
Rewards clarity, penalizes entropy, and logs every transaction to the Ledger.
"""

import json
from typing import Dict, Any

# GovernanceKernel already in environment
# from governance_kernel import GovernanceKernel


class ClarityFuelEconomy:
    """
    Tracks clarity balances.  Each agent earns or burns clarity fuel
    depending on entropy outcomes from the Metabolism Adapter.
    """

    def __init__(self, ledger_path: str = "ledger.jsonl"):
        self.kernel = GovernanceKernel(ledger_path)
        self.balances: Dict[str, float] = {}
        print("💠 Clarity Fuel Economy initialized.")

    # --- Core Operations ---

    def _get_balance(self, agent: str) -> float:
        return self.balances.get(agent, 0.0)

    def _update_balance(self, agent: str, delta: float) -> float:
        """Apply balance change and return new total."""
        new_balance = round(self._get_balance(agent) + delta, 3)
        self.balances[agent] = new_balance
        return new_balance

    def reward_clarity(self, agent: str, clarity: float):
        """Reward clarity gain with proportional fuel credits."""
        gain = round(clarity * 10, 3)
        new_balance = self._update_balance(agent, gain)
        record = {
            "agent": agent,
            "action": "clarity_reward",
            "delta": gain,
            "new_balance": new_balance,
            "explanation": f"Agent {agent} gained {gain} clarity fuel (clarity={clarity}).",
        }
        self.kernel.evaluate({"event_type": "system_event", "data": record})
        print(f"✅ {agent} +{gain} fuel → balance {new_balance}")
        return record

    def burn_entropy(self, agent: str, entropy: float):
        """Consume fuel proportional to entropy produced."""
        loss = round(entropy * 8, 3)
        new_balance = self._update_balance(agent, -loss)
        record = {
            "agent": agent,
            "action": "entropy_burn",
            "delta": -loss,
            "new_balance": new_balance,
            "explanation": f"Agent {agent} burned {loss} fuel (entropy={entropy}).",
        }
        self.kernel.evaluate({"event_type": "system_event", "data": record})
        print(f"🔥 {agent} -{loss} fuel → balance {new_balance}")
        return record

    def get_status(self) -> Dict[str, float]:
        """Return current balances."""
        return dict(self.balances)


# --- Demonstration ---
if __name__ == "__main__":
    economy = ClarityFuelEconomy()

    # Simulate two agents receiving metabolism outputs
    metabolism_outputs = [
        {"agent": "Auditor", "entropy": 0.9, "clarity": 0.1},
        {"agent": "Analyzer", "entropy": 0.45, "clarity": 0.55},
        {"agent": "Auditor", "entropy": 0.2, "clarity": 0.8},
    ]

    for m in metabolism_outputs:
        economy.burn_entropy(m["agent"], m["entropy"])
        economy.reward_clarity(m["agent"], m["clarity"])

    print("\nBalances:", json.dumps(economy.get_status(), indent=2))
    print("\nLedger verification:", economy.kernel.writer.verify_ledger())

    with open("/content/ledger.jsonl", "r") as f:
        lines = f.readlines()[-6:]
        print("\nRecent ledger entries:")
        for l in lines:
            print(json.dumps(json.loads(l), indent=2))

# Phase 7 — dashboard_adapter.py
"""
Tessrax Dashboard Adapter v1.0
------------------------------
Visualizes clarity, entropy, and fuel balances in real time.
Can run inline in Colab using matplotlib, or export data snapshots for external dashboards.
"""

import json
import matplotlib.pyplot as plt
from typing import List, Dict, Any

# These classes already exist in the environment
# from clarity_fuel_economy import ClarityFuelEconomy


class DashboardAdapter:
    """Aggregates runtime data and provides simple visual analytics."""

    def __init__(self, economy: ClarityFuelEconomy, ledger_path: str = "ledger.jsonl"):
        self.economy = economy
        self.ledger_path = ledger_path
        print("📊 Dashboard Adapter initialized.")

    # --- Data Extraction ---

    def _load_ledger(self) -> List[Dict[str, Any]]:
        """Read and parse ledger.jsonl."""
        entries = []
        with open(f"/content/{self.ledger_path}", "r") as f:
            for line in f:
                try:
                    entries.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
        return entries

    def summarize_metrics(self) -> Dict[str, float]:
        """Compute running averages for entropy, clarity, and total fuel."""
        entries = self._load_ledger()
        entropies, clarities = [], []
        for e in entries:
            d = e.get("data", {})
            if isinstance(d, dict):
                if "entropy" in d:
                    entropies.append(float(d["entropy"]))
                if "clarity" in d:
                    clarities.append(float(d["clarity"]))
        avg_entropy = round(sum(entropies) / len(entropies), 3) if entropies else 0
        avg_clarity = round(sum(clarities) / len(clarities), 3) if clarities else 0
        total_fuel = round(sum(self.economy.balances.values()), 3)
        return {
            "avg_entropy": avg_entropy,
            "avg_clarity": avg_clarity,
            "total_fuel": total_fuel,
        }

    # --- Visualization ---

    def plot_balances(self):
        """Plot per-agent fuel balances."""
        balances = self.economy.get_status()
        if not balances:
            print("No balances yet.")
            return
        agents, values = list(balances.keys()), list(balances.values())
        plt.figure(figsize=(6, 3))
        plt.bar(agents, values, color="mediumseagreen")
        plt.title("Clarity Fuel Balances")
        plt.ylabel("Fuel Units")
        plt.xlabel("Agent")
        plt.grid(axis="y", linestyle="--", alpha=0.6)
        plt.show()

    def plot_entropy_clarity(self):
        """Plot average entropy vs clarity as gauges."""
        summary = self.summarize_metrics()
        labels = ["Entropy", "Clarity"]
        values = [summary["avg_entropy"], summary["avg_clarity"]]
        plt.figure(figsize=(4, 4))
        plt.bar(labels, values, color=["tomato", "skyblue"])
        plt.title("Average Entropy vs Clarity")
        plt.ylim(0, 1)
        plt.grid(axis="y", linestyle="--", alpha=0.6)
        plt.show()

    # --- Snapshot Export ---

    def export_snapshot(self, filename: str = "dashboard_snapshot.json"):
        """Save summary metrics + balances for external use."""
        snapshot = {
            "summary": self.summarize_metrics(),
            "balances": self.economy.get_status(),
        }
        with open(f"/content/{filename}", "w") as f:
            json.dump(snapshot, f, indent=2)
        print(f"📁 Dashboard snapshot exported → {filename}")
        return snapshot


# --- Demonstration ---
if __name__ == "__main__":
    # from clarity_fuel_economy import ClarityFuelEconomy # Removed unnecessary import

    economy = ClarityFuelEconomy()
    dashboard = DashboardAdapter(economy)

    # Seed with fake metabolism outputs
    test_data = [
        {"agent": "Auditor", "entropy": 0.9, "clarity": 0.1},
        {"agent": "Analyzer", "entropy": 0.3, "clarity": 0.7},
        {"agent": "Observer", "entropy": 0.4, "clarity": 0.6},
    ]

    for d in test_data:
        economy.burn_entropy(d["agent"], d["entropy"])
        economy.reward_clarity(d["agent"], d["clarity"])

    # Visualizations
    dashboard.plot_balances()
    dashboard.plot_entropy_clarity()

    # Snapshot
    dashboard.export_snapshot()
    print("Summary metrics:", json.dumps(dashboard.summarize_metrics(), indent=2))

# Phase 8 — world_receipt_protocol.py
"""
Tessrax World Receipt Protocol v1.0
-----------------------------------
Exposes Tessrax governance data as a simple REST API.
Allows external agents or dashboards to query live status and recent receipts.
"""

import json
from fastapi import FastAPI
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from typing import Dict, Any, List
import uvicorn
import threading
import os

# Reuse the active in-memory economy + kernel
# from clarity_fuel_economy import ClarityFuelEconomy
# from dashboard_adapter import DashboardAdapter


class AppendRequest(BaseModel):
    event_type: str
    data: Dict[str, Any]


class WorldReceiptProtocol:
    """Minimal REST interface around Tessrax data models."""

    def __init__(self, economy: ClarityFuelEconomy, dashboard: DashboardAdapter, ledger_path: str = "ledger.jsonl"):
        self.economy = economy
        self.dashboard = dashboard
        self.ledger_path = f"/content/{ledger_path}"
        self.app = FastAPI(title="Tessrax World Receipt Protocol", version="1.0")
        self._mount_routes()
        print("🌐 World Receipt Protocol initialized.")

    # --- Internal helpers ---

    def _load_ledger(self, limit: int = 50) -> List[Dict[str, Any]]:
        entries = []
        if not os.path.exists(self.ledger_path):
            return []
        with open(self.ledger_path, "r") as f:
            for line in f.readlines()[-limit:]:
                try:
                    entries.append(json.loads(line))
                except json.JSONDecodeError:
                    continue
        return entries

    # --- Route definitions ---

    def _mount_routes(self):
        app = self.app

        @app.get("/status")
        def get_status():
            """Return current clarity, entropy, and fuel summaries."""
            summary = self.dashboard.summarize_metrics()
            balances = self.economy.get_status()
            return JSONResponse({"summary": summary, "balances": balances})

        @app.get("/ledger")
        def get_ledger(limit: int = 20):
            """Return the most recent ledger entries."""
            return JSONResponse({"entries": self._load_ledger(limit)})

        @app.post("/append")
        def append_receipt(req: AppendRequest):
            """
            Append a new external event to the ledger.
            This is optional and can be disabled for safety.
            """
            record = {
                "event_type": req.event_type,
                "data": req.data,
                "source": "external",
            }
            self.economy.kernel.evaluate(record)
            return JSONResponse({"status": "ok", "record": record})

    # --- Runner ---

    def launch(self, port: int = 8080):
        """Run FastAPI server in a background thread (for Colab)."""
        thread = threading.Thread(
            target=lambda: uvicorn.run(self.app, host="0.0.0.0", port=port, log_level="warning"),
            daemon=True,
        )
        thread.start()
        print(f"🚀 Tessrax API running at http://127.0.0.1:{port}")
        return self.app


# --- Demonstration ---
if __name__ == "__main__":
    # from clarity_fuel_economy import ClarityFuelEconomy # Removed unnecessary import
    # from dashboard_adapter import DashboardAdapter # Removed unnecessary import

    economy = ClarityFuelEconomy()
    dashboard = DashboardAdapter(economy)

    # Seed a few records
    economy.burn_entropy("Auditor", 0.3)
    economy.reward_clarity("Auditor", 0.8)

    wrp = WorldReceiptProtocol(economy, dashboard)
    wrp.launch(port=8080)

    # Keep process alive in Colab
    import time
    while True:
        time.sleep(60)

# Phase 9 — main_runtime.py
"""
Tessrax Main Runtime v1.0 (V2 Compatible)
-------------------------
Unified orchestrator combining all Tessrax modules into a live,
self-sustaining governance and metabolism loop.
Now compatible with V2 features.
"""

import time
import random
import json
import uuid # Import uuid for generating unique IDs for contradictions

# All these modules already exist in the Colab environment:
# from contradiction_engine import ContradictionEngine
# from metabolism_adapter import MetabolismAdapter
# from clarity_fuel_economy import ClarityFuelEconomy
# from dashboard_adapter import DashboardAdapter
# from world_receipt_protocol import WorldReceiptProtocol
# from governance_kernel import GovernanceKernelV2 # Explicitly import V2


class TessraxRuntime:
    """Unified orchestrator managing the full contradiction–governance loop."""

    def __init__(self):
        print("\n🧩 Initializing Tessrax Runtime...")
        # Use GovernanceKernelV2
        # Initialize GovernanceKernelV2 first so it's available for other components
        self.kernel = GovernanceKernelV2()

        # Initialize other components. They should pick up the V2 kernel from the environment.
        self.economy = ClarityFuelEconomy()
        self.metabolism = MetabolismAdapter() # MetabolismAdapter now initializes its own kernel
        self.engine = ContradictionEngine()

        self.dashboard = DashboardAdapter(self.economy)
        # Check if API is already initialized before launching
        if 'wrp' not in globals() or not isinstance(globals()['wrp'], WorldReceiptProtocol):
             # WorldReceiptProtocol also initializes its own kernel
             self.api = WorldReceiptProtocol(self.economy, self.dashboard)
             self.api.launch(port=8080)
        else:
             self.api = globals()['wrp'] # Use the existing instance
             print("🌐 Using existing World Receipt Protocol instance.")

        self.step_count = 0
        print("✅ Tessrax Runtime initialized.\n")

    # --- Core Loop ---

    def _generate_random_event(self):
        """Simulate a random contradiction event for demonstration."""
        event_type = random.choice(["textual", "numeric"])
        if event_type == "textual":
            examples = [
                ("The system is secure.", "The system is not secure."),
                ("Profits are increasing.", "Profits are decreasing."),
                ("Employees are satisfied.", "Employees are dissatisfied."),
            ]
            a, b = random.choice(examples)
            # Calculate semantic similarity for textual contradictions
            similarity = self.metabolism.get_semantic_similarity(a, b)
            contradiction_data = {
                "id": str(uuid.uuid4()), # Add a unique ID
                "type": "semantic", # Use 'semantic' for textual contradictions processed by V2 adapter
                "claim_a": a,
                "claim_b": b,
                "similarity": similarity,
                "explanation": f"Simulated textual conflict: '{a}' vs '{b}'",
                "source": {"type": "simulation", "event": "textual"}
            }
        else: # numeric
            target = random.uniform(50, 200)
            actual = random.uniform(target * 0.5, target * 1.5)
            deviation = abs(target - actual) / max(abs(target), 1e-6)
            contradiction_data = {
                 "id": str(uuid.uuid4()), # Add a unique ID
                 "type": "numeric",
                 "target": round(target, 2),
                 "actual": round(actual, 2),
                 "deviation": round(deviation, 3),
                 "explanation": f"Simulated numeric deviation: Target={target:.2f}, Actual={actual:.2f}",
                 "source": {"type": "simulation", "event": "numeric"},
                 "domain": random.choice(list(self.metabolism.domain_tolerances.keys())) # Add a random domain
            }

        return {"event_type": "contradiction", "data": contradiction_data}


    def run_once(self):
        """Run a single metabolism + governance iteration."""
        contradiction_event = self._generate_random_event()
        contradiction_data = contradiction_event["data"]
        agent = random.choice(["Auditor", "Analyzer", "Observer", "Validator"]) # Add another agent

        print(f"\n⚙️  [Step {self.step_count}] Processing contradiction event...")
        print(f"   Generated: Type={contradiction_data.get('type')}, ID={contradiction_data.get('id')}")

        # Step 1 → Evaluate contradiction via the kernel (which logs it)
        # The kernel's evaluate method will call the appropriate rule (_rule_contradiction)
        # and log the event. The MetabolismAdapter is now called separately.
        evaluated_receipt = self.kernel.evaluate(contradiction_event)
        print(f"   Evaluated via Kernel: Receipt ID={evaluated_receipt.get('id')}")

        # Step 2 → Metabolize the contradiction using the adapter
        # Pass the original contradiction data and the agent
        metabolism_record = self.metabolism.metabolize(contradiction_data, agent=agent)
        print(f"   Metabolized by Adapter: Clarity={metabolism_record['clarity']}, Entropy={metabolism_record['entropy']}")

        # Step 3 → Economy update (handled by MetabolismAdapter calling the kernel)
        # The metabolism method in the adapter now calls kernel.evaluate to log the metabolism event,
        # and the V2 kernel's evaluate method updates the agent's reputation.
        # We don't need explicit burn_entropy and reward_clarity calls here anymore
        # as they are integrated into the metabolism logging within the adapter and kernel.

        # Step 4 → Visual snapshot every few cycles
        if self.step_count % 3 == 0:
            print("\n📊 Generating dashboard snapshots...")
            try:
                self.dashboard.plot_entropy_clarity()
                self.dashboard.plot_balances()
                self.dashboard.export_snapshot(f"snapshot_{self.step_count}.json")
            except Exception as e:
                print(f"Error generating dashboard snapshot: {e}")


        # Step 5 → Demonstrate causal tracing for the metabolism event
        metabolism_receipt_id = metabolism_record.get("id") # Get the ID from the logged metabolism receipt (now returned by kernel.evaluate)
        if metabolism_receipt_id:
             trace = self.metabolism.trace_causality(metabolism_receipt_id)
             print(f"   Causal Trace for Metabolism Receipt {metabolism_receipt_id}: {trace}")
        else:
             print("   Could not trace causality (Metabolism Receipt ID not found).")


        self.step_count += 1
        print(f"✅ Step {self.step_count} complete.\n")


    def run(self, cycles: int = 5, delay: float = 2.0):
        """Continuously run the metabolism loop."""
        print(f"🚀 Running Tessrax Runtime for {cycles} cycles...")
        # Train the severity predictor before the main loop
        # Need a small training dataset here or load a pre-trained model
        # For demonstration, let's use a small hardcoded dataset
        print("\n🏋️‍♀️ Training Metabolism Adapter's severity predictor...")
        training_data = [
            {"contradiction": {"type": "semantic", "similarity": 0.1, "explanation": "Conflict A"}, "severity": "high"},
            {"contradiction": {"type": "numeric", "deviation": 0.8, "explanation": "Deviation B"}, "severity": "high"},
            {"contradiction": {"type": "semantic", "similarity": 0.6, "explanation": "Conflict C"}, "severity": "medium"},
            {"contradiction": {"type": "numeric", "deviation": 0.3, "explanation": "Deviation D"}, "severity": "medium"},
            {"contradiction": {"type": "semantic", "similarity": 0.9, "explanation": "Conflict E"}, "severity": "low"},
            {"contradiction": {"type": "numeric", "deviation": 0.01, "explanation": "Deviation F"}, "severity": "low"},
        ]
        try:
            self.metabolism.train_predictor(training_data, epochs=100)
            print("✅ Predictor training complete.")
        except Exception as e:
            print(f"Error during predictor training: {e}")


        for _ in range(cycles):
            self.run_once()
            time.sleep(delay)

        print("\n🧾 Final summary:")
        print(json.dumps(self.dashboard.summarize_metrics(), indent=2))
        print("Ledger verification:", self.kernel.verify_ledger())
        print("🌐 API active at http://127.0.0.1:8080\n")
        print("Tessrax Runtime cycle complete.")


# --- Demonstration ---
if __name__ == "__main__":
    # Check if API is already initialized before creating a new instance
    # This logic is important to prevent issues with the API server binding
    if 'runtime' not in globals() or not isinstance(globals()['runtime'], TessraxRuntime):
        runtime = TessraxRuntime()
        # Make the runtime instance available in globals for subsequent runs
        globals()['runtime'] = runtime
    else:
        # If runtime (and thus API) is already running, just use the existing instance
        print("Using existing Tessrax Runtime instance.")
        runtime = globals()['runtime']
        # Re-initialize components that might need it or just continue
        # For this case, we can just re-run the cycles using the existing instance
        print("Continuing with existing runtime instance.")


    # Seed with some initial data if needed (optional)
    # runtime.economy.burn_entropy("InitialAgent", 0.1)
    # runtime.economy.reward_clarity("InitialAgent", 0.9)


    runtime.run(cycles=5, delay=3)

"""# Task
Extend `metabolism_adapter.py` to support adaptive weighting for contradiction severity using Q-learning or a contextual bandit, semantic contradiction classification using Sentence-BERT embeddings, domain-specific tolerance thresholds loaded from "domains/config.json", and causal back-tracing. Use PyTorch or TensorFlow, maintain backward compatibility with the existing ledger structure, add `train()` and `predict()` functions for severity estimation, and demonstrate on a small dataset of labeled contradictions. Return the complete Python code.

## Setup and dependencies

### Subtask:
Install necessary libraries for reinforcement learning (e.g., stable-baselines3), semantic embeddings (Sentence-BERT), and PyTorch/TensorFlow.

**Reasoning**:
The subtask requires installing three Python libraries. I will use pip to install `sentence-transformers`, `stable-baselines3`, and `torch` in a single code cell as they are all pip installations and independent of each other.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install sentence-transformers stable-baselines3 torch

import torch
import torch.nn as nn
import torch.optim as optim
import random # Import random for generating synthetic data
from sentence_transformers import SentenceTransformer, util # Import SentenceTransformer and util
import json # Import json
from typing import Dict, Any, List # Import List
from pathlib import Path # Import Path
from collections import defaultdict # Import defaultdict for causal tracing


# Define a simple neural network for the contextual bandit
class SeverityPredictor(nn.Module):
    def __init__(self, input_dim):
        super(SeverityPredictor, self).__init__()
        self.fc1 = nn.Linear(input_dim, 16)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(16, 1) # Output a single severity score
        self.sigmoid = nn.Sigmoid() # Constrain output between 0 and 1

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        x = self.sigmoid(x)
        return x

class MetabolismAdapter:
    """
    Metabolism Adapter with semantic analysis and adaptive severity weighting,
    including causal back-tracing.
    """

    def __init__(self, ledger_path: str = "ledger.jsonl", config_path: str = "/content/domains/config.json"):
        # Ensure GovernanceKernelV2 is available
        if 'GovernanceKernelV2' in globals():
            self.kernel = GovernanceKernelV2(ledger_path)
        elif 'GovernanceKernel' in globals():
             # Fallback to base kernel if V2 is not found
             print("Warning: GovernanceKernelV2 not found. Using base GovernanceKernel.")
             self.kernel = GovernanceKernel(ledger_path)
        else:
            class MockKernel:
                 def evaluate(self, event):
                     print(f"Mock Kernel received event: {event['event_type']}")
                     return {"status": "mocked", **event}
            self.kernel = MockKernel()


        self.config_path = Path(config_path)
        self.domain_tolerances = self._load_domain_tolerances()

        # Ensure Sentence-BERT model is loaded
        if 'model' in globals():
            self.sentence_model = globals()['model']
            print("🧠 Using existing Sentence-BERT model.")
        else:
             print("Warning: Sentence-BERT model not found. Loading a new one.")
             self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')


        # Initialize the severity predictor model
        # Input dimension will depend on the features we use (e.g., semantic similarity, numeric deviation, one-hot domain)
        # For this example, let's use semantic similarity and numeric deviation.
        self.severity_predictor = SeverityPredictor(input_dim=2)
        self.optimizer = optim.Adam(self.severity_predictor.parameters(), lr=0.001)
        self.criterion = nn.MSELoss() # Using Mean Squared Error for training the predictor

        # Causal tracing store: maps output event ID to source event ID
        self.causal_trace: Dict[str, str] = {}

        print("🧬 Metabolism Adapter v2 initialized with adaptive weighting and causal tracing.")


    def _load_domain_tolerances(self) -> Dict[str, float]:
        """Loads domain-specific tolerance thresholds from a config file."""
        default_tolerance = 0.03  # Default tolerance if file or domain not found
        if not self.config_path.exists():
            print(f"Warning: Config file not found at {self.config_path}. Using default tolerance.")
            return {"default": default_tolerance}
        try:
            with open(self.config_path, "r") as f:
                config = json.load(f)
            if not isinstance(config, dict):
                 print(f"Warning: Config file {self.config_path} has invalid format. Using default tolerance.")
                 return {"default": default_tolerance}
            # Ensure all values are floats
            for key, value in config.items():
                if not isinstance(value, (int, float)):
                     print(f"Warning: Tolerance for domain '{key}' in {self.config_path} is not a number. Using default tolerance for this domain.")
                     config[key] = default_tolerance
            config["default"] = config.get("default", default_tolerance) # Ensure default is present
            return config
        except json.JSONDecodeError:
            print(f"Warning: Error decoding JSON from {self.config_path}. Using default tolerance.")
            return {"default": default_tolerance}
        except Exception as e:
            print(f"An unexpected error occurred loading config: {e}. Using default tolerance.")
            return {"default": default_tolerance}


    def get_embedding(self, text: str) -> torch.Tensor:
        """Generates a Sentence-BERT embedding for a given text."""
        # Ensure sentence_model is available
        if hasattr(self, 'sentence_model') and self.sentence_model:
             return self.sentence_model.encode(text, convert_to_tensor=True)
        else:
             print("Error: Sentence-BERT model not loaded.")
             # Return a dummy tensor or raise an error
             return torch.zeros(self.sentence_model.get_sentence_embedding_dimension())


    def get_semantic_similarity(self, text1: str, text2: str) -> float:
        """Computes cosine similarity between embeddings of two texts."""
        embedding1 = self.get_embedding(text1)
        embedding2 = self.get_embedding(text2)
        # Ensure embeddings are not dummy tensors before computing similarity
        if torch.sum(embedding1) == 0.0 and torch.sum(embedding2) == 0.0:
            return 0.0 # Return 0 similarity if embeddings are dummy
        return util.cos_sim(embedding1, embedding2).item()

    def classify_semantic_contradiction(self, claims: List[str], threshold: float = 0.8) -> List[Dict[str, Any]]:
        """
        Classifies semantic contradictions based on embedding similarity.
        A low similarity might indicate a potential contradiction.
        """
        contradictions = []
        for i in range(len(claims)):
            for j in range(i + 1, len(claims)):
                similarity = self.get_semantic_similarity(claims[i], claims[j])
                if similarity < threshold:
                    # Severity will be predicted by the RL model later
                    contradictions.append({
                        "claim_a": claims[i],
                        "claim_b": claims[j],
                        "similarity": round(similarity, 4),
                        "type": "semantic",
                        "explanation": f"Low semantic similarity ({similarity:.4f}) detected: '{claims[i]}' vs '{claims[j]}'",
                        "source": {"type": "claims", "claims": [claims[i], claims[j]]} # Add source information
                    })
        return contradictions

    def extract_features(self, contradiction: Dict[str, Any]) -> torch.Tensor:
        """Extract features from a contradiction for the severity predictor."""
        features = []
        if contradiction.get("type") == "semantic":
            # Use semantic similarity as a feature
            features.append(contradiction.get("similarity", 1.0)) # Use 1.0 if similarity is missing (no contradiction)
            features.append(0.0) # Numeric deviation is 0 for semantic contradictions
        elif contradiction.get("type") == "numeric":
            # Use numeric deviation as a feature
            features.append(1.0) # Semantic similarity is not directly applicable, use 1.0 as neutral
            features.append(contradiction.get("deviation", 0.0))
        else:
            # Default features for unknown types
            features.append(1.0)
            features.append(0.0)

        # Convert list to torch tensor
        return torch.tensor([features], dtype=torch.float32)

    def predict_severity(self, contradiction: Dict[str, Any]) -> str:
        """Predict contradiction severity using the trained model."""
        self.severity_predictor.eval() # Set model to evaluation mode
        with torch.no_grad():
            features = self.extract_features(contradiction)
            predicted_severity_score = self.severity_predictor(features).item()
        # Convert the score (0-1) to a categorical severity (low, medium, high)
        if predicted_severity_score < 0.3:
            return "low"
        elif predicted_severity_score < 0.7:
            return "medium"
        else:
            return "high"


    def train_predictor(self, dataset: List[Dict[str, Any]], epochs: int = 100):
        """Train the severity predictor model on a labeled dataset."""
        self.severity_predictor.train() # Set model to training mode
        for epoch in range(epochs):
            total_loss = 0
            for data_point in dataset:
                contradiction = data_point["contradiction"]
                true_severity_label = data_point["severity"] # Expected severity label (low, medium, high)

                # Convert true severity label to a numerical target (e.g., low=0.1, medium=0.5, high=0.9)
                severity_mapping = {"low": 0.1, "medium": 0.5, "high": 0.9}
                true_severity_score = severity_mapping.get(true_severity_label, 0.5) # Default to medium if label is unknown

                features = self.extract_features(contradiction)
                predicted_severity_score = self.severity_predictor(features)

                # Calculate loss
                loss = self.criterion(predicted_severity_score, torch.tensor([[true_severity_score]], dtype=torch.float32))

                # Backpropagation and optimization
                self.optimizer.zero_grad()
                loss.backward()
                self.optimizer.step()

                total_loss += loss.item()

            if (epoch + 1) % 10 == 0:
                print(f"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataset):.4f}")


    # --- Core Calculations ---

    def compute_entropy(self, severity: str, deviation: float = 0.0) -> float:
        """
        Assign an entropy value based on contradiction severity or numeric deviation.
        """
        import math # Added math import
        base = {"low": 0.2, "medium": 0.5, "high": 0.9}.get(severity, 0.1)
        entropy = base + math.log1p(abs(deviation)) * 0.5
        return round(min(entropy, 1.0), 3)

    def compute_clarity(self, entropy: float) -> float:
        """
        Clarity is the inverse of entropy — capped to [0,1].
        """
        return round(1.0 - entropy, 3)

    # --- Processing Interface ---

    def metabolize(self, contradiction: Dict[str, Any], agent: str = "System") -> Dict[str, Any]:
        """
        Converts a contradiction record into clarity metrics and logs the result.
        Uses the trained model to predict severity.
        Includes agent information for reputation tracking and causal tracing.
        """
        # Predict severity using the trained model
        predicted_severity = self.predict_severity(contradiction)
        print(f"Predicted severity: {predicted_severity}")

        dev = contradiction.get("deviation", 0.0)

        # Example of using domain tolerance (this part needs further integration later)
        domain = contradiction.get("domain", "default")
        tolerance = self.domain_tolerances.get(domain, self.domain_tolerances.get("default", 0.03))
        # For now, just demonstrate access to tolerance; actual use depends on future steps
        # print(f"Using tolerance {tolerance} for domain {domain}")


        entropy = self.compute_entropy(predicted_severity, dev) # Use predicted severity
        clarity = self.compute_clarity(entropy)

        metabolism_record = {
            "type": "metabolism",
            "entropy": entropy,
            "clarity": clarity,
            "source": contradiction.get("source", "unknown"), # Include source for causal tracing
            "explanation": f"Contradiction converted → Entropy={entropy}, Clarity={clarity}",
            "predicted_severity": predicted_severity, # Log the predicted severity
            "agent": agent # Include agent in the record
        }

        # Log the metabolism record via the kernel
        logged_receipt = self.kernel.evaluate({"event_type": "system_event", "data": metabolism_record, "agent": agent}) # Pass agent to kernel

        # Store causal link: metabolism receipt ID -> source contradiction ID (if available)
        source_id = contradiction.get("id")
        if source_id and logged_receipt and logged_receipt.get("id"):
             self.causal_trace[logged_receipt["id"]] = source_id

        print(f"⚖️  Metabolized contradiction → Clarity {clarity}, Entropy {entropy}")
        return metabolism_record

    # --- Causal Tracing ---

    def trace_causality(self, event_id: str) -> List[str]:
        """Traces the causal chain back to the original source event."""
        chain = [event_id]
        current_id = event_id
        while current_id in self.causal_trace:
            source_id = self.causal_trace[current_id]
            chain.append(source_id)
            current_id = source_id
            # Prevent infinite loops in case of circular dependencies (shouldn't happen with this logic, but good practice)
            if len(chain) > 100: # Arbitrary limit
                print("Warning: Causal chain tracing exceeded maximum depth.")
                break
        return chain

# --- Demonstration ---
if __name__ == "__main__":
    # Ensure GovernanceKernelV2 is available for the demo
    if 'GovernanceKernelV2' not in globals():
        print("GovernanceKernelV2 not found. Using a mock setup for demonstration.")
        class MockLedger:
            def __init__(self, path="ledger.jsonl"):
                 self.path = Path(f"/content/{path}")
                 self.path.touch(exist_ok=True)
            def append(self, record):
                 record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
                 record["prev_hash"] = "mock"
                 record["hash"] = "mock"
                 record["id"] = str(uuid.uuid4()) # Add a mock ID
                 with self.path.open("a") as f:
                      f.write(json.dumps(record) + "\n")
                 return record
            def verify(self):
                 return True
        class MockReceiptWriter: # Define ReceiptWriter if not global
             def __init__(self, ledger_path="ledger.jsonl"):
                  self.ledger = MockLedger(ledger_path)
             def log(self, event_type, data):
                  return self.ledger.append({"event_type": event_type, "data": data})
             def verify_ledger(self):
                  return self.ledger.verify()
        class GovernanceKernelV2: # Define GovernanceKernelV2 if not global
             def __init__(self, ledger_path="ledger.jsonl"):
                  self.writer = MockReceiptWriter(ledger_path)
                  self.agent_reputation = defaultdict(lambda: 0.5)
             def evaluate(self, event):
                  print(f"Mock GovernanceKernelV2 received event: {event['event_type']}")
                  # Simulate reputation update if event data has metabolism info and agent
                  if "metabolism" in event.get("data", {}).get("type", "") and event.get("agent"):
                       agent = event["agent"]
                       clarity_gain = event["data"].get("clarity", 0.0)
                       entropy_burn = event["data"].get("entropy", 0.0)
                       change = (clarity_gain * 0.1) - (entropy_burn * 0.05)
                       new_reputation = self.agent_reputation[agent] + change
                       self.agent_reputation[agent] = max(0.0, min(1.0, new_reputation))
                       print(f"Mock Reputation update for {agent}: {self.agent_reputation[agent]:.3f}")
                  # Return a mock receipt with an ID
                  return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {})}
             def get_reputation(self, agent):
                  return self.agent_reputation.get(agent, 0.5)


    # Ensure Sentence-BERT model is loaded
    if 'model' not in globals():
         print("Sentence-BERT model not found, loading for demonstration.")
         model = SentenceTransformer('all-MiniLM-L6-v2')

    # Need uuid for mock Ledger/ReceiptWriter
    import uuid
    import time


    adapter = MetabolismAdapter()

    # Small dataset of labeled contradictions for training
    training_data = [
        {"contradiction": {"type": "semantic", "similarity": 0.1, "explanation": "Conflict A"}, "severity": "high"},
        {"contradiction": {"type": "numeric", "deviation": 0.8, "explanation": "Deviation B"}, "severity": "high"},
        {"contradiction": {"type": "semantic", "similarity": 0.6, "explanation": "Conflict C"}, "severity": "medium"},
        {"contradiction": {"type": "numeric", "deviation": 0.3, "explanation": "Deviation D"}, "severity": "medium"},
        {"contradiction": {"type": "semantic", "similarity": 0.9, "explanation": "Conflict E"}, "severity": "low"},
        {"contradiction": {"type": "numeric", "deviation": 0.01, "explanation": "Deviation F"}, "severity": "low"},
    ]

    print("\nTraining the severity predictor...")
    adapter.train_predictor(training_data, epochs=200)

    print("\nDemonstrating metabolism with predicted severity and causal tracing:")
    # Demonstrate metabolizing with a contradiction
    test_contradiction_semantic = {
        "id": str(uuid.uuid4()), # Add an ID for tracing
        "type": "semantic",
        "claim_a": "Revenue increased significantly.",
        "claim_b": "Revenue remained flat.",
        "similarity": adapter.get_semantic_similarity("Revenue increased significantly.", "Revenue remained flat."), # Calculate similarity
        "explanation": "Test semantic contradiction",
        "source": {"type": "report", "id": "Q3_2024_Financials"} # Add source
    }
    metabolism_receipt_semantic = adapter.metabolize(test_contradiction_semantic, agent="FinancialAgent") # Pass agent

    test_contradiction_numeric = {
        "id": str(uuid.uuid4()), # Add an ID for tracing
        "type": "numeric",
        "target": 500,
        "actual": 750,
        "deviation": abs(500-750)/500,
        "explanation": "Test numeric contradiction",
        "source": {"type": "metric", "name": "UserEngagementScore"} # Add source
    }
    metabolism_receipt_numeric = adapter.metabolize(test_contradiction_numeric, agent="MetricsAgent") # Pass agent


    print("\nCausal Tracing:")
    if metabolism_receipt_semantic.get("id"):
        trace_semantic = adapter.trace_causality(metabolism_receipt_semantic["id"])
        print(f"Trace for semantic metabolism ({metabolism_receipt_semantic['id']}): {trace_semantic}")

    if metabolism_receipt_numeric.get("id"):
        trace_numeric = adapter.trace_causality(metabolism_receipt_numeric["id"])
        print(f"Trace for numeric metabolism ({metabolism_receipt_numeric['id']}): {trace_numeric}")


    print("\nAdaptive weighting and causal tracing demonstration complete.")

"""## Adaptive Weighting (Q-learning/Contextual Bandit)

### Subtask:
- Design the state space, action space, and reward function for the RL agent.
- Implement a Q-learning or contextual bandit algorithm using a chosen library (PyTorch/TensorFlow as backend).
- Add `train()` and `predict()` functions for the RL model to estimate contradiction severity based on context (semantic similarity, domain, etc.).

## Semantic Contradiction Classification

### Subtask:
- Load a pre-trained Sentence-BERT model.
- Implement a function to generate embeddings for textual claims.
- Develop a method to compare embeddings and semantically classify contradictions.
"""

from sentence_transformers import SentenceTransformer, util
import torch
import json # Added json import for demonstration
from typing import Dict, Any, List # Added List import for type hinting
from pathlib import Path # Added Path import for config file path

# Load a pre-trained Sentence-BERT model
# Using a smaller model for demonstration purposes
model = SentenceTransformer('all-MiniLM-L6-v2')
print("🧠 Sentence-BERT model loaded.")

class MetabolismAdapter:
    """
    Computes clarity and entropy metrics from contradictions.
    Includes semantic analysis and adaptive weighting (to be implemented).
    """

    def __init__(self, ledger_path: str = "ledger.jsonl", config_path: str = "/content/domains/config.json"):
        # Use a mock kernel if running the demonstration block directly
        if 'GovernanceKernel' in globals():
            self.kernel = GovernanceKernel(ledger_path)
        else:
            # This case should ideally not happen if cells are run sequentially,
            # but adding a safeguard.
            print("Warning: GovernanceKernel not found. Using a mock kernel.")
            class MockKernel:
                 def evaluate(self, event):
                     print(f"Mock Kernel received event: {event['event_type']}")
                     return {"status": "mocked", **event}
            self.kernel = MockKernel()

        self.config_path = Path(config_path)
        self.domain_tolerances = self._load_domain_tolerances()
        self.sentence_model = model # Use the loaded model
        print("🧬 Metabolism Adapter initialized with semantic capabilities.")


    def _load_domain_tolerances(self) -> Dict[str, float]:
        """Loads domain-specific tolerance thresholds from a config file."""
        default_tolerance = 0.03  # Default tolerance if file or domain not found
        if not self.config_path.exists():
            print(f"Warning: Config file not found at {self.config_path}. Using default tolerance.")
            return {"default": default_tolerance}
        try:
            with open(self.config_path, "r") as f:
                config = json.load(f)
            if not isinstance(config, dict):
                 print(f"Warning: Config file {self.config_path} has invalid format. Using default tolerance.")
                 return {"default": default_tolerance}
            # Ensure all values are floats
            for key, value in config.items():
                if not isinstance(value, (int, float)):
                     print(f"Warning: Tolerance for domain '{key}' in {self.config_path} is not a number. Using default tolerance for this domain.")
                     config[key] = default_tolerance
            config["default"] = config.get("default", default_tolerance) # Ensure default is present
            return config
        except json.JSONDecodeError:
            print(f"Warning: Error decoding JSON from {self.config_path}. Using default tolerance.")
            return {"default": default_tolerance}
        except Exception as e:
            print(f"An unexpected error occurred loading config: {e}. Using default tolerance.")
            return {"default": default_tolerance}

    def get_embedding(self, text: str) -> torch.Tensor:
        """Generates a Sentence-BERT embedding for a given text."""
        return self.sentence_model.encode(text, convert_to_tensor=True)

    def get_semantic_similarity(self, text1: str, text2: str) -> float:
        """Computes cosine similarity between embeddings of two texts."""
        embedding1 = self.get_embedding(text1)
        embedding2 = self.get_embedding(text2)
        return util.cos_sim(embedding1, embedding2).item()

    def classify_semantic_contradiction(self, claims: List[str], threshold: float = 0.8) -> List[Dict[str, Any]]:
        """
        Classifies semantic contradictions based on embedding similarity.
        A low similarity might indicate a potential contradiction.
        """
        contradictions = []
        # For simplicity, compare each pair of claims
        for i in range(len(claims)):
            for j in range(i + 1, len(claims)):
                similarity = self.get_semantic_similarity(claims[i], claims[j])
                # Lower similarity indicates potential contradiction
                if similarity < threshold:
                    # Assign severity based on how far below the threshold the similarity is
                    severity = "high" if similarity < threshold * 0.5 else "medium"
                    contradictions.append({
                        "claim_a": claims[i],
                        "claim_b": claims[j],
                        "similarity": round(similarity, 4),
                        "severity": severity,
                        "type": "semantic",
                        "explanation": f"Low semantic similarity ({similarity:.4f}) detected: '{claims[i]}' vs '{claims[j]}'"
                    })
        return contradictions


    # --- Core Calculations ---

    def compute_entropy(self, severity: str, deviation: float = 0.0) -> float:
        """
        Assign an entropy value based on contradiction severity or numeric deviation.
        """
        import math # Added math import
        base = {"low": 0.2, "medium": 0.5, "high": 0.9}.get(severity, 0.1)
        entropy = base + math.log1p(abs(deviation)) * 0.5
        return round(min(entropy, 1.0), 3)

    def compute_clarity(self, entropy: float) -> float:
        """
        Clarity is the inverse of entropy — capped to [0,1].
        """
        return round(1.0 - entropy, 3)

    # --- Processing Interface ---

    def metabolize(self, contradiction: Dict[str, Any]) -> Dict[str, Any]:
        """
        Converts a contradiction record into clarity metrics and logs the result.
        """
        sev = contradiction.get("severity", "low")
        dev = contradiction.get("deviation", 0.0)

        # Example of using domain tolerance (this part needs further integration later)
        domain = contradiction.get("domain", "default")
        tolerance = self.domain_tolerances.get(domain, self.domain_tolerances.get("default", 0.03))
        # For now, just demonstrate access to tolerance; actual use depends on future steps
        # print(f"Using tolerance {tolerance} for domain {domain}")


        entropy = self.compute_entropy(sev, dev)
        clarity = self.compute_clarity(entropy)

        metabolism_record = {
            "type": "metabolism",
            "entropy": entropy,
            "clarity": clarity,
            "source": contradiction.get("source", "unknown"), # Include source for causal tracing
            "explanation": f"Contradiction converted → Entropy={entropy}, Clarity={clarity}",
        }

        self.kernel.evaluate({"event_type": "system_event", "data": metabolism_record})
        print(f"⚖️  Metabolized contradiction → Clarity {clarity}, Entropy {entropy}")
        return metabolism_record

# Demonstration of semantic classification
if __name__ == "__main__":
     # Ensure GovernanceKernel is available for the demo
    if 'GovernanceKernel' not in globals():
        class MockKernel:
             def evaluate(self, event):
                 print(f"Mock Kernel received event: {event['event_type']}")
                 return {"status": "mocked", **event}
        GovernanceKernel = MockKernel
        print("Using Mock GovernanceKernel for semantic classification demo")


    adapter = MetabolismAdapter()

    claims_to_classify = [
        "The product is completely safe for all users.",
        "There are potential safety risks associated with the product.",
        "The quarterly profits exceeded expectations.",
        "The financial results were below forecasts."
    ]

    semantic_contradictions = adapter.classify_semantic_contradiction(claims_to_classify, threshold=0.5)

    print("\nDetected Semantic Contradictions:")
    for c in semantic_contradictions:
        print(json.dumps(c, indent=2))

    # Demonstrate metabolizing a semantic contradiction (using a detected one)
    if semantic_contradictions:
        print("\nMetabolizing a semantic contradiction:")
        adapter.metabolize(semantic_contradictions[0])

    print("\nSemantic classification demonstration complete.")

"""## Configuration loading

### Subtask:
Implement logic to load domain-specific tolerance thresholds from a `domains/config.json` file.

**Reasoning**:
The subtask requires creating a directory and a configuration file, then implementing a function to load this configuration within the `MetabolismAdapter` class. I will create the directory and file and then write the code to read it in the `MetabolismAdapter` class.
"""

import os
import json
from pathlib import Path

# Create the 'domains' directory if it doesn't exist
domains_dir = Path("/content/domains")
domains_dir.mkdir(exist_ok=True)

# Define the path for the config file
config_path = domains_dir / "config.json"

# Define the default configuration
default_config = {"finance": 0.02, "healthcare": 0.05, "default": 0.03}

# Write the default configuration to the file if it doesn't exist
if not config_path.exists():
    with open(config_path, "w") as f:
        json.dump(default_config, f, indent=2)

# Now modify the MetabolismAdapter class to load this config
# Assuming the original MetabolismAdapter is defined in the previous cell and available in the environment.

class MetabolismAdapter:
    """
    Computes clarity and entropy metrics from contradictions.
    """

    def __init__(self, ledger_path: str = "ledger.jsonl", config_path: str = "/content/domains/config.json"):
        self.kernel = GovernanceKernel(ledger_path)
        self.config_path = Path(config_path)
        self.domain_tolerances = self._load_domain_tolerances()
        print("🧬 Metabolism Adapter initialized.")

    def _load_domain_tolerances(self) -> Dict[str, float]:
        """Loads domain-specific tolerance thresholds from a config file."""
        default_tolerance = 0.03  # Default tolerance if file or domain not found
        if not self.config_path.exists():
            print(f"Warning: Config file not found at {self.config_path}. Using default tolerance.")
            return {"default": default_tolerance}
        try:
            with open(self.config_path, "r") as f:
                config = json.load(f)
            if not isinstance(config, dict):
                 print(f"Warning: Config file {self.config_path} has invalid format. Using default tolerance.")
                 return {"default": default_tolerance}
            # Ensure all values are floats
            for key, value in config.items():
                if not isinstance(value, (int, float)):
                     print(f"Warning: Tolerance for domain '{key}' in {self.config_path} is not a number. Using default tolerance for this domain.")
                     config[key] = default_tolerance
            config["default"] = config.get("default", default_tolerance) # Ensure default is present
            return config
        except json.JSONDecodeError:
            print(f"Warning: Error decoding JSON from {self.config_path}. Using default tolerance.")
            return {"default": default_tolerance}
        except Exception as e:
            print(f"An unexpected error occurred loading config: {e}. Using default tolerance.")
            return {"default": default_tolerance}


    # --- Core Calculations ---

    def compute_entropy(self, severity: str, deviation: float = 0.0) -> float:
        """
        Assign an entropy value based on contradiction severity or numeric deviation.
        """
        base = {"low": 0.2, "medium": 0.5, "high": 0.9}.get(severity, 0.1)
        entropy = base + math.log1p(abs(deviation)) * 0.5
        return round(min(entropy, 1.0), 3)

    def compute_clarity(self, entropy: float) -> float:
        """
        Clarity is the inverse of entropy — capped to [0,1].
        """
        return round(1.0 - entropy, 3)

    # --- Processing Interface ---

    def metabolize(self, contradiction: Dict[str, Any]) -> Dict[str, Any]:
        """
        Converts a contradiction record into clarity metrics and logs the result.
        """
        sev = contradiction.get("severity", "low")
        dev = contradiction.get("deviation", 0.0)

        # Example of using domain tolerance (this part needs further integration later)
        domain = contradiction.get("domain", "default")
        tolerance = self.domain_tolerances.get(domain, self.domain_tolerances.get("default", 0.03))
        # For now, just demonstrate access to tolerance; actual use depends on future steps
        print(f"Using tolerance {tolerance} for domain {domain}")


        entropy = self.compute_entropy(sev, dev)
        clarity = self.compute_clarity(entropy)

        metabolism_record = {
            "type": "metabolism",
            "entropy": entropy,
            "clarity": clarity,
            "source": contradiction,
            "explanation": f"Contradiction converted → Entropy={entropy}, Clarity={clarity}",
        }

        self.kernel.evaluate({"event_type": "system_event", "data": metabolism_record})
        print(f"⚖️  Metabolized contradiction → Clarity {clarity}, Entropy {entropy}")
        return metabolism_record

# Demonstration - Re-run the demonstration part to test the new __init__ and loading
if __name__ == "__main__":
    # Ensure GovernanceKernel is available in the environment
    if 'GovernanceKernel' not in globals():
        # Minimal mock if running this cell in isolation for testing
        class MockKernel:
             def evaluate(self, event):
                 print(f"Mock Kernel received event: {event['event_type']}")
                 return {"status": "mocked", **event}
        GovernanceKernel = MockKernel
        print("Using Mock GovernanceKernel")


    adapter = MetabolismAdapter()

    # Example 1: From textual contradiction with implicit default domain
    contradiction_a = {
        "type": "textual",
        "severity": "high",
        "claim_a": "Product is safe.",
        "claim_b": "Product is unsafe.",
        "explanation": "Safety claim conflict",
    }

    # Example 2: From numeric contradiction with specified domain
    contradiction_b = {
        "type": "numeric",
        "severity": "medium",
        "target": 100,
        "actual": 160,
        "deviation": 0.6,
        "domain": "finance" # Add domain
    }

    # Example 3: Contradiction with unknown domain
    contradiction_c = {
        "type": "numeric",
        "severity": "low",
        "target": 50,
        "actual": 55,
        "deviation": 0.1,
        "domain": "unknown_domain" # Add unknown domain
    }


    adapter.metabolize(contradiction_a)
    adapter.metabolize(contradiction_b)
    adapter.metabolize(contradiction_c)

    # Note: The verification and file peeking require a real Ledger/ReceiptWriter/Kernel setup
    # Skipping ledger verification and file peeking for this isolated test cell demonstration
    print("\nDemonstration of config loading complete.")

# Phase 3 — governance_kernel.py (V2)
"""
Tessrax Governance Kernel v2.0
-------------------------------
Evaluates governance events, applies basic policy checks,
and logs outcomes to the Tessrax Ledger using ReceiptWriter.
Includes hierarchical quorum, dynamic amendments, reputation, and explainability.
"""

import json
from typing import Dict, Any, List
from collections import defaultdict
import math # Import math for sigmoid function
import time # Import time for timestamp

# Import from previous phases (assuming they are available in the environment)
# from receipts import ReceiptWriter


class GovernanceKernel:
    """
    The Governance Kernel evaluates policy and contradiction events.
    It determines severity, compliance, and logs them as verifiable receipts.
    (Base class for V2)
    """

    def __init__(self, ledger_path: str = "ledger.jsonl"):
        # Use the ReceiptWriter class already defined in the environment
        if 'ReceiptWriter' in globals():
             self.writer = ReceiptWriter(ledger_path)
        else:
             # Fallback mock for isolated testing if ReceiptWriter is not defined
             print("Warning: ReceiptWriter not found. Using a mock writer.")
             class MockReceiptWriter:
                  def log(self, event_type, data):
                      print(f"Mock ReceiptWriter logged event: {event_type}")
                      return {"event_type": event_type, "data": data, "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()), "prev_hash": "mock", "hash": "mock"}
                  def verify_ledger(self):
                      print("Mock ReceiptWriter verified ledger.")
                      return True
             self.writer = MockReceiptWriter()


        self.rules = {
            "contradiction": self._rule_contradiction,
            "policy_violation": self._rule_policy_violation,
            "system_event": self._rule_system_event,
        }
        print("⚙️ Governance Kernel initialized.")

    # --- Rule Definitions (from V1, potentially extended in V2) ---

    def _rule_contradiction(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Detect and classify contradictions in incoming events."""
        description = data.get("explanation", data.get("description", "")) # Use explanation from V2 metabolism
        if "conflict" in description.lower() or "inconsistent" in description.lower() or "contradictory" in description.lower():
            data["severity"] = data.get("predicted_severity", "high") # Use predicted severity if available
            data["evaluation"] = "Contradiction detected"
        else:
            data["severity"] = data.get("predicted_severity", "low") # Use predicted severity if available
            data["evaluation"] = "Contradiction found (low severity)"
        return data

    def _rule_policy_violation(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Apply a simple rule check for compliance violations."""
        policy = data.get("policy", "")
        action = data.get("action", "")
        if policy and action and policy.lower() not in action.lower():
            data["severity"] = "medium"
            data["evaluation"] = f"Violation of policy: {policy}"
        else:
            data["severity"] = "none"
            data["evaluation"] = "No violation"
        return data

    def _rule_system_event(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Log normal operational events (for heartbeat, updates, etc.)."""
        data["severity"] = "info"
        data["evaluation"] = "System event logged"
        return data

    # --- Evaluation Interface (from V1, potentially extended in V2) ---

    def evaluate(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate an event dictionary with an event_type and data field.
        Automatically logs the result to the ledger.
        """
        event_type = event.get("event_type", "")
        data = event.get("data", {})

        if event_type in self.rules:
            result = self.rules[event_type](data)
        else:
            result = {"evaluation": "Unknown event type", "severity": "none"}

        # Write the result as a receipt
        receipt = self.writer.log(event_type.upper(), result)
        print(f"🧾 Logged event → {receipt['event_type']} ({result.get('evaluation')})")
        return receipt

    def verify_ledger(self) -> bool:
        """Run a chain integrity check on the ledger."""
        return self.writer.verify_ledger()


class GovernanceKernelV2(GovernanceKernel):
    """
    Governance Kernel with hierarchical quorum, dynamic amendments, reputation,
    and explainability.
    """

    def __init__(self, ledger_path: str = "ledger.jsonl", amendment_threshold: int = 5):
        super().__init__(ledger_path)
        self.amendment_threshold = amendment_threshold
        self.rule_contradiction_counts: Dict[str, int] = defaultdict(int)
        self.agent_reputation: Dict[str, float] = defaultdict(lambda: 0.5) # Start with neutral reputation
        self.quorum_thresholds = {
            "local": 0.6,    # e.g., 60% agreement needed locally
            "regional": 0.75, # e.g., 75% agreement needed regionally
            "global": 0.9    # e.g., 90% agreement needed globally
        }
        print("👑 Governance Kernel V2 initialized.")

    # --- Hierarchical Quorum ---

    def check_quorum(self, votes: Dict[str, float], level: str) -> bool:
        """
        Checks if a set of votes meets the quorum threshold for a given level.
        Votes is a dict of agent_id: vote_value (0 to 1).
        """
        if not votes:
            return False
        total_votes = len(votes)
        agreement_score = sum(votes.values()) / total_votes
        threshold = self.quorum_thresholds.get(level, 0.5) # Default to 0.5 if level is unknown
        print(f"Quorum check for level '{level}': Agreement = {agreement_score:.2f}, Threshold = {threshold:.2f}")
        return agreement_score >= threshold

    # --- Dynamic Constitutional Updates ---

    def _propose_amendment(self, rule_name: str, reason: str):
        """Logs a proposed amendment to the ledger."""
        amendment_proposal = {
            "type": "amendment_proposal",
            "rule": rule_name,
            "reason": reason,
            "status": "proposed",
            "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        }
        self.writer.log("AMENDMENT_PROPOSED", amendment_proposal)
        print(f"📜 Proposed amendment for rule '{rule_name}' due to frequent contradictions.")


    def _check_amendment_trigger(self, rule_name: str):
        """Checks if a rule has triggered enough contradictions to propose an amendment."""
        if rule_name in self.rule_contradiction_counts and self.rule_contradiction_counts[rule_name] >= self.amendment_threshold:
            self._propose_amendment(rule_name, f"Rule '{rule_name}' has been contradicted {self.amendment_threshold} or more times.")
            self.rule_contradiction_counts[rule_name] = 0 # Reset counter after proposal

    # --- Reputation Tracking ---

    def update_reputation(self, agent: str, clarity_gain: float, entropy_burn: float):
        """Updates agent reputation based on clarity and entropy."""
        # Simple reputation update: reward clarity, penalize entropy
        # Use a sigmoid-like function to keep reputation between 0 and 1
        change = (clarity_gain * 0.1) - (entropy_burn * 0.05) # Adjust weights as needed
        new_reputation = self.agent_reputation[agent] + change
        self.agent_reputation[agent] = max(0.0, min(1.0, new_reputation)) # Clamp between 0 and 1
        print(f"👤 Updated reputation for {agent}: {self.agent_reputation[agent]:.3f}")
        # Log reputation update as a system event
        reputation_record = {
            "type": "reputation_update",
            "agent": agent,
            "change": round(change, 3),
            "new_reputation": round(self.agent_reputation[agent], 3)
        }
        self.writer.log("SYSTEM_EVENT", reputation_record)


    def get_reputation(self, agent: str) -> float:
        """Returns the current reputation of an agent."""
        return self.agent_reputation.get(agent, 0.5) # Default to neutral if agent not found

    # --- Explainability ---

    def explain_decision(self, receipt: Dict[str, Any]) -> str:
        """Generates a natural-language rationale for a given receipt."""
        event_type = receipt.get("event_type", "UNKNOWN_EVENT")
        data = receipt.get("data", {})
        timestamp = receipt.get("timestamp", "an unknown time")
        evaluation = data.get("evaluation", "no specific evaluation")
        severity = data.get("severity", "unknown severity")
        explanation = data.get("explanation", "no detailed explanation")
        source = data.get("source", "an unknown source")

        rationale = f"Decision for event type '{event_type}' logged at {timestamp}.\n"
        rationale += f"Evaluation: {evaluation}.\n"
        rationale += f"Assigned Severity: {severity}.\n"

        if explanation and explanation != "no detailed explanation":
             rationale += f"Details: {explanation}\n"

        if isinstance(source, dict):
             source_type = source.get("type", "unknown")
             if source_type == "metabolism":
                  source_details = source.get("source", {})
                  source_type_orig = source_details.get("type", "contradiction")
                  rationale += f"This record is a metabolism outcome derived from a {source_type_orig} event.\n"
             elif source_type == "report":
                  rationale += f"This event originated from a report with ID: {source.get('id', 'N/A')}.\n"
             elif source_type == "metric":
                  rationale += f"This event originated from a metric named: {source.get('name', 'N/A')}.\n"
             elif source_type == "claims":
                  rationale += f"This event originated from comparing claims: {source.get('claims', [])}.\n"
             else:
                  rationale += f"This event originated from a source of type: {source_type}.\n"
        else:
             rationale += f"This event originated from: {source}.\n"


        # Add context specific to event types
        if event_type == "CONTRADICTION":
            rationale += "This event indicates a detected contradiction within the system or data.\n"
        elif event_type == "POLICY_VIOLATION":
            policy = data.get("policy", "an unspecified policy")
            rationale += f"This event indicates a violation of policy: {policy}.\n"
        elif event_type == "AMENDMENT_PROPOSED":
             rule = data.get("rule", "an unspecified rule")
             reason = data.get("reason", "no specific reason")
             rationale += f"An amendment has been proposed for rule '{rule}' because: {reason}.\n"
        elif event_type == "SYSTEM_EVENT":
             system_event_type = data.get("type", "general system event")
             rationale += f"This is a system event of type: {system_event_type}.\n"


        return rationale.strip() # Remove trailing newline


    # --- Extended Evaluation Interface (Override) ---

    def evaluate(self, event: Dict[str, Any]) -> Dict[str, Any]:
        """
        Evaluate an event, log to ledger, and perform V2 specific actions
        like checking for amendment triggers and updating reputation.
        """
        event_type = event.get("event_type", "")
        data = event.get("data", {})
        agent = event.get("agent") # Get agent information if available

        # Perform V1 evaluation and logging
        receipt = super().evaluate(event)

        # V2: Check for amendment triggers if it's a contradiction
        if event_type == "contradiction":
             # Assuming the original rule name is in the data or derived
             # For this example, let's use the event_type as the rule name if not specified
             rule_name = data.get("rule_name", event_type)
             self.rule_contradiction_counts[rule_name] += 1
             self._check_amendment_trigger(rule_name)

        # V2: Update agent reputation if agent information and relevant data are present
        if agent and "metabolism" in data.get("type", ""):
             clarity_gain = data.get("clarity", 0.0)
             entropy_burn = data.get("entropy", 0.0)
             self.update_reputation(agent, clarity_gain, entropy_burn)


        return receipt


# --- Demonstration ---
if __name__ == "__main__":
    # Ensure ReceiptWriter and Ledger are available for the demo
    if 'ReceiptWriter' not in globals():
        print("ReceiptWriter not found. Using a mock setup for demonstration.")
        class MockLedger:
            def __init__(self, path="ledger.jsonl"):
                 self.path = Path(f"/content/{path}")
                 self.path.touch(exist_ok=True)
            def append(self, record):
                 record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
                 record["prev_hash"] = "mock"
                 record["hash"] = "mock"
                 with self.path.open("a") as f:
                      f.write(json.dumps(record) + "\n")
                 return record
            def verify(self):
                 return True
        class ReceiptWriter: # Define ReceiptWriter if not global
             def __init__(self, ledger_path="ledger.jsonl"):
                  self.ledger = MockLedger(ledger_path)
             def log(self, event_type, data):
                  return self.ledger.append({"event_type": event_type, "data": data})
             def verify_ledger(self):
                  return self.ledger.verify()

    kernel_v2 = GovernanceKernelV2(amendment_threshold=3) # Set a low threshold for demo

    # Simulate some contradiction events to trigger an amendment proposal
    print("\nSimulating contradictions to trigger amendment proposal...")
    for i in range(4):
        kernel_v2.evaluate({"event_type": "contradiction", "data": {"description": f"Simulated conflict {i+1}"}, "agent": "SimAgent"})

    # Simulate a system event with metabolism data to update reputation
    print("\nSimulating metabolism event to update reputation...")
    metabolism_output = {
        "type": "metabolism",
        "entropy": 0.8,
        "clarity": 0.2,
        "source": {"type": "simulated_data"},
        "explanation": "Simulated high entropy event"
    }
    kernel_v2.evaluate({"event_type": "system_event", "data": metabolism_output, "agent": "ReputationAgent"})

    # Check reputation
    print(f"\nReputation of SimAgent: {kernel_v2.get_reputation('SimAgent'):.3f}")
    print(f"Reputation of ReputationAgent: {kernel_v2.get_reputation('ReputationAgent'):.3f}")


    # Demonstrate quorum check
    print("\nDemonstrating quorum checks:")
    local_votes = {"agent1": 0.7, "agent2": 0.5, "agent3": 0.8}
    regional_votes = {"region_a": 0.8, "region_b": 0.7}
    global_votes = {"continent_x": 0.95}

    print(f"Local quorum met? {kernel_v2.check_quorum(local_votes, 'local')}")
    print(f"Regional quorum met? {kernel_v2.check_quorum(regional_votes, 'regional')}")
    print(f"Global quorum met? {kernel_v2.check_quorum(global_votes, 'global')}")


    # Demonstrate explain_decision for a recent event
    print("\nDemonstrating decision explanation:")
    # Assuming the last event logged was the amendment proposal
    try:
        with open("/content/ledger.jsonl", "r") as f:
            last_line = f.readlines()[-1]
            last_receipt = json.loads(last_line)
            print(kernel_v2.explain_decision(last_receipt))
    except Exception as e:
        print(f"Could not read last receipt from ledger: {e}")


    print("\nGovernance Kernel V2 demonstration complete.")

"""# Task
Build a new file `clarity_market.py` that introduces a token-based clarity fuel marketplace, allows staking and slashing (agents stake fuel on accuracy of contradiction assessments), integrates external ESG or carbon-credit APIs (mock endpoints acceptable), and adds function `predictive_maintenance()` that detects decline in transaction velocity. Output full module code with example transactions between two agents and ensure it logs to "ledger.jsonl" for traceability.

## Define the claritymarket class

### Subtask:
Create the basic structure for the `ClarityMarket` class, initializing it with access to the existing ledger.

**Reasoning**:
The subtask requires creating a new Python file and defining a class with an initialization method that interacts with the existing `GovernanceKernelV2`. I will use the `%%writefile` magic command to create the file and add the basic class structure and initialization logic as described in the instructions.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile clarity_market.py
# """
# Tessrax Clarity Fuel Economy v2.0 - Clarity Market
# --------------------------------------------------
# Introduces a token-based clarity fuel marketplace, allowing staking and slashing,
# integration with external APIs, and predictive maintenance based on transaction velocity.
# """
# 
# import json
# from typing import Dict, Any, List
# from collections import defaultdict
# 
# # GovernanceKernelV2 should be available in the environment from previous steps
# # from governance_kernel import GovernanceKernelV2
# # ReceiptWriter should also be available
# # from receipts import ReceiptWriter
# 
# class ClarityMarket:
#     """
#     Manages the clarity fuel token economy, including transactions, staking,
#     slashing, and external data integration.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Initialize with access to the GovernanceKernelV2
#         # Assuming GovernanceKernelV2 is available in the Colab environment
#         if 'GovernanceKernelV2' in globals():
#              self.kernel = GovernanceKernelV2(ledger_path)
#         elif 'GovernanceKernel' in globals():
#              # Fallback to base kernel if V2 is not found
#              print("Warning: GovernanceKernelV2 not found. Using base GovernanceKernel.")
#              self.kernel = GovernanceKernel(ledger_path)
#         else:
#              # Mock kernel if neither is found (for isolated testing)
#              print("Error: Neither GovernanceKernelV2 nor GovernanceKernel found. Using a mock kernel.")
#              class MockKernel:
#                   def evaluate(self, event):
#                       print(f"Mock Kernel received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
#                       # Return a mock receipt with an ID for potential tracing
#                       return {"id": "mock_" + str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {})}
#              self.kernel = MockKernel()
#              import uuid # Import uuid for mock ID
# 
#         self.balances: Dict[str, float] = defaultdict(float) # Using defaultdict for balances
#         self.staked_balances: Dict[str, float] = defaultdict(float) # To track staked fuel
#         self.transaction_history: List[Dict[str, Any]] = [] # To track transactions for velocity
#         self.last_transaction_timestamp: Dict[str, float] = {} # To track last transaction time per agent
# 
# 
#         print("💰 Clarity Fuel Market initialized.")
# 
# 
#     # Basic transaction function (can be extended later)
#     def transfer_fuel(self, sender: str, recipient: str, amount: float):
#         """Transfers clarity fuel between agents."""
#         if self.balances[sender] >= amount > 0:
#             self.balances[sender] -= amount
#             self.balances[recipient] += amount
#             transaction_record = {
#                 "type": "fuel_transfer",
#                 "sender": sender,
#                 "recipient": recipient,
#                 "amount": amount,
#                 "sender_balance": round(self.balances[sender], 3),
#                 "recipient_balance": round(self.balances[recipient], 3),
#                 "explanation": f"Fuel transfer from {sender} to {recipient}: {amount}"
#             }
#             # Log the transaction via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": transaction_record, "agent": sender})
# 
#             # Update transaction history and timestamp for predictive maintenance
#             self.transaction_history.append(transaction_record)
#             current_time = time.time() # Use epoch time for velocity calculation
#             self.last_transaction_timestamp[sender] = current_time
#             self.last_transaction_timestamp[recipient] = current_time
# 
# 
#             print(f"💸 {sender} transferred {amount} fuel to {recipient}.")
#             return True
#         else:
#             print(f"❌ Transfer failed: {sender} has insufficient funds or invalid amount.")
#             return False
# 
# 
#     # Placeholder for staking logic
#     def stake_fuel(self, agent: str, amount: float):
#         """Stakes clarity fuel on contradiction assessment accuracy."""
#         # Implementation for staking will go here
#         print(f"⏳ Staking logic for {agent} is not yet implemented.")
#         pass
# 
#     # Placeholder for slashing logic
#     def slash_fuel(self, agent: str, amount: float, reason: str):
#         """Slahses staked fuel due to inaccurate assessments."""
#         # Implementation for slashing will go here
#         print(f"🔪 Slashing logic for {agent} is not yet implemented.")
#         pass
# 
#     # Placeholder for external API integration (ESG/Carbon Credits)
#     def integrate_external_data(self, data_type: str, source: str):
#         """Integrates data from external APIs (ESG, Carbon Credits, etc.)."""
#         # Implementation for external API integration will go here
#         print(f"🔌 External API integration for {data_type} from {source} is not yet implemented.")
#         pass
# 
#     # Placeholder for predictive maintenance
#     def predictive_maintenance(self):
#         """Detects decline in transaction velocity or other anomalies."""
#         # Implementation for predictive maintenance will go here
#         print("⚠️ Predictive maintenance logic is not yet implemented.")
#         pass
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#      # Ensure necessary classes are available for the demo
#      if 'GovernanceKernelV2' not in globals():
#          print("GovernanceKernelV2 not found. Using a mock setup for demonstration.")
#          class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#              def append(self, record):
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = "mock"
#                   record["hash"] = "mock"
#                   record["id"] = str(uuid.uuid4()) # Add a mock ID
#                   with self.path.open("a") as f:
#                        f.write(json.dumps(record) + "\n")
#                   return record
#              def verify(self):
#                   return True
#          class MockReceiptWriter: # Define ReceiptWriter if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.ledger = MockLedger(ledger_path)
#               def log(self, event_type, data):
#                    return self.ledger.append({"event_type": event_type, "data": data})
#               def verify_ledger(self):
#                    return self.ledger.verify()
#          class GovernanceKernelV2: # Define GovernanceKernelV2 if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.writer = MockReceiptWriter(ledger_path)
#                    self.agent_reputation = defaultdict(lambda: 0.5)
#               def evaluate(self, event):
#                    print(f"Mock GovernanceKernelV2 received event: {event['event_type']}")
#                    # Simulate reputation update if event data has metabolism info and agent
#                    if "metabolism" in event.get("data", {}).get("type", "") and event.get("agent"):
#                         agent = event["agent"]
#                         clarity_gain = event["data"].get("clarity", 0.0)
#                         entropy_burn = event["data"].get("entropy", 0.0)
#                         change = (clarity_gain * 0.1) - (entropy_burn * 0.05)
#                         new_reputation = self.agent_reputation[agent] + change
#                         self.agent_reputation[agent] = max(0.0, min(1.0, new_reputation))
#                         print(f"Mock Reputation update for {agent}: {self.agent_reputation[agent]:.3f}")
#                    # Return a mock receipt with an ID
#                    return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {})}
#               def get_reputation(self, agent):
#                    return self.agent_reputation.get(agent, 0.5)
#          import time # Import time for mock timestamp
#          import uuid # Import uuid for mock ID
# 
# 
#      market = ClarityMarket()
# 
#      # Seed some initial balances for demonstration
#      market.balances["AgentA"] = 100.0
#      market.balances["AgentB"] = 50.0
#      market.balances["AgentC"] = 200.0
# 
#      print("\nInitial Balances:", dict(market.balances))
# 
#      # Demonstrate transactions
#      print("\nDemonstrating transactions:")
#      market.transfer_fuel("AgentA", "AgentB", 20.0)
#      market.transfer_fuel("AgentC", "AgentA", 50.0)
#      market.transfer_fuel("AgentB", "AgentC", 10.0)
#      market.transfer_fuel("AgentA", "AgentD", 30.0) # Transfer to a new agent
# 
#      # Demonstrate failed transaction
#      market.transfer_fuel("AgentB", "AgentA", 100.0) # Should fail due to insufficient funds
# 
#      print("\nFinal Balances:", dict(market.balances))
# 
#      # Demonstrate placeholders for future functionality
#      market.stake_fuel("AgentA", 10.0)
#      market.slash_fuel("AgentB", 5.0, "Inaccurate assessment")
#      market.integrate_external_data("ESG", "some_api.com")
#      market.predictive_maintenance()
# 
# 
#      print("\nClarity Market basic structure and transaction demo complete.")

"""## Implement token mechanics

### Subtask:
Implement methods for agents to hold, transfer, stake, and unstake clarity fuel tokens.

**Reasoning**:
Implement the methods for holding, transferring, staking, and unstaking clarity fuel tokens as specified in the subtask instructions.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile clarity_market.py
# """
# Tessrax Clarity Fuel Economy v2.0 - Clarity Market
# --------------------------------------------------
# Introduces a token-based clarity fuel marketplace, allowing staking and slashing,
# integration with external APIs, and predictive maintenance based on transaction velocity.
# """
# 
# import json
# from typing import Dict, Any, List
# from collections import defaultdict
# import time # Import time for timestamp
# import uuid # Import uuid for logging
# 
# # GovernanceKernelV2 should be available in the environment from previous steps
# # from governance_kernel import GovernanceKernelV2
# # ReceiptWriter should also be available
# # from receipts import ReceiptWriter
# 
# class ClarityMarket:
#     """
#     Manages the clarity fuel token economy, including transactions, staking,
#     slashing, and external data integration.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Initialize with access to the GovernanceKernelV2
#         # Assuming GovernanceKernelV2 is available in the Colab environment
#         if 'GovernanceKernelV2' in globals():
#              self.kernel = GovernanceKernelV2(ledger_path)
#         elif 'GovernanceKernel' in globals():
#              # Fallback to base kernel if V2 is not found
#              print("Warning: GovernanceKernelV2 not found. Using base GovernanceKernel.")
#              self.kernel = GovernanceKernel(ledger_path)
#         else:
#              # Mock kernel if neither is found (for isolated testing)
#              print("Error: Neither GovernanceKernelV2 nor GovernanceKernel found. Using a mock kernel.")
#              class MockKernel:
#                   def evaluate(self, event):
#                       print(f"Mock Kernel received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
#                       # Return a mock receipt with an ID for potential tracing
#                       return {"id": "mock_" + str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#              self.kernel = MockKernel()
# 
# 
#         self.balances: Dict[str, float] = defaultdict(float) # Using defaultdict for balances
#         self.staked_balances: Dict[str, float] = defaultdict(float) # To track staked fuel
#         self.transaction_history: List[Dict[str, Any]] = [] # To track transactions for velocity
#         self.last_transaction_timestamp: Dict[str, float] = {} # To track last transaction time per agent
# 
# 
#         print("💰 Clarity Fuel Market initialized.")
# 
# 
#     # Basic transaction function (can be extended later)
#     def transfer_fuel(self, sender: str, recipient: str, amount: float):
#         """Transfers clarity fuel between agents."""
#         if amount <= 0:
#             print(f"❌ Transfer failed: Amount must be positive.")
#             return False
# 
#         if self.balances[sender] >= amount:
#             self.balances[sender] -= amount
#             self.balances[recipient] += amount
#             transaction_record = {
#                 "type": "fuel_transfer",
#                 "sender": sender,
#                 "recipient": recipient,
#                 "amount": round(amount, 3),
#                 "sender_balance": round(self.balances[sender], 3),
#                 "recipient_balance": round(self.balances[recipient], 3),
#                 "explanation": f"Fuel transfer from {sender} to {recipient}: {amount}"
#             }
#             # Log the transaction via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": transaction_record, "agent": sender})
# 
#             # Update transaction history and timestamp for predictive maintenance
#             self.transaction_history.append(transaction_record)
#             current_time = time.time() # Use epoch time for velocity calculation
#             self.last_transaction_timestamp[sender] = current_time
#             self.last_transaction_timestamp[recipient] = current_time
# 
# 
#             print(f"💸 {sender} transferred {round(amount, 3)} fuel to {recipient}.")
#             return True
#         else:
#             print(f"❌ Transfer failed: {sender} has insufficient funds ({self.balances[sender]:.3f} < {amount:.3f}).")
#             return False
# 
# 
#     def stake_fuel(self, agent: str, amount: float):
#         """Stakes clarity fuel on contradiction assessment accuracy."""
#         if amount <= 0:
#             print(f"❌ Staking failed: Amount must be positive.")
#             return False
# 
#         if self.balances[agent] >= amount:
#             self.balances[agent] -= amount
#             self.staked_balances[agent] += amount
#             staking_record = {
#                 "type": "fuel_stake",
#                 "agent": agent,
#                 "amount": round(amount, 3),
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} staked {amount} fuel."
#             }
#             # Log the staking event via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": staking_record, "agent": agent})
#             print(f"✅ {agent} staked {round(amount, 3)} fuel.")
#             return True
#         else:
#             print(f"❌ Staking failed: {agent} has insufficient funds ({self.balances[agent]:.3f} < {amount:.3f}).")
#             return False
# 
#     def unstake_fuel(self, agent: str, amount: float):
#         """Unstakes clarity fuel."""
#         if amount <= 0:
#             print(f"❌ Unstaking failed: Amount must be positive.")
#             return False
# 
#         if self.staked_balances[agent] >= amount:
#             self.staked_balances[agent] -= amount
#             self.balances[agent] += amount
#             unstaking_record = {
#                 "type": "fuel_unstake",
#                 "agent": agent,
#                 "amount": round(amount, 3),
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} unstaked {amount} fuel."
#             }
#              # Log the unstaking event via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": unstaking_record, "agent": agent})
#             print(f"✅ {agent} unstaked {round(amount, 3)} fuel.")
#             return True
#         else:
#             print(f"❌ Unstaking failed: {agent} has insufficient staked fuel ({self.staked_balances[agent]:.3f} < {amount:.3f}).")
#             return False
# 
# 
#     # Placeholder for slashing logic
#     def slash_fuel(self, agent: str, amount: float, reason: str):
#         """Slahses staked fuel due to inaccurate assessments."""
#         # Implementation for slashing will go here
#         print(f"🔪 Slashing logic for {agent} is not yet implemented.")
#         pass
# 
#     # Placeholder for external API integration (ESG/Carbon Credits)
#     def integrate_external_data(self, data_type: str, source: str):
#         """Integrates data from external APIs (ESG, Carbon Credits, etc.)."""
#         # Implementation for external API integration will go here
#         print(f"🔌 External API integration for {data_type} from {source} is not yet implemented.")
#         pass
# 
#     # Placeholder for predictive maintenance
#     def predictive_maintenance(self):
#         """Detects decline in transaction velocity or other anomalies."""
#         # Implementation for predictive maintenance will go here
#         print("⚠️ Predictive maintenance logic is not yet implemented.")
#         pass
# 
#     def get_status(self) -> Dict[str, Dict[str, float]]:
#         """Return current balances and staked balances."""
#         status = {}
#         all_agents = set(list(self.balances.keys()) + list(self.staked_balances.keys()))
#         for agent in all_agents:
#             status[agent] = {
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3)
#             }
#         return status
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#      # Ensure necessary classes are available for the demo
#      if 'GovernanceKernelV2' not in globals():
#          print("GovernanceKernelV2 not found. Using a mock setup for demonstration.")
#          class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#              def append(self, record):
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = "mock"
#                   record["hash"] = "mock"
#                   record["id"] = str(uuid.uuid4()) # Add a mock ID
#                   with self.path.open("a") as f:
#                        f.write(json.dumps(record) + "\n")
#                   return record
#              def verify(self):
#                   return True
#          class MockReceiptWriter: # Define ReceiptWriter if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.ledger = MockLedger(ledger_path)
#               def log(self, event_type, data):
#                    return self.ledger.append({"event_type": event_type, "data": data})
#               def verify_ledger(self):
#                    return self.ledger.verify()
#          class GovernanceKernelV2: # Define GovernanceKernelV2 if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.writer = MockReceiptWriter(ledger_path)
#                    self.agent_reputation = defaultdict(lambda: 0.5)
#               def evaluate(self, event):
#                    print(f"Mock GovernanceKernelV2 received event: {event['event_type']}")
#                    # Simulate reputation update if event data has metabolism info and agent
#                    if "metabolism" in event.get("data", {}).get("type", "") and event.get("agent"):
#                         agent = event["agent"]
#                         clarity_gain = event["data"].get("clarity", 0.0)
#                         entropy_burn = event["data"].get("entropy", 0.0)
#                         change = (clarity_gain * 0.1) - (entropy_burn * 0.05)
#                         new_reputation = self.agent_reputation[agent] + change
#                         self.agent_reputation[agent] = max(0.0, min(1.0, new_reputation))
#                         print(f"Mock Reputation update for {agent}: {self.agent_reputation[agent]:.3f}")
#                    # Return a mock receipt with an ID
#                    return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#               def get_reputation(self, agent):
#                    return self.agent_reputation.get(agent, 0.5)
#          import time # Import time for mock timestamp
#          import uuid # Import uuid for mock ID
# 
# 
#      market = ClarityMarket()
# 
#      # Seed some initial balances for demonstration
#      market.balances["AgentA"] = 100.0
#      market.balances["AgentB"] = 50.0
#      market.balances["AgentC"] = 200.0
# 
#      print("\nInitial Balances:", json.dumps(market.get_status(), indent=2))
# 
#      # Demonstrate transactions
#      print("\nDemonstrating transfers:")
#      market.transfer_fuel("AgentA", "AgentB", 20.0)
#      market.transfer_fuel("AgentC", "AgentA", 50.0)
#      market.transfer_fuel("AgentB", "AgentC", 10.0)
#      market.transfer_fuel("AgentA", "AgentD", 30.0) # Transfer to a new agent
# 
#      # Demonstrate failed transaction
#      market.transfer_fuel("AgentB", "AgentA", 100.0) # Should fail due to insufficient funds
#      market.transfer_fuel("AgentA", "AgentB", -10.0) # Should fail due to negative amount
# 
#      print("\nBalances after transfers:", json.dumps(market.get_status(), indent=2))
# 
#      # Demonstrate staking
#      print("\nDemonstrating staking:")
#      market.stake_fuel("AgentA", 40.0) # Should succeed
#      market.stake_fuel("AgentB", 50.0) # Should fail (balance is 40)
#      market.stake_fuel("AgentC", -10.0) # Should fail (negative amount)
# 
#      print("\nBalances after staking:", json.dumps(market.get_status(), indent=2))
# 
#      # Demonstrate unstaking
#      print("\nDemonstrating unstaking:")
#      market.unstake_fuel("AgentA", 10.0) # Should succeed
#      market.unstake_fuel("AgentA", 40.0) # Should fail (staked is 30)
#      market.unstake_fuel("AgentB", 5.0) # Should fail (staked is 0)
#      market.unstake_fuel("AgentC", -5.0) # Should fail (negative amount)
# 
#      print("\nBalances after unstaking:", json.dumps(market.get_status(), indent=2))
# 
# 
#      # Demonstrate placeholders for future functionality
#      market.slash_fuel("AgentB", 5.0, "Inaccurate assessment")
#      market.integrate_external_data("ESG", "some_api.com")
#      market.predictive_maintenance()
# 
# 
#      print("\nClarity Market token mechanics demo complete.")

"""## Implement staking and slashing

### Subtask:
Design and implement the logic for staking tokens on contradiction assessments and slashing tokens for inaccurate assessments.

**Reasoning**:
Implement the staking and slashing logic by adding parameters to the `stake_fuel` method to track staked assessments, creating a `slash_fuel` method that reduces staked balances based on inaccuracy, and maintaining a record of stakes linked to assessments. Update the demonstration to show staking and slashing.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile clarity_market.py
# """
# Tessrax Clarity Fuel Economy v2.0 - Clarity Market
# --------------------------------------------------
# Introduces a token-based clarity fuel marketplace, allowing staking and slashing,
# integration with external APIs, and predictive maintenance based on transaction velocity.
# """
# 
# import json
# from typing import Dict, Any, List
# from collections import defaultdict
# import time # Import time for timestamp
# import uuid # Import uuid for logging
# 
# # GovernanceKernelV2 should be available in the environment from previous steps
# # from governance_kernel import GovernanceKernelV2
# # ReceiptWriter should also be available
# # from receipts import ReceiptWriter
# 
# class ClarityMarket:
#     """
#     Manages the clarity fuel token economy, including transactions, staking,
#     slashing, and external data integration.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Initialize with access to the GovernanceKernelV2
#         # Assuming GovernanceKernelV2 is available in the Colab environment
#         if 'GovernanceKernelV2' in globals():
#              self.kernel = GovernanceKernelV2(ledger_path)
#         elif 'GovernanceKernel' in globals():
#              # Fallback to base kernel if V2 is not found
#              print("Warning: GovernanceKernelV2 not found. Using base GovernanceKernel.")
#              self.kernel = GovernanceKernel(ledger_path)
#         else:
#              # Mock kernel if neither is found (for isolated testing)
#              print("Error: Neither GovernanceKernelV2 nor GovernanceKernel found. Using a mock kernel.")
#              class MockKernel:
#                   def evaluate(self, event):
#                       print(f"Mock Kernel received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
#                       # Return a mock receipt with an ID for potential tracing
#                       return {"id": "mock_" + str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#              self.kernel = MockKernel()
# 
# 
#         self.balances: Dict[str, float] = defaultdict(float) # Using defaultdict for balances
#         self.staked_balances: Dict[str, float] = defaultdict(float) # To track total staked fuel per agent
#         # New: Track individual stakes linked to assessment IDs
#         # Format: {agent_id: {assessment_id: staked_amount}}
#         self.stakes_by_assessment: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))
#         # Optional: Track total staked amount per assessment ID for quick lookup
#         self.total_staked_on_assessment: Dict[str, float] = defaultdict(float)
# 
#         self.transaction_history: List[Dict[str, Any]] = [] # To track transactions for velocity
#         self.last_transaction_timestamp: Dict[str, float] = {} # To track last transaction time per agent
# 
# 
#         print("💰 Clarity Fuel Market initialized.")
# 
# 
#     # Basic transaction function (can be extended later)
#     def transfer_fuel(self, sender: str, recipient: str, amount: float):
#         """Transfers clarity fuel between agents."""
#         if amount <= 0:
#             print(f"❌ Transfer failed: Amount must be positive.")
#             return False
# 
#         if self.balances[sender] >= amount:
#             self.balances[sender] -= amount
#             self.balances[recipient] += amount
#             transaction_record = {
#                 "type": "fuel_transfer",
#                 "sender": sender,
#                 "recipient": recipient,
#                 "amount": round(amount, 3),
#                 "sender_balance": round(self.balances[sender], 3),
#                 "recipient_balance": round(self.balances[recipient], 3),
#                 "explanation": f"Fuel transfer from {sender} to {recipient}: {amount}"
#             }
#             # Log the transaction via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": transaction_record, "agent": sender})
# 
#             # Update transaction history and timestamp for predictive maintenance
#             self.transaction_history.append(transaction_record)
#             current_time = time.time() # Use epoch time for velocity calculation
#             self.last_transaction_timestamp[sender] = current_time
#             self.last_transaction_timestamp[recipient] = current_time
# 
# 
#             print(f"💸 {sender} transferred {round(amount, 3)} fuel to {recipient}.")
#             return True
#         else:
#             print(f"❌ Transfer failed: {sender} has insufficient funds ({self.balances[sender]:.3f} < {amount:.3f}).")
#             return False
# 
# 
#     def stake_fuel(self, agent: str, amount: float, assessment_id: str):
#         """Stakes clarity fuel on a specific contradiction assessment."""
#         if amount <= 0:
#             print(f"❌ Staking failed: Amount must be positive.")
#             return False
# 
#         if not assessment_id:
#             print("❌ Staking failed: assessment_id must be provided.")
#             return False
# 
#         if self.balances[agent] >= amount:
#             self.balances[agent] -= amount
#             self.staked_balances[agent] += amount
#             self.stakes_by_assessment[agent][assessment_id] += amount
#             self.total_staked_on_assessment[assessment_id] += amount # Update total staked on this assessment
# 
#             staking_record = {
#                 "type": "fuel_stake",
#                 "agent": agent,
#                 "amount": round(amount, 3),
#                 "assessment_id": assessment_id,
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} staked {amount} fuel on assessment {assessment_id}."
#             }
#             # Log the staking event via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": staking_record, "agent": agent})
#             print(f"✅ {agent} staked {round(amount, 3)} fuel on assessment {assessment_id}.")
#             return True
#         else:
#             print(f"❌ Staking failed: {agent} has insufficient funds ({self.balances[agent]:.3f} < {amount:.3f}).")
#             return False
# 
#     def unstake_fuel(self, agent: str, amount: float):
#         """Unstakes clarity fuel. Note: This unstakes from the agent's total staked balance.
#            Specific stakes linked to assessments need a separate mechanism for withdrawal
#            or are released/slashed via assessment outcome."""
#         if amount <= 0:
#             print(f"❌ Unstaking failed: Amount must be positive.")
#             return False
# 
#         # For simplicity here, unstaking reduces the agent's total staked balance.
#         # A more complex system would require specifying *which* stake to unstake,
#         # especially if stakes are locked until assessment resolution.
#         # We'll assume this is a request to withdraw from the general staked pool,
#         # not necessarily tied to a specific assessment outcome yet.
#         if self.staked_balances[agent] >= amount:
#             self.staked_balances[agent] -= amount
#             self.balances[agent] += amount
#             unstaking_record = {
#                 "type": "fuel_unstake",
#                 "agent": agent,
#                 "amount": round(amount, 3),
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} unstaked {amount} fuel."
#             }
#              # Log the unstaking event via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": unstaking_record, "agent": agent})
#             print(f"✅ {agent} unstaked {round(amount, 3)} fuel.")
#             return True
#         else:
#             print(f"❌ Unstaking failed: {agent} has insufficient staked fuel ({self.staked_balances[agent]:.3f} < {amount:.3f}).")
#             return False
# 
# 
#     def slash_fuel(self, agent: str, amount: float, reason: str, assessment_id: str = None):
#         """Slashes staked fuel due to inaccurate assessments."""
#         if amount <= 0:
#             print(f"❌ Slashing failed: Amount must be positive.")
#             return False
# 
#         # Find the amount staked by this agent on this specific assessment, if provided
#         staked_on_assessment = 0.0
#         if assessment_id and agent in self.stakes_by_assessment and assessment_id in self.stakes_by_assessment[agent]:
#              staked_on_assessment = self.stakes_by_assessment[agent][assessment_id]
# 
#         # The amount to slash should not exceed the total staked by the agent,
#         # and if assessment_id is given, it shouldn't exceed the amount staked on that assessment.
#         actual_slash_amount = min(amount, self.staked_balances[agent])
#         if assessment_id:
#             actual_slash_amount = min(actual_slash_amount, staked_on_assessment)
# 
#         if actual_slash_amount > 0:
#             self.staked_balances[agent] -= actual_slash_amount
# 
#             # Reduce the specific stake amount if assessment_id is provided
#             if assessment_id and agent in self.stakes_by_assessment and assessment_id in self.stakes_by_assessment[agent]:
#                  self.stakes_by_assessment[agent][assessment_id] -= actual_slash_amount
#                  if self.stakes_by_assessment[agent][assessment_id] <= 1e-9: # Clean up small floating point remainders
#                       del self.stakes_by_assessment[agent][assessment_id]
#                  if not self.stakes_by_assessment[agent]: # Clean up agent entry if no more stakes
#                       del self.stakes_by_assessment[agent]
# 
#                  # Also update total staked on assessment
#                  self.total_staked_on_assessment[assessment_id] -= actual_slash_amount
#                  if self.total_staked_on_assessment[assessment_id] <= 1e-9: # Clean up small floating point remainders
#                       del self.total_staked_on_assessment[assessment_id]
# 
# 
#             slashing_record = {
#                 "type": "fuel_slash",
#                 "agent": agent,
#                 "amount": round(actual_slash_amount, 3),
#                 "reason": reason,
#                 "assessment_id": assessment_id,
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} slashed {actual_slash_amount} fuel for reason: {reason} (Assessment ID: {assessment_id if assessment_id else 'N/A'})."
#             }
#             # Log the slashing event via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": slashing_record, "agent": agent})
#             print(f"🔪 {agent} slashed {round(actual_slash_amount, 3)} fuel for reason: {reason}.")
#             return True
#         else:
#             print(f"🚫 Slashing failed for {agent}: No staked fuel to slash (requested {amount:.3f}, available {self.staked_balances[agent]:.3f} total, {staked_on_assessment:.3f} on assessment {assessment_id if assessment_id else 'N/A'}).")
#             return False
# 
# 
#     # Placeholder for external API integration (ESG/Carbon Credits)
#     def integrate_external_data(self, data_type: str, source: str):
#         """Integrates data from external APIs (ESG, Carbon Credits, etc.)."""
#         # Implementation for external API integration will go here
#         print(f"🔌 External API integration for {data_type} from {source} is not yet implemented.")
#         pass
# 
#     # Placeholder for predictive maintenance
#     def predictive_maintenance(self):
#         """Detects decline in transaction velocity or other anomalies."""
#         # Implementation for predictive maintenance will go here
#         print("⚠️ Predictive maintenance logic is not yet implemented.")
#         pass
# 
#     def get_status(self) -> Dict[str, Dict[str, float]]:
#         """Return current balances and staked balances."""
#         status = {}
#         all_agents = set(list(self.balances.keys()) + list(self.staked_balances.keys()))
#         for agent in all_agents:
#             status[agent] = {
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3)
#             }
#         return status
# 
#     def get_stakes_by_assessment(self, assessment_id: str) -> Dict[str, float]:
#         """Return stakes for a specific assessment."""
#         return dict(self.stakes_by_assessment.get(assessment_id, {})) # Return copy
# 
#     def get_total_staked_on_assessment(self, assessment_id: str) -> float:
#         """Return total fuel staked on a specific assessment."""
#         return round(self.total_staked_on_assessment.get(assessment_id, 0.0), 3)
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#      # Ensure necessary classes are available for the demo
#      if 'GovernanceKernelV2' not in globals():
#          print("GovernanceKernelV2 not found. Using a mock setup for demonstration.")
#          class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#              def append(self, record):
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = "mock"
#                   record["hash"] = "mock"
#                   record["id"] = str(uuid.uuid4()) # Add a mock ID
#                   with self.path.open("a") as f:
#                        f.write(json.dumps(record) + "\n")
#                   return record
#              def verify(self):
#                   return True
#          class MockReceiptWriter: # Define ReceiptWriter if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.ledger = MockLedger(ledger_path)
#               def log(self, event_type, data):
#                    return self.ledger.append({"event_type": event_type, "data": data})
#               def verify_ledger(self):
#                    return self.ledger.verify()
#          class GovernanceKernelV2: # Define GovernanceKernelV2 if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.writer = MockReceiptWriter(ledger_path)
#                    self.agent_reputation = defaultdict(lambda: 0.5)
#               def evaluate(self, event):
#                    print(f"Mock GovernanceKernelV2 received event: {event['event_type']}")
#                    # Simulate reputation update if event data has metabolism info and agent
#                    if "metabolism" in event.get("data", {}).get("type", "") and event.get("agent"):
#                         agent = event["agent"]
#                         clarity_gain = event["data"].get("clarity", 0.0)
#                         entropy_burn = event["data"].get("entropy", 0.0)
#                         change = (clarity_gain * 0.1) - (entropy_burn * 0.05)
#                         new_reputation = self.agent_reputation[agent] + change
#                         self.agent_reputation[agent] = max(0.0, min(1.0, new_reputation))
#                         print(f"Mock Reputation update for {agent}: {self.agent_reputation[agent]:.3f}")
#                    # Return a mock receipt with an ID
#                    return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#               def get_reputation(self, agent):
#                    return self.agent_reputation.get(agent, 0.5)
#          import time # Import time for mock timestamp
#          import uuid # Import uuid for mock ID
# 
# 
#      market = ClarityMarket()
# 
#      # Seed some initial balances for demonstration
#      market.balances["AgentA"] = 100.0
#      market.balances["AgentB"] = 50.0
#      market.balances["AgentC"] = 200.0
# 
#      print("\nInitial Balances:", json.dumps(market.get_status(), indent=2))
# 
#      # Simulate an assessment ID
#      assessment_1_id = str(uuid.uuid4())
#      assessment_2_id = str(uuid.uuid4())
# 
#      # Demonstrate staking on assessments
#      print(f"\nDemonstrating staking on assessments (ID 1: {assessment_1_id}, ID 2: {assessment_2_id}):")
#      market.stake_fuel("AgentA", 20.0, assessment_1_id) # AgentA stakes 20 on assessment 1
#      market.stake_fuel("AgentB", 10.0, assessment_1_id) # AgentB stakes 10 on assessment 1
#      market.stake_fuel("AgentA", 15.0, assessment_2_id) # AgentA stakes 15 on assessment 2
#      market.stake_fuel("AgentC", 50.0, assessment_1_id) # AgentC stakes 50 on assessment 1
#      market.stake_fuel("AgentB", 30.0, assessment_2_id) # AgentB stakes 30 on assessment 2 (should fail, only 40 balance after initial seed)
# 
#      print("\nBalances after staking:", json.dumps(market.get_status(), indent=2))
#      print(f"\nStakes on Assessment {assessment_1_id}:", dict(market.stakes_by_assessment.get("AgentA", {}).get(assessment_1_id, 0),
#                                                                     market.stakes_by_assessment.get("AgentB", {}).get(assessment_1_id, 0),
#                                                                     market.stakes_by_assessment.get("AgentC", {}).get(assessment_1_id, 0)))
#      print(f"Total staked on Assessment {assessment_1_id}: {market.get_total_staked_on_assessment(assessment_1_id)}")
# 
#      print(f"\nStakes on Assessment {assessment_2_id}:", dict(market.stakes_by_assessment.get("AgentA", {}).get(assessment_2_id, 0),
#                                                                     market.stakes_by_assessment.get("AgentB", {}).get(assessment_2_id, 0)))
#      print(f"Total staked on Assessment {assessment_2_id}: {market.get_total_staked_on_assessment(assessment_2_id)}")
# 
# 
#      # Demonstrate slashing fuel based on assessment outcome (simulated)
#      print("\nDemonstrating slashing fuel:")
#      # Simulate assessment 1 being deemed inaccurate
#      print(f"\nAssessment {assessment_1_id} deemed inaccurate. Slashing agents who staked on it.")
#      # Slash AgentA's stake on assessment 1 (20 fuel staked)
#      market.slash_fuel("AgentA", 20.0, "Staked on inaccurate assessment", assessment_id=assessment_1_id)
#       # Slash AgentB's stake on assessment 1 (10 fuel staked)
#      market.slash_fuel("AgentB", 10.0, "Staked on inaccurate assessment", assessment_id=assessment_1_id)
#       # Slash AgentC's stake on assessment 1 (50 fuel staked)
#      market.slash_fuel("AgentC", 50.0, "Staked on inaccurate assessment", assessment_id=assessment_1_id)
# 
#      # Try to slash AgentA more than they staked on assessment 2
#      market.slash_fuel("AgentA", 20.0, "Attempted over-slashing on assessment 2", assessment_id=assessment_2_id) # Should only slash 15
# 
#      # Try to slash an agent with no stake
#      market.slash_fuel("AgentD", 10.0, "Slashing attempt on agent with no stake", assessment_id=assessment_1_id) # Should fail
# 
#      print("\nBalances after slashing:", json.dumps(market.get_status(), indent=2))
#      print(f"\nStakes on Assessment {assessment_1_id} after slashing:", dict(market.stakes_by_assessment.get("AgentA", {}).get(assessment_1_id, 0),
#                                                                     market.stakes_by_assessment.get("AgentB", {}).get(assessment_1_id, 0),
#                                                                     market.stakes_by_assessment.get("AgentC", {}).get(assessment_1_id, 0)))
#      print(f"Total staked on Assessment {assessment_1_id} after slashing: {market.get_total_staked_on_assessment(assessment_1_id)}")
# 
#      print(f"\nStakes on Assessment {assessment_2_id} after slashing:", dict(market.stakes_by_assessment.get("AgentA", {}).get(assessment_2_id, 0),
#                                                                     market.stakes_by_assessment.get("AgentB", {}).get(assessment_2_id, 0)))
#      print(f"Total staked on Assessment {assessment_2_id} after slashing: {market.get_total_staked_on_assessment(assessment_2_id)}")
# 
# 
#      # Demonstrate remaining placeholders
#      market.transfer_fuel("AgentA", "AgentB", 5.0)
#      market.unstake_fuel("AgentA", 5.0) # Unstake from remaining staked on assessment 2 (should be 10)
#      market.integrate_external_data("ESG", "some_api.com")
#      market.predictive_maintenance()
# 
# 
#      print("\nClarity Market staking and slashing demo complete.")

"""## Integrate external apis

### Subtask:
Add methods to interact with mock external ESG or carbon-credit APIs.

**Reasoning**:
Implement the `integrate_external_data` method to simulate fetching and logging external data, and update the demonstration to call this method.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile clarity_market.py
# """
# Tessrax Clarity Fuel Economy v2.0 - Clarity Market
# --------------------------------------------------
# Introduces a token-based clarity fuel marketplace, allowing staking and slashing,
# integration with external APIs, and predictive maintenance based on transaction velocity.
# """
# 
# import json
# from typing import Dict, Any, List
# from collections import defaultdict
# import time # Import time for timestamp
# import uuid # Import uuid for logging
# import random # Import random for mock data
# 
# # GovernanceKernelV2 should be available in the environment from previous steps
# # from governance_kernel import GovernanceKernelV2
# # ReceiptWriter should also be available
# # from receipts import ReceiptWriter
# 
# class ClarityMarket:
#     """
#     Manages the clarity fuel token economy, including transactions, staking,
#     slashing, and external data integration.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Initialize with access to the GovernanceKernelV2
#         # Assuming GovernanceKernelV2 is available in the Colab environment
#         if 'GovernanceKernelV2' in globals():
#              self.kernel = GovernanceKernelV2(ledger_path)
#         elif 'GovernanceKernel' in globals():
#              # Fallback to base kernel if V2 is not found
#              print("Warning: GovernanceKernelV2 not found. Using base GovernanceKernel.")
#              self.kernel = GovernanceKernel(ledger_path)
#         else:
#              # Mock kernel if neither is found (for isolated testing)
#              print("Error: Neither GovernanceKernelV2 nor GovernanceKernel found. Using a mock kernel.")
#              class MockKernel:
#                   def evaluate(self, event):
#                       print(f"Mock Kernel received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
#                       # Return a mock receipt with an ID for potential tracing
#                       return {"id": "mock_" + str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#              self.kernel = MockKernel()
# 
# 
#         self.balances: Dict[str, float] = defaultdict(float) # Using defaultdict for balances
#         self.staked_balances: Dict[str, float] = defaultdict(float) # To track total staked fuel per agent
#         # New: Track individual stakes linked to assessment IDs
#         # Format: {agent_id: {assessment_id: staked_amount}}
#         self.stakes_by_assessment: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))
#         # Optional: Track total staked amount per assessment ID for quick lookup
#         self.total_staked_on_assessment: Dict[str, float] = defaultdict(float)
# 
#         self.transaction_history: List[Dict[str, Any]] = [] # To track transactions for velocity
#         self.last_transaction_timestamp: Dict[str, float] = {} # To track last transaction time per agent
# 
# 
#         print("💰 Clarity Fuel Market initialized.")
# 
# 
#     # Basic transaction function (can be extended later)
#     def transfer_fuel(self, sender: str, recipient: str, amount: float):
#         """Transfers clarity fuel between agents."""
#         if amount <= 0:
#             print(f"❌ Transfer failed: Amount must be positive.")
#             return False
# 
#         if self.balances[sender] >= amount:
#             self.balances[sender] -= amount
#             self.balances[recipient] += amount
#             transaction_record = {
#                 "type": "fuel_transfer",
#                 "sender": sender,
#                 "recipient": recipient,
#                 "amount": round(amount, 3),
#                 "sender_balance": round(self.balances[sender], 3),
#                 "recipient_balance": round(self.balances[recipient], 3),
#                 "explanation": f"Fuel transfer from {sender} to {recipient}: {amount}"
#             }
#             # Log the transaction via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": transaction_record, "agent": sender})
# 
#             # Update transaction history and timestamp for predictive maintenance
#             self.transaction_history.append(transaction_record)
#             current_time = time.time() # Use epoch time for velocity calculation
#             self.last_transaction_timestamp[sender] = current_time
#             self.last_transaction_timestamp[recipient] = current_time
# 
# 
#             print(f"💸 {sender} transferred {round(amount, 3)} fuel to {recipient}.")
#             return True
#         else:
#             print(f"❌ Transfer failed: {sender} has insufficient funds ({self.balances[sender]:.3f} < {amount:.3f}).")
#             return False
# 
# 
#     def stake_fuel(self, agent: str, amount: float, assessment_id: str):
#         """Stakes clarity fuel on a specific contradiction assessment."""
#         if amount <= 0:
#             print(f"❌ Staking failed: Amount must be positive.")
#             return False
# 
#         if not assessment_id:
#             print("❌ Staking failed: assessment_id must be provided.")
#             return False
# 
#         if self.balances[agent] >= amount:
#             self.balances[agent] -= amount
#             self.staked_balances[agent] += amount
#             self.stakes_by_assessment[agent][assessment_id] += amount
#             self.total_staked_on_assessment[assessment_id] += amount # Update total staked on this assessment
# 
#             staking_record = {
#                 "type": "fuel_stake",
#                 "agent": agent,
#                 "amount": round(amount, 3),
#                 "assessment_id": assessment_id,
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} staked {amount} fuel on assessment {assessment_id}."
#             }
#             # Log the staking event via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": staking_record, "agent": agent})
#             print(f"✅ {agent} staked {round(amount, 3)} fuel on assessment {assessment_id}.")
#             return True
#         else:
#             print(f"❌ Staking failed: {agent} has insufficient funds ({self.balances[agent]:.3f} < {amount:.3f}).")
#             return False
# 
#     def unstake_fuel(self, agent: str, amount: float):
#         """Unstakes clarity fuel. Note: This unstakes from the agent's total staked balance.
#            Specific stakes linked to assessments need a separate mechanism for withdrawal
#            or are released/slashed via assessment outcome."""
#         if amount <= 0:
#             print(f"❌ Unstaking failed: Amount must be positive.")
#             return False
# 
#         # For simplicity here, unstaking reduces the agent's total staked balance.
#         # A more complex system would require specifying *which* stake to unstake,
#         # especially if stakes are locked until assessment resolution.
#         # We'll assume this is a request to withdraw from the general staked pool,
#         # not necessarily tied to a specific assessment outcome yet.
#         if self.staked_balances[agent] >= amount:
#             self.staked_balances[agent] -= amount
#             self.balances[agent] += amount
#             unstaking_record = {
#                 "type": "fuel_unstake",
#                 "agent": agent,
#                 "amount": round(amount, 3),
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} unstaked {amount} fuel."
#             }
#              # Log the unstaking event via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": unstaking_record, "agent": agent})
#             print(f"✅ {agent} unstaked {round(amount, 3)} fuel.")
#             return True
#         else:
#             print(f"❌ Unstaking failed: {agent} has insufficient staked fuel ({self.staked_balances[agent]:.3f} < {amount:.3f}).")
#             return False
# 
# 
#     def slash_fuel(self, agent: str, amount: float, reason: str, assessment_id: str = None):
#         """Slashes staked fuel due to inaccurate assessments."""
#         if amount <= 0:
#             print(f"❌ Slashing failed: Amount must be positive.")
#             return False
# 
#         # Find the amount staked by this agent on this specific assessment, if provided
#         staked_on_assessment = 0.0
#         if assessment_id and agent in self.stakes_by_assessment and assessment_id in self.stakes_by_assessment[agent]:
#              staked_on_assessment = self.stakes_by_assessment[agent][assessment_id]
# 
#         # The amount to slash should not exceed the total staked by the agent,
#         # and if assessment_id is given, it shouldn't exceed the amount staked on that assessment.
#         actual_slash_amount = min(amount, self.staked_balances[agent])
#         if assessment_id:
#             actual_slash_amount = min(actual_slash_amount, staked_on_assessment)
# 
#         if actual_slash_amount > 0:
#             self.staked_balances[agent] -= actual_slash_amount
# 
#             # Reduce the specific stake amount if assessment_id is provided
#             if assessment_id and agent in self.stakes_by_assessment and assessment_id in self.stakes_by_assessment[agent]:
#                  self.stakes_by_assessment[agent][assessment_id] -= actual_slash_amount
#                  if self.stakes_by_assessment[agent][assessment_id] <= 1e-9: # Clean up small floating point remainders
#                       del self.stakes_by_assessment[agent][assessment_id]
#                  if not self.stakes_by_assessment[agent]: # Clean up agent entry if no more stakes
#                       del self.stakes_by_assessment[agent]
# 
#                  # Also update total staked on assessment
#                  self.total_staked_on_assessment[assessment_id] -= actual_slash_amount
#                  if self.total_staked_on_assessment[assessment_id] <= 1e-9: # Clean up small floating point remainders
#                       del self.total_staked_on_assessment[assessment_id]
# 
# 
#             slashing_record = {
#                 "type": "fuel_slash",
#                 "agent": agent,
#                 "amount": round(actual_slash_amount, 3),
#                 "reason": reason,
#                 "assessment_id": assessment_id,
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} slashed {actual_slash_amount} fuel for reason: {reason} (Assessment ID: {assessment_id if assessment_id else 'N/A'})."
#             }
#             # Log the slashing event via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": slashing_record, "agent": agent})
#             print(f"🔪 {agent} slashed {round(actual_slash_amount, 3)} fuel for reason: {reason}.")
#             return True
#         else:
#             print(f"🚫 Slashing failed for {agent}: No staked fuel to slash (requested {amount:.3f}, available {self.staked_balances[agent]:.3f} total, {staked_on_assessment:.3f} on assessment {assessment_id if assessment_id else 'N/A'}).")
#             return False
# 
# 
#     def integrate_external_data(self, data_type: str, source: str) -> Dict[str, Any]:
#         """
#         Simulates fetching data from external APIs (ESG, Carbon Credits, etc.)
#         and logs the event. Returns mock data.
#         """
#         print(f"🔌 Attempting to integrate external data: Type='{data_type}', Source='{source}'")
# 
#         # Simulate fetching data based on data_type
#         mock_data: Dict[str, Any] = {}
#         if data_type.lower() == "esg":
#             mock_data = {
#                 "type": "esg_score",
#                 "source": source,
#                 "score": round(random.uniform(0, 100), 2),
#                 "category_scores": {
#                     "environmental": round(random.uniform(0, 100), 2),
#                     "social": round(random.uniform(0, 100), 2),
#                     "governance": round(random.uniform(0, 100), 2)
#                 },
#                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#             }
#             print(f"   Simulated ESG data fetched from {source}.")
#         elif data_type.lower() == "carboncredit":
#             mock_data = {
#                 "type": "carbon_credit_value",
#                 "source": source,
#                 "value_per_ton": round(random.uniform(10, 200), 2),
#                 "unit": "USD/ton",
#                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#             }
#             print(f"   Simulated Carbon Credit data fetched from {source}.")
#         else:
#             mock_data = {
#                 "type": "unknown_external_data",
#                 "source": source,
#                 "payload": {"status": "simulated_data_not_recognized"},
#                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#             }
#             print(f"   Simulated unknown external data fetched from {source}.")
# 
#         integration_record = {
#             "type": "external_data_integration",
#             "data_type": data_type,
#             "source": source,
#             "payload": mock_data, # Include the mock data payload
#             "explanation": f"Integrated external data of type '{data_type}' from '{source}'."
#         }
# 
#         # Log the integration event via the kernel
#         logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": integration_record, "agent": "System"}) # Attribute to System agent
# 
#         return mock_data
# 
# 
#     # Placeholder for predictive maintenance
#     def predictive_maintenance(self):
#         """Detects decline in transaction velocity or other anomalies."""
#         # Implementation for predictive maintenance will go here
#         print("⚠️ Predictive maintenance logic is not yet implemented.")
#         pass
# 
#     def get_status(self) -> Dict[str, Dict[str, float]]:
#         """Return current balances and staked balances."""
#         status = {}
#         all_agents = set(list(self.balances.keys()) + list(self.staked_balances.keys()))
#         for agent in all_agents:
#             status[agent] = {
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3)
#             }
#         return status
# 
#     def get_stakes_by_assessment(self, assessment_id: str) -> Dict[str, float]:
#         """Return stakes for a specific assessment."""
#         # Note: self.stakes_by_assessment is agent -> assessment -> amount
#         # The request seems to imply assessment -> {agent: amount}, so let's adapt
#         stakes_on_id: Dict[str, float] = {}
#         for agent, stakes in self.stakes_by_assessment.items():
#             if assessment_id in stakes:
#                  stakes_on_id[agent] = stakes[assessment_id]
#         return stakes_on_id
# 
# 
#     def get_total_staked_on_assessment(self, assessment_id: str) -> float:
#         """Return total fuel staked on a specific assessment."""
#         # Recalculate from stakes_by_assessment to be robust
#         total = sum(self.stakes_by_assessment[agent].get(assessment_id, 0.0) for agent in self.stakes_by_assessment)
#         return round(total, 3)
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#      # Ensure necessary classes are available for the demo
#      if 'GovernanceKernelV2' not in globals():
#          print("GovernanceKernelV2 not found. Using a mock setup for demonstration.")
#          class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#              def append(self, record):
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = "mock"
#                   record["hash"] = "mock"
#                   record["id"] = str(uuid.uuid4()) # Add a mock ID
#                   with self.path.open("a") as f:
#                        f.write(json.dumps(record) + "\n")
#                   return record
#              def verify(self):
#                   return True
#          class MockReceiptWriter: # Define ReceiptWriter if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.ledger = MockLedger(ledger_path)
#               def log(self, event_type, data):
#                    return self.ledger.append({"event_type": event_type, "data": data})
#               def verify_ledger(self):
#                    return self.ledger.verify()
#          class GovernanceKernelV2: # Define GovernanceKernelV2 if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.writer = MockReceiptWriter(ledger_path)
#                    self.agent_reputation = defaultdict(lambda: 0.5)
#               def evaluate(self, event):
#                    print(f"Mock GovernanceKernelV2 received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
#                    # Simulate reputation update if event data has metabolism info and agent
#                    if "metabolism" in event.get("data", {}).get("type", "") and event.get("agent"):
#                         agent = event["agent"]
#                         clarity_gain = event["data"].get("clarity", 0.0)
#                         entropy_burn = event["data"].get("entropy", 0.0)
#                         change = (clarity_gain * 0.1) - (entropy_burn * 0.05)
#                         new_reputation = self.agent_reputation[agent] + change
#                         self.agent_reputation[agent] = max(0.0, min(1.0, new_reputation))
#                         print(f"Mock Reputation update for {agent}: {self.agent_reputation[agent]:.3f}")
#                    # Return a mock receipt with an ID
#                    return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#               def get_reputation(self, agent):
#                    return self.agent_reputation.get(agent, 0.5)
#          import time # Import time for mock timestamp
#          import uuid # Import uuid for mock ID
# 
# 
#      market = ClarityMarket()
# 
#      # Seed some initial balances for demonstration
#      market.balances["AgentA"] = 100.0
#      market.balances["AgentB"] = 50.0
#      market.balances["AgentC"] = 200.0
# 
#      print("\nInitial Balances:", json.dumps(market.get_status(), indent=2))
# 
#      # Simulate an assessment ID for staking demo
#      assessment_1_id = str(uuid.uuid4())
# 
#      # Demonstrate staking on assessment
#      market.stake_fuel("AgentA", 20.0, assessment_1_id)
# 
#      # Demonstrate external data integration
#      print("\nDemonstrating external data integration:")
#      esg_data = market.integrate_external_data("ESG", "https://api.example.com/esg/company_xyz")
#      print("Fetched ESG Data:", json.dumps(esg_data, indent=2))
# 
#      carbon_data = market.integrate_external_data("CarbonCredit", "https://api.carbonmarket.net/spot_prices")
#      print("Fetched Carbon Credit Data:", json.dumps(carbon_data, indent=2))
# 
#      unknown_data = market.integrate_external_data("Weather", "https://api.weather.com/forecast")
#      print("Fetched Unknown Data:", json.dumps(unknown_data, indent=2))
# 
#      # Demonstrate remaining placeholders (staking/slashing demo is above)
#      market.transfer_fuel("AgentA", "AgentB", 5.0)
#      market.unstake_fuel("AgentA", 5.0)
#      market.slash_fuel("AgentA", 10.0, "Inaccurate assessment", assessment_id=assessment_1_id)
#      market.predictive_maintenance()
# 
# 
#      print("\nClarity Market external data integration demo complete.")

"""## Implement predictive maintenance

### Subtask:
Implement the `predictive_maintenance()` function to analyze transaction velocity from the ledger and detect declines.

**Reasoning**:
Implement the predictive_maintenance function to analyze transaction velocity from the ledger and detect declines, logging an alert if necessary.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile clarity_market.py
# """
# Tessrax Clarity Fuel Economy v2.0 - Clarity Market
# --------------------------------------------------
# Introduces a token-based clarity fuel marketplace, allowing staking and slashing,
# integration with external APIs, and predictive maintenance based on transaction velocity.
# """
# 
# import json
# from typing import Dict, Any, List
# from collections import defaultdict
# import time # Import time for timestamp
# import uuid # Import uuid for logging
# import random # Import random for mock data
# from datetime import datetime, timedelta # Import datetime and timedelta for time window analysis
# 
# # GovernanceKernelV2 should be available in the environment from previous steps
# # from governance_kernel import GovernanceKernelV2
# # ReceiptWriter should also be available
# # from receipts import ReceiptWriter
# 
# class ClarityMarket:
#     """
#     Manages the clarity fuel token economy, including transactions, staking,
#     slashing, and external data integration.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Initialize with access to the GovernanceKernelV2
#         # Assuming GovernanceKernelV2 is available in the Colab environment
#         if 'GovernanceKernelV2' in globals():
#              self.kernel = GovernanceKernelV2(ledger_path)
#         elif 'GovernanceKernel' in globals():
#              # Fallback to base kernel if V2 is not found
#              print("Warning: GovernanceKernelV2 not found. Using base GovernanceKernel.")
#              self.kernel = GovernanceKernel(ledger_path)
#         else:
#              # Mock kernel if neither is found (for isolated testing)
#              print("Error: Neither GovernanceKernelV2 nor GovernanceKernel found. Using a mock kernel.")
#              class MockKernel:
#                   def evaluate(self, event):
#                       print(f"Mock Kernel received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
#                       # Return a mock receipt with an ID for potential tracing
#                       return {"id": "mock_" + str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#              self.kernel = MockKernel()
# 
# 
#         self.balances: Dict[str, float] = defaultdict(float) # Using defaultdict for balances
#         self.staked_balances: Dict[str, float] = defaultdict(float) # To track total staked fuel per agent
#         # New: Track individual stakes linked to assessment IDs
#         # Format: {agent_id: {assessment_id: staked_amount}}
#         self.stakes_by_assessment: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))
#         # Optional: Track total staked amount per assessment ID for quick lookup
#         self.total_staked_on_assessment: Dict[str, float] = defaultdict(float)
# 
#         self.transaction_history: List[Dict[str, Any]] = [] # To track transactions for velocity
#         # self.last_transaction_timestamp: Dict[str, float] = {} # No longer strictly needed with ledger reading
# 
#         # Predictive maintenance parameters
#         self.velocity_time_window = timedelta(minutes=5) # Analyze transactions in the last 5 minutes
#         self.velocity_threshold_transactions_per_window = 2 # Alert if fewer than 2 transactions in the window
#         self.last_maintenance_check = time.time() # Track last check time to avoid excessive checks
# 
# 
#         print("💰 Clarity Fuel Market initialized.")
# 
# 
#     # Basic transaction function (can be extended later)
#     def transfer_fuel(self, sender: str, recipient: str, amount: float):
#         """Transfers clarity fuel between agents."""
#         if amount <= 0:
#             print(f"❌ Transfer failed: Amount must be positive.")
#             return False
# 
#         if self.balances[sender] >= amount:
#             self.balances[sender] -= amount
#             self.balances[recipient] += amount
#             transaction_record = {
#                 "type": "fuel_transfer",
#                 "sender": sender,
#                 "recipient": recipient,
#                 "amount": round(amount, 3),
#                 "sender_balance": round(self.balances[sender], 3),
#                 "recipient_balance": round(self.balances[recipient], 3),
#                 "explanation": f"Fuel transfer from {sender} to {recipient}: {amount}"
#             }
#             # Log the transaction via the kernel
#             logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": transaction_record, "agent": sender})
# 
#             # Add to transaction history (optional, can rely solely on ledger)
#             if logged_receipt: # Only add if logging was successful (e.g., not mock kernel)
#                  self.transaction_history.append(logged_receipt)
# 
# 
#             print(f"💸 {sender} transferred {round(amount, 3)} fuel to {recipient}.")
#             return True
#         else:
#             print(f"❌ Transfer failed: {sender} has insufficient funds ({self.balances[sender]:.3f} < {amount:.3f}).")
#             return False
# 
# 
#     def stake_fuel(self, agent: str, amount: float, assessment_id: str):
#         """Stakes clarity fuel on a specific contradiction assessment."""
#         if amount <= 0:
#             print(f"❌ Staking failed: Amount must be positive.")
#             return False
# 
#         if not assessment_id:
#             print("❌ Staking failed: assessment_id must be provided.")
#             return False
# 
#         if self.balances[agent] >= amount:
#             self.balances[agent] -= amount
#             self.staked_balances[agent] += amount
#             self.stakes_by_assessment[agent][assessment_id] += amount
#             self.total_staked_on_assessment[assessment_id] += amount # Update total staked on this assessment
# 
#             staking_record = {
#                 "type": "fuel_stake",
#                 "agent": agent,
#                 "amount": round(amount, 3),
#                 "assessment_id": assessment_id,
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} staked {amount} fuel on assessment {assessment_id}."
#             }
#             # Log the staking event via the kernel
#             logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": staking_record, "agent": agent})
#             if logged_receipt: # Add to history if logged
#                  self.transaction_history.append(logged_receipt)
# 
#             print(f"✅ {agent} staked {round(amount, 3)} fuel on assessment {assessment_id}.")
#             return True
#         else:
#             print(f"❌ Staking failed: {agent} has insufficient funds ({self.balances[agent]:.3f} < {amount:.3f}).")
#             return False
# 
#     def unstake_fuel(self, agent: str, amount: float):
#         """Unstakes clarity fuel. Note: This unstakes from the agent's total staked balance.
#            Specific stakes linked to assessments need a separate mechanism for withdrawal
#            or are released/slashed via assessment outcome."""
#         if amount <= 0:
#             print(f"❌ Unstaking failed: Amount must be positive.")
#             return False
# 
#         # For simplicity here, unstaking reduces the agent's total staked balance.
#         # A more complex system would require specifying *which* stake to unstake,
#         # especially if stakes are locked until assessment resolution.
#         # We'll assume this is a request to withdraw from the general staked pool,
#         # not necessarily tied to a specific assessment outcome yet.
#         if self.staked_balances[agent] >= amount:
#             self.staked_balances[agent] -= amount
#             self.balances[agent] += amount
#             unstaking_record = {
#                 "type": "fuel_unstake",
#                 "agent": agent,
#                 "amount": round(amount, 3),
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} unstaked {amount} fuel."
#             }
#              # Log the unstaking event via the kernel
#             logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": unstaking_record, "agent": agent})
#             if logged_receipt: # Add to history if logged
#                  self.transaction_history.append(logged_receipt)
# 
#             print(f"✅ {agent} unstaked {round(amount, 3)} fuel.")
#             return True
#         else:
#             print(f"❌ Unstaking failed: {agent} has insufficient staked fuel ({self.staked_balances[agent]:.3f} < {amount:.3f}).")
#             return False
# 
# 
#     def slash_fuel(self, agent: str, amount: float, reason: str, assessment_id: str = None):
#         """Slashes staked fuel due to inaccurate assessments."""
#         if amount <= 0:
#             print(f"❌ Slashing failed: Amount must be positive.")
#             return False
# 
#         # Find the amount staked by this agent on this specific assessment, if provided
#         staked_on_assessment = 0.0
#         if assessment_id and agent in self.stakes_by_assessment and assessment_id in self.stakes_by_assessment[agent]:
#              staked_on_assessment = self.stakes_by_assessment[agent][assessment_id]
# 
#         # The amount to slash should not exceed the total staked by the agent,
#         # and if assessment_id is given, it shouldn't exceed the amount staked on that assessment.
#         actual_slash_amount = min(amount, self.staked_balances[agent])
#         if assessment_id:
#             actual_slash_amount = min(actual_slash_amount, staked_on_assessment)
# 
#         if actual_slash_amount > 0:
#             self.staked_balances[agent] -= actual_slash_amount
# 
#             # Reduce the specific stake amount if assessment_id is provided
#             if assessment_id and agent in self.stakes_by_assessment and assessment_id in self.stakes_by_assessment[agent]:
#                  self.stakes_by_assessment[agent][assessment_id] -= actual_slash_amount
#                  if self.stakes_by_assessment[agent][assessment_id] <= 1e-9: # Clean up small floating point remainders
#                       del self.stakes_by_assessment[agent][assessment_id]
#                  if not self.stakes_by_assessment[agent]: # Clean up agent entry if no more stakes
#                       del self.stakes_by_assessment[agent]
# 
#                  # Also update total staked on assessment
#                  self.total_staked_on_assessment[assessment_id] -= actual_slash_amount
#                  if self.total_staked_on_assessment[assessment_id] <= 1e-9: # Clean up small floating point remainders
#                       del self.total_staked_on_assessment[assessment_id]
# 
# 
#             slashing_record = {
#                 "type": "fuel_slash",
#                 "agent": agent,
#                 "amount": round(actual_slash_amount, 3),
#                 "reason": reason,
#                 "assessment_id": assessment_id,
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} slashed {actual_slash_amount} fuel for reason: {reason} (Assessment ID: {assessment_id if assessment_id else 'N/A'})."
#             }
#             # Log the slashing event via the kernel
#             logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": slashing_record, "agent": agent})
#             if logged_receipt: # Add to history if logged
#                  self.transaction_history.append(logged_receipt)
# 
#             print(f"🔪 {agent} slashed {round(actual_slash_amount, 3)} fuel for reason: {reason}.")
#             return True
#         else:
#             print(f"🚫 Slashing failed for {agent}: No staked fuel to slash (requested {amount:.3f}, available {self.staked_balances[agent]:.3f} total, {staked_on_assessment:.3f} on assessment {assessment_id if assessment_id else 'N/A'}).")
#             return False
# 
# 
#     def integrate_external_data(self, data_type: str, source: str) -> Dict[str, Any]:
#         """
#         Simulates fetching data from external APIs (ESG, Carbon Credits, etc.)
#         and logs the event. Returns mock data.
#         """
#         print(f"🔌 Attempting to integrate external data: Type='{data_type}', Source='{source}'")
# 
#         # Simulate fetching data based on data_type
#         mock_data: Dict[str, Any] = {}
#         if data_type.lower() == "esg":
#             mock_data = {
#                 "type": "esg_score",
#                 "source": source,
#                 "score": round(random.uniform(0, 100), 2),
#                 "category_scores": {
#                     "environmental": round(random.uniform(0, 100), 2),
#                     "social": round(random.uniform(0, 100), 2),
#                     "governance": round(random.uniform(0, 100), 2)
#                 },
#                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#             }
#             print(f"   Simulated ESG data fetched from {source}.")
#         elif data_type.lower() == "carboncredit":
#             mock_data = {
#                 "type": "carbon_credit_value",
#                 "source": source,
#                 "value_per_ton": round(random.uniform(10, 200), 2),
#                 "unit": "USD/ton",
#                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#             }
#             print(f"   Simulated Carbon Credit data fetched from {source}.")
#         else:
#             mock_data = {
#                 "type": "unknown_external_data",
#                 "source": source,
#                 "payload": {"status": "simulated_data_not_recognized"},
#                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#             }
#             print(f"   Simulated unknown external data fetched from {source}.")
# 
#         integration_record = {
#             "type": "external_data_integration",
#             "data_type": data_type,
#             "source": source,
#             "payload": mock_data, # Include the mock data payload
#             "explanation": f"Integrated external data of type '{data_type}' from '{source}'."
#         }
# 
#         # Log the integration event via the kernel
#         logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": integration_record, "agent": "System"}) # Attribute to System agent
#         if logged_receipt: # Add to history if logged
#              self.transaction_history.append(logged_receipt)
# 
#         return mock_data
# 
# 
#     def predictive_maintenance(self):
#         """
#         Detects decline in transaction velocity from the ledger
#         and logs an alert if necessary.
#         """
#         print("⚠️ Running predictive maintenance check...")
# 
#         # Load recent transactions from the ledger
#         # We need to read the actual ledger file to get timestamps
#         recent_transactions = []
#         ledger_path = f"/content/{self.kernel.writer.ledger.path.name}" # Get ledger path from kernel's writer
#         try:
#             with open(ledger_path, "r") as f:
#                 # Read all lines and filter by timestamp
#                 for line in f:
#                     try:
#                         entry = json.loads(line)
#                         # Consider only 'fuel_transfer', 'fuel_stake', 'fuel_unstake', 'fuel_slash' events as transactions
#                         if entry.get("event_type") == "SYSTEM_EVENT" and entry.get("data", {}).get("type") in ["fuel_transfer", "fuel_stake", "fuel_unstake", "fuel_slash"]:
#                             timestamp_str = entry.get("timestamp")
#                             if timestamp_str:
#                                 # Parse timestamp string (e.g., "YYYY-MM-DDTHH:MM:SSZ")
#                                 timestamp = datetime.strptime(timestamp_str, "%Y-%m-%dT%H:%M:%SZ")
#                                 # Check if the transaction is within the time window
#                                 if datetime.utcnow() - timestamp <= self.velocity_time_window:
#                                     recent_transactions.append(entry)
#                     except (json.JSONDecodeError, ValueError) as e:
#                         print(f"Error parsing ledger entry or timestamp: {e}")
#                         continue # Skip invalid lines
# 
#         except FileNotFoundError:
#             print(f"Ledger file not found at {ledger_path}. Cannot check velocity.")
#             current_velocity = 0
#         except Exception as e:
#             print(f"An error occurred while reading the ledger: {e}")
#             current_velocity = 0 # Assume 0 velocity on error
# 
#         current_velocity = len(recent_transactions)
#         print(f"   Detected {current_velocity} transactions in the last {self.velocity_time_window}.")
# 
#         # Check if velocity falls below the threshold
#         if current_velocity < self.velocity_threshold_transactions_per_window:
#             alert_message = f"Transaction velocity decline detected! Only {current_velocity} transactions in the last {self.velocity_time_window}, below threshold of {self.velocity_threshold_transactions_per_window}."
#             print(f"🚨 {alert_message}")
#             alert_record = {
#                 "type": "predictive_maintenance_alert",
#                 "metric": "transaction_velocity",
#                 "current_velocity": current_velocity,
#                 "threshold": self.velocity_threshold_transactions_per_window,
#                 "time_window_seconds": self.velocity_time_window.total_seconds(),
#                 "explanation": alert_message
#             }
#             # Log the alert via the kernel
#             self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": alert_record, "agent": "PredictiveMaintenance"})
#         else:
#             print("✅ Transaction velocity is healthy.")
# 
#         # Update last check time (optional, could be used to control check frequency)
#         self.last_maintenance_check = time.time()
# 
# 
#     def get_status(self) -> Dict[str, Dict[str, float]]:
#         """Return current balances and staked balances."""
#         status = {}
#         all_agents = set(list(self.balances.keys()) + list(self.staked_balances.keys()))
#         for agent in all_agents:
#             status[agent] = {
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3)
#             }
#         return status
# 
#     def get_stakes_by_assessment(self, assessment_id: str) -> Dict[str, float]:
#         """Return stakes for a specific assessment."""
#         # Note: self.stakes_by_assessment is agent -> assessment -> amount
#         # The request seems to imply assessment -> {agent: amount}, so let's adapt
#         stakes_on_id: Dict[str, float] = {}
#         for agent, stakes in self.stakes_by_assessment.items():
#             if assessment_id in stakes:
#                  stakes_on_id[agent] = stakes[assessment_id]
#         return stakes_on_id
# 
# 
#     def get_total_staked_on_assessment(self, assessment_id: str) -> float:
#         """Return total fuel staked on a specific assessment."""
#         # Recalculate from stakes_by_assessment to be robust
#         total = sum(self.stakes_by_assessment[agent].get(assessment_id, 0.0) for agent in self.stakes_by_assessment)
#         return round(total, 3)
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#      # Ensure necessary classes are available for the demo
#      if 'GovernanceKernelV2' not in globals():
#          print("GovernanceKernelV2 not found. Using a mock setup for demonstration.")
#          class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#                   self._entries = [] # Keep entries in memory for mock reading
#              def append(self, record):
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = "mock"
#                   record["hash"] = "mock"
#                   record["id"] = str(uuid.uuid4()) # Add a mock ID
#                   self._entries.append(record) # Add to in-memory list
#                   # Simulate writing to file (optional for mock, but good for testing read logic)
#                   try:
#                        with self.path.open("a") as f:
#                             f.write(json.dumps(record) + "\n")
#                   except Exception as e:
#                        print(f"Mock Ledger write error: {e}")
#                   return record
#              def verify(self):
#                   return True
#              def read_all(self): # Helper to read all entries for mock velocity check
#                   # In a real scenario, you'd read from the file
#                   return self._entries
# 
#          class MockReceiptWriter: # Define ReceiptWriter if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.ledger = MockLedger(ledger_path)
#               def log(self, event_type, data):
#                    return self.ledger.append({"event_type": event_type, "data": data})
#               def verify_ledger(self):
#                    return self.ledger.verify()
#          class GovernanceKernelV2: # Define GovernanceKernelV2 if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.writer = MockReceiptWriter(ledger_path)
#                    self.agent_reputation = defaultdict(lambda: 0.5)
#               def evaluate(self, event):
#                    print(f"Mock GovernanceKernelV2 received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
#                    # Simulate reputation update if event data has metabolism info and agent
#                    if "metabolism" in event.get("data", {}).get("type", "") and event.get("agent"):
#                         agent = event["agent"]
#                         clarity_gain = event["data"].get("clarity", 0.0)
#                         entropy_burn = event["data"].get("entropy", 0.0)
#                         change = (clarity_gain * 0.1) - (entropy_burn * 0.05)
#                         new_reputation = self.agent_reputation[agent] + change
#                         self.agent_reputation[agent] = max(0.0, min(1.0, new_reputation))
#                         print(f"Mock Reputation update for {agent}: {self.agent_reputation[agent]:.3f}")
#                    # Return a mock receipt with an ID
#                    return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#               def get_reputation(self, agent):
#                    return self.agent_reputation.get(agent, 0.5)
#          import time # Import time for mock timestamp
#          import uuid # Import uuid for mock ID
#          from pathlib import Path # Import Path for mock ledger path
# 
# 
#      market = ClarityMarket()
# 
#      # Seed some initial balances for demonstration
#      market.balances["AgentA"] = 100.0
#      market.balances["AgentB"] = 50.0
# 
#      print("\nInitial Balances:", json.dumps(market.get_status(), indent=2))
# 
#      # --- Demonstrate Predictive Maintenance ---
#      print("\n--- Demonstrating Predictive Maintenance ---")
# 
#      # Simulate some recent transactions (within the time window)
#      print("\nSimulating recent transactions...")
#      market.transfer_fuel("AgentA", "AgentB", 5.0)
#      market.stake_fuel("AgentB", 10.0, str(uuid.uuid4()))
#      market.transfer_fuel("AgentB", "AgentA", 2.0)
#      market.unstake_fuel("AgentB", 5.0)
# 
#      # Run predictive maintenance check - should be healthy
#      print("\nRunning first maintenance check (should be healthy):")
#      market.predictive_maintenance()
# 
#      # Simulate a period of low transaction activity (wait longer than the time window)
#      print(f"\nSimulating low transaction activity (waiting {market.velocity_time_window.total_seconds() + 5} seconds)...")
#      # In a real scenario, time would naturally pass. For demo, we simulate it.
#      # We can't actually fast-forward time. Instead, we'll manually add older entries
#      # to the mock ledger if using the mock setup, or rely on the actual ledger
#      # if running in an environment where time passes between cell executions.
# 
#      # If using the mock ledger, add some old transactions manually
#      if isinstance(market.kernel.writer.ledger, MockLedger):
#           print("Adding old transactions to mock ledger...")
#           old_time = datetime.utcnow() - market.velocity_time_window - timedelta(minutes=1) # Outside the window
#           old_timestamp_str = old_time.strftime("%Y-%m-%dT%H:%M:%SZ")
#           old_transfer = {"event_type": "SYSTEM_EVENT", "data": {"type": "fuel_transfer", "sender": "AgentX", "recipient": "AgentY", "amount": 1.0}, "timestamp": old_timestamp_str, "id": str(uuid.uuid4())}
#           old_stake = {"event_type": "SYSTEM_EVENT", "data": {"type": "fuel_stake", "agent": "AgentZ", "amount": 2.0, "assessment_id": "old_assessment"}, "timestamp": old_timestamp_str, "id": str(uuid.uuid4())}
#           market.kernel.writer.ledger._entries.extend([old_transfer, old_stake])
#           # Need to rewrite the ledger file for the adapter to read it
#           try:
#                with open(ledger_path, "w") as f:
#                     for entry in market.kernel.writer.ledger._entries:
#                          f.write(json.dumps(entry) + "\n")
#                print("Mock ledger file updated with old entries.")
#           except Exception as e:
#                print(f"Error writing mock ledger file: {e}")
# 
#      else:
#           # If not using mock, rely on actual time passing between cell executions
#           # For a single cell execution demo, this wait won't work as intended
#           # time.sleep(market.velocity_time_window.total_seconds() + 5)
#           print("Skipping actual time sleep in this demonstration cell.")
# 
# 
#      # Run predictive maintenance check again - should detect decline (if mock old data added or enough time passed)
#      print("\nRunning second maintenance check (should detect decline):")
#      market.predictive_maintenance()
# 
#      print("\n--- Predictive Maintenance Demo Complete ---")
# 
# 
#      # Demonstrate remaining placeholders (staking/slashing/external data demos are above)
#      print("\nDemonstrating other placeholders:")
#      market.slash_fuel("AgentA", 1.0, "Simulated minor infraction")
#      market.integrate_external_data("ESG", "https://api.example.com/esg/another_company")
# 
#      print("\nClarity Market full demonstration complete.")

"""## Integrate with existing modules

### Subtask:
Ensure the `ClarityMarket` can interact with the `GovernanceKernelV2` and log transactions to the ledger.

**Reasoning**:
Review the `ClarityMarket` class and its demonstration code to ensure that all relevant events (`fuel_transfer`, `fuel_stake`, `fuel_unstake`, `fuel_slash`, `external_data_integration`, `predictive_maintenance_alert`) are being logged via `self.kernel.evaluate()`, that the necessary data for each event is included, and that the agent parameter is correctly passed for agent-specific events. Also, verify the mock `GovernanceKernelV2` handles these events and that the demonstration shows the logging.
"""

"""
Tessrax Clarity Fuel Economy v2.0 - Clarity Market
--------------------------------------------------
Introduces a token-based clarity fuel marketplace, allowing staking and slashing,
integration with external APIs, and predictive maintenance based on transaction velocity.
"""

import json
from typing import Dict, Any, List
from collections import defaultdict
import time # Import time for timestamp
import uuid # Import uuid for logging
import random # Import random for mock data
from datetime import datetime, timedelta # Import datetime and timedelta for time window analysis
from pathlib import Path # Import Path for ledger path in mock

# GovernanceKernelV2 should be available in the environment from previous steps
# from governance_kernel import GovernanceKernelV2
# ReceiptWriter should also be available
# from receipts import ReceiptWriter

class ClarityMarket:
    """
    Manages the clarity fuel token economy, including transactions, staking,
    slashing, and external data integration.
    """

    def __init__(self, ledger_path: str = "ledger.jsonl"):
        # Initialize with access to the GovernanceKernelV2
        # Assuming GovernanceKernelV2 is available in the Colab environment
        if 'GovernanceKernelV2' in globals():
             self.kernel = GovernanceKernelV2(ledger_path)
        elif 'GovernanceKernel' in globals():
             # Fallback to base kernel if V2 is not found
             print("Warning: GovernanceKernelV2 not found. Using base GovernanceKernel.")
             self.kernel = GovernanceKernel(ledger_path)
        else:
             # Mock kernel if neither is found (for isolated testing)
             print("Error: Neither GovernanceKernelV2 nor GovernanceKernel found. Using a mock kernel.")
             class MockKernel:
                  def evaluate(self, event):
                      print(f"Mock GovernanceKernel received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
                      # Simulate reputation update if event data has metabolism info and agent
                      if "metabolism" in event.get("data", {}).get("type", "") and event.get("agent"):
                           agent = event["agent"]
                           clarity_gain = event["data"].get("clarity", 0.0)
                           entropy_burn = event["data"].get("entropy", 0.0)
                           change = (clarity_gain * 0.1) - (entropy_burn * 0.05)
                           # Mock update - in a real kernel, this would update a persistent state
                           print(f"Mock Reputation update for {agent} simulated.")
                      # Return a mock receipt with an ID for potential tracing
                      return {"id": "mock_" + str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
             self.kernel = MockKernel()


        self.balances: Dict[str, float] = defaultdict(float) # Using defaultdict for balances
        self.staked_balances: Dict[str, float] = defaultdict(float) # To track total staked fuel per agent
        # New: Track individual stakes linked to assessment IDs
        # Format: {agent_id: {assessment_id: {'amount': float, 'timestamp': float}}} - Adding timestamp for potential lock-up
        self.stakes_by_assessment: Dict[str, Dict[str, Dict[str, float]]] = defaultdict(lambda: defaultdict(lambda: {'amount': 0.0, 'timestamp': 0.0}))
        # Optional: Track total staked amount per assessment ID for quick lookup
        self.total_staked_on_assessment: Dict[str, float] = defaultdict(float)

        # Transaction history is primarily read from the ledger now, but keeping a list for recent additions before ledger read is updated
        self.recent_logged_events: List[Dict[str, Any]] = []


        # Predictive maintenance parameters
        self.velocity_time_window = timedelta(seconds=30) # Analyze transactions in the last 30 seconds for demo speed
        self.velocity_threshold_transactions_per_window = 2 # Alert if fewer than 2 transactions in the window
        self.last_maintenance_check = time.time() # Track last check time to avoid excessive checks

        # Staking parameters
        self.staking_lockup_period = timedelta(seconds=10) # Example lock-up period for staked fuel


        print("💰 Clarity Fuel Market initialized.")


    # Basic transaction function (can be extended later)
    def transfer_fuel(self, sender: str, recipient: str, amount: float):
        """Transfers clarity fuel between agents."""
        if amount <= 0:
            print(f"❌ Transfer failed: Amount must be positive.")
            return False

        if self.balances[sender] >= amount:
            self.balances[sender] -= amount
            self.balances[recipient] += amount
            transaction_record = {
                "type": "fuel_transfer",
                "sender": sender,
                "recipient": recipient,
                "amount": round(amount, 3),
                "sender_balance": round(self.balances[sender], 3),
                "recipient_balance": round(self.balances[recipient], 3),
                "explanation": f"Fuel transfer from {sender} to {recipient}: {amount}"
            }
            # Log the transaction via the kernel
            logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": transaction_record, "agent": sender})

            # Add to recent events list if logged
            if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
                 self.recent_logged_events.append(logged_receipt)


            print(f"💸 {sender} transferred {round(amount, 3)} fuel to {recipient}.")
            return True
        else:
            print(f"❌ Transfer failed: {sender} has insufficient funds ({self.balances[sender]:.3f} < {amount:.3f}).")
            return False


    def stake_fuel(self, agent: str, amount: float, assessment_id: str):
        """Stakes clarity fuel on a specific contradiction assessment."""
        if amount <= 0:
            print(f"❌ Staking failed: Amount must be positive.")
            return False

        if not assessment_id:
            print("❌ Staking failed: assessment_id must be provided.")
            return False

        if self.balances[agent] >= amount:
            self.balances[agent] -= amount
            self.staked_balances[agent] += amount
            # Store amount and timestamp for this specific stake
            self.stakes_by_assessment[agent][assessment_id]['amount'] += amount
            self.stakes_by_assessment[agent][assessment_id]['timestamp'] = time.time() # Record staking time

            self.total_staked_on_assessment[assessment_id] += amount # Update total staked on this assessment

            staking_record = {
                "type": "fuel_stake",
                "agent": agent,
                "amount": round(amount, 3),
                "assessment_id": assessment_id,
                "balance": round(self.balances[agent], 3),
                "staked_balance": round(self.staked_balances[agent], 3),
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()), # Log timestamp of staking
                "explanation": f"Agent {agent} staked {amount} fuel on assessment {assessment_id}."
            }
            # Log the staking event via the kernel
            logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": staking_record, "agent": agent})
            if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
                 self.recent_logged_events.append(logged_receipt)

            print(f"✅ {agent} staked {round(amount, 3)} fuel on assessment {assessment_id}.")
            return True
        else:
            print(f"❌ Staking failed: {agent} has insufficient funds ({self.balances[agent]:.3f} < {amount:.3f}).")
            return False

    def unstake_fuel(self, agent: str, assessment_id: str = None, amount: float = None):
        """
        Unstakes clarity fuel. Can unstake a specific amount from a specific assessment
        after the lock-up period, or unstake any available fuel not locked.
        """
        if amount is not None and amount <= 0:
            print(f"❌ Unstaking failed: Amount must be positive.")
            return False

        if assessment_id:
            # Unstaking from a specific assessment stake
            if agent not in self.stakes_by_assessment or assessment_id not in self.stakes_by_assessment[agent]:
                print(f"❌ Unstaking failed: No stake found for agent {agent} on assessment {assessment_id}.")
                return False

            staked_info = self.stakes_by_assessment[agent][assessment_id]
            available_to_unstake = staked_info['amount']
            stake_timestamp = staked_info['timestamp']

            # Check for lock-up period
            if time.time() - stake_timestamp < self.staking_lockup_period.total_seconds():
                remaining_time = self.staking_lockup_period.total_seconds() - (time.time() - stake_timestamp)
                print(f"⏳ Unstaking failed: Stake on assessment {assessment_id} is locked for another {remaining_time:.2f} seconds.")
                return False

            # Determine the actual amount to unstake
            unstake_amount = amount if amount is not None else available_to_unstake
            if unstake_amount > available_to_unstake:
                print(f"❌ Unstaking failed: Requested {unstake_amount:.3f}, but only {available_to_unstake:.3f} is available to unstake on assessment {assessment_id}.")
                return False

            if unstake_amount > 0:
                self.staked_balances[agent] -= unstake_amount
                self.balances[agent] += unstake_amount
                self.stakes_by_assessment[agent][assessment_id]['amount'] -= unstake_amount
                self.total_staked_on_assessment[assessment_id] -= unstake_amount

                # Clean up if the entire stake is unstaked
                if self.stakes_by_assessment[agent][assessment_id]['amount'] <= 1e-9:
                    del self.stakes_by_assessment[agent][assessment_id]
                    if not self.stakes_by_assessment[agent]:
                        del self.stakes_by_assessment[agent]

                if self.total_staked_on_assessment[assessment_id] <= 1e-9:
                     del self.total_staked_on_assessment[assessment_id]


                unstaking_record = {
                    "type": "fuel_unstake",
                    "agent": agent,
                    "amount": round(unstake_amount, 3),
                    "assessment_id": assessment_id,
                    "balance": round(self.balances[agent], 3),
                    "staked_balance": round(self.staked_balances[agent], 3),
                    "explanation": f"Agent {agent} unstaked {unstake_amount} fuel from assessment {assessment_id}."
                }
                logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": unstaking_record, "agent": agent})
                if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
                     self.recent_logged_events.append(logged_receipt)

                print(f"✅ {agent} unstaked {round(unstake_amount, 3)} fuel from assessment {assessment_id}.")
                return True
            else:
                 print(f"❌ Unstaking failed: No amount specified or amount is zero.")
                 return False

        else:
            # Unstaking from general staked balance (only if not locked in assessments)
            # This requires knowing which part of the staked_balances is NOT tied to a locked assessment.
            # A simple approach for now: allow unstaking from the total staked balance,
            # assuming any locked stakes are handled separately. This is a simplification.
            # A more robust approach would track 'available_staked' vs 'locked_staked'.
            # For this iteration, we'll allow unstaking from the total staked balance,
            # but the check for locked stakes in the assessment-specific path is the primary mechanism.

            # Let's implement a simplified check: can only unstake from total staked if
            # there are no active, locked stakes on any assessment for this agent.
            has_locked_stakes = False
            if agent in self.stakes_by_assessment:
                 for ass_id, stake_info in self.stakes_by_assessment[agent].items():
                      if time.time() - stake_info['timestamp'] < self.staking_lockup_period.total_seconds():
                           has_locked_stakes = True
                           break

            if has_locked_stakes:
                 print(f"⏳ Unstaking failed: Agent {agent} has locked stakes on active assessments. Cannot unstake from general staked balance.")
                 return False

            unstake_amount = amount if amount is not None else self.staked_balances[agent]
            if unstake_amount > self.staked_balances[agent]:
                 print(f"❌ Unstaking failed: Requested {unstake_amount:.3f}, but only {self.staked_balances[agent]:.3f} is available in total staked balance.")
                 return False

            if unstake_amount > 0:
                self.staked_balances[agent] -= unstake_amount
                self.balances[agent] += unstake_amount
                unstaking_record = {
                    "type": "fuel_unstake_general",
                    "agent": agent,
                    "amount": round(unstake_amount, 3),
                    "balance": round(self.balances[agent], 3),
                    "staked_balance": round(self.staked_balances[agent], 3),
                    "explanation": f"Agent {agent} unstaked {unstake_amount} fuel from general staked balance."
                }
                logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": unstaking_record, "agent": agent})
                if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
                     self.recent_logged_events.append(logged_receipt)

                print(f"✅ {agent} unstaked {round(unstake_amount, 3)} fuel from general staked balance.")
                return True
            else:
                 print(f"❌ Unstaking failed: No amount specified or amount is zero.")
                 return False


    def slash_fuel(self, agent: str, amount: float, reason: str, assessment_id: str = None, distributor: str = "CommunityPool"):
        """
        Slashes staked fuel due to inaccurate assessments and distributes it.
        If assessment_id is provided, slashes from that specific stake first.
        Distributes slashed fuel to a specified distributor (e.g., "CommunityPool").
        """
        if amount <= 0:
            print(f"❌ Slashing failed: Amount must be positive.")
            return False

        # Determine the maximum amount that can be slashed from the agent's total staked balance
        max_slashable_total = self.staked_balances[agent]
        actual_slash_amount = min(amount, max_slashable_total)

        staked_on_assessment = 0.0
        if assessment_id and agent in self.stakes_by_assessment and assessment_id in self.stakes_by_assessment[agent]:
             staked_on_assessment = self.stakes_by_assessment[agent][assessment_id]['amount']
             # If assessment_id is provided, the amount to slash is also limited by the stake on that assessment
             actual_slash_amount = min(actual_slash_amount, staked_on_assessment)


        if actual_slash_amount > 0:
            self.staked_balances[agent] -= actual_slash_amount

            # Reduce the specific stake amount if assessment_id is provided
            if assessment_id and agent in self.stakes_by_assessment and assessment_id in self.stakes_by_assessment[agent]:
                 self.stakes_by_assessment[agent][assessment_id]['amount'] -= actual_slash_amount
                 if self.stakes_by_assessment[agent][assessment_id]['amount'] <= 1e-9: # Clean up small floating point remainders
                      del self.stakes_by_assessment[agent][assessment_id]
                      if not self.stakes_by_assessment[agent]: # Clean up agent entry if no more stakes
                           del self.stakes_by_assessment[agent]

                 # Also update total staked on assessment
                 self.total_staked_on_assessment[assessment_id] -= actual_slash_amount
                 if self.total_staked_on_assessment[assessment_id] <= 1e-9: # Clean up small floating point remainders
                      del self.total_staked_on_assessment[assessment_id]


            # Distribute the slashed fuel
            self.balances[distributor] += actual_slash_amount # Add to the distributor's balance

            slashing_record = {
                "type": "fuel_slash",
                "agent": agent,
                "amount": round(actual_slash_amount, 3),
                "reason": reason,
                "assessment_id": assessment_id,
                "staked_balance": round(self.staked_balances[agent], 3),
                "distributor": distributor,
                "distributor_balance": round(self.balances[distributor], 3),
                "explanation": f"Agent {agent} slashed {actual_slash_amount} fuel for reason: {reason}. Distributed to {distributor} (Assessment ID: {assessment_id if assessment_id else 'N/A'})."
            }
            # Log the slashing event via the kernel
            logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": slashing_record, "agent": agent})
            if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
                 self.recent_logged_events.append(logged_receipt)

            print(f"🔪 {agent} slashed {round(actual_slash_amount, 3)} fuel for reason: {reason}. Distributed to {distributor}.")
            return True
        else:
            print(f"🚫 Slashing failed for {agent}: No staked fuel to slash (requested {amount:.3f}, available {self.staked_balances[agent]:.3f} total, {staked_on_assessment:.3f} on assessment {assessment_id if assessment_id else 'N/A'}).")
            return False


    def integrate_external_data(self, data_type: str, source: str) -> Dict[str, Any]:
        """
        Simulates fetching data from external APIs (ESG, Carbon Credits, etc.)
        and logs the event. Returns mock data.
        """
        print(f"🔌 Attempting to integrate external data: Type='{data_type}', Source='{source}'")

        # Simulate fetching data based on data_type
        mock_data: Dict[str, Any] = {}
        if data_type.lower() == "esg":
            mock_data = {
                "type": "esg_score",
                "source": source,
                "score": round(random.uniform(0, 100), 2),
                "category_scores": {
                    "environmental": round(random.uniform(0, 100), 2),
                    "social": round(random.uniform(0, 100), 2),
                    "governance": round(random.uniform(0, 100), 2)
                },
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
            }
            print(f"   Simulated ESG data fetched from {source}.")
        elif data_type.lower() == "carboncredit":
            mock_data = {
                "type": "carbon_credit_value",
                "source": source,
                "value_per_ton": round(random.uniform(10, 200), 2),
                "unit": "USD/ton",
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
            }
            print(f"   Simulated Carbon Credit data fetched from {source}.")
        else:
            mock_data = {
                "type": "unknown_external_data",
                "source": source,
                "payload": {"status": "simulated_data_not_recognized"},
                "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
            }
            print(f"   Simulated unknown external data fetched from {source}.")

        integration_record = {
            "type": "external_data_integration",
            "data_type": data_type,
            "source": source,
            "payload": mock_data, # Include the mock data payload
            "explanation": f"Integrated external data of type '{data_type}' from '{source}'."
        }

        # Log the integration event via the kernel
        logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": integration_record, "agent": "System"}) # Attribute to System agent
        if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
             self.recent_logged_events.append(logged_receipt)


        return mock_data


    def predictive_maintenance(self):
        """
        Detects decline in transaction velocity from the ledger
        and logs an alert if necessary.
        """
        print("⚠️ Running predictive maintenance check...")

        # Load recent transactions from the ledger
        # We need to read the actual ledger file to get timestamps
        recent_transactions = []
        # Safely access the ledger path from the kernel's writer
        ledger_path = None
        if hasattr(self.kernel, 'writer') and hasattr(self.kernel.writer, 'ledger') and hasattr(self.kernel.writer.ledger, 'path'):
             ledger_path = self.kernel.writer.ledger.path
        elif isinstance(self.kernel, MockKernel) and hasattr(self.kernel, 'writer') and hasattr(self.kernel.writer.ledger, 'path'):
             ledger_path = self.kernel.writer.ledger.path


        if not ledger_path or not ledger_path.exists():
             print(f"Ledger file not found at {ledger_path}. Cannot check velocity.")
             current_velocity = 0
        else:
            try:
                with open(ledger_path, "r") as f:
                    # Read all lines and filter by timestamp
                    for line in f:
                        try:
                            entry = json.loads(line)
                            # Consider only 'fuel_transfer', 'fuel_stake', 'fuel_unstake', 'fuel_slash' events as transactions
                            if entry.get("event_type") == "SYSTEM_EVENT" and entry.get("data", {}).get("type") in ["fuel_transfer", "fuel_stake", "fuel_unstake", "fuel_slash"]:
                                timestamp_str = entry.get("timestamp")
                                if timestamp_str:
                                    # Parse timestamp string (e.g., "YYYY-MM-DDTHH:MM:SSZ")
                                    # Use a robust parser that can handle potential variations if needed, but standard format is expected
                                    timestamp = datetime.strptime(timestamp_str, "%Y-%m-%dT%H:%M:%SZ")
                                    # Check if the transaction is within the time window
                                    if datetime.utcnow() - timestamp <= self.velocity_time_window:
                                        recent_transactions.append(entry)
                        except (json.JSONDecodeError, ValueError) as e:
                            print(f"Error parsing ledger entry or timestamp: {e}")
                            continue # Skip invalid lines

            except Exception as e:
                print(f"An error occurred while reading the ledger: {e}")
                current_velocity = 0 # Assume 0 velocity on error
                recent_transactions = [] # Clear any partial results


            current_velocity = len(recent_transactions)


        print(f"   Detected {current_velocity} transactions in the last {self.velocity_time_window}.")

        # Check if velocity falls below the threshold
        if current_velocity < self.velocity_threshold_transactions_per_window:
            alert_message = f"Transaction velocity decline detected! Only {current_velocity} transactions in the last {self.velocity_time_window}, below threshold of {self.velocity_threshold_transactions_per_window}."
            print(f"🚨 {alert_message}")
            alert_record = {
                "type": "predictive_maintenance_alert",
                "metric": "transaction_velocity",
                "current_velocity": current_velocity,
                "threshold": self.velocity_threshold_transactions_per_window,
                "time_window_seconds": self.velocity_time_window.total_seconds(),
                "explanation": alert_message
            }
            # Log the alert via the kernel
            logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": alert_record, "agent": "PredictiveMaintenance"})
            if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
                 self.recent_logged_events.append(logged_receipt)

        else:
            print("✅ Transaction velocity is healthy.")

        # Update last check time (optional, could be used to control check frequency)
        self.last_maintenance_check = time.time()


    def get_status(self) -> Dict[str, Dict[str, float]]:
        """Return current balances and staked balances."""
        status = {}
        all_agents = set(list(self.balances.keys()) + list(self.staked_balances.keys()))
        for agent in all_agents:
            status[agent] = {
                "balance": round(self.balances[agent], 3),
                "staked_balance": round(self.staked_balances[agent], 3)
            }
        return status

    def get_stakes_by_assessment(self, assessment_id: str) -> Dict[str, float]:
        """Return stakes for a specific assessment."""
        # Note: self.stakes_by_assessment is agent -> assessment -> {'amount': ..., 'timestamp': ...}
        # We need to return assessment -> {agent: amount} format as requested previously
        stakes_on_id: Dict[str, float] = {}
        for agent, stakes in self.stakes_by_assessment.items():
            if assessment_id in stakes:
                 stakes_on_id[agent] = stakes[assessment_id]['amount']
        return stakes_on_id


    def get_total_staked_on_assessment(self, assessment_id: str) -> float:
        """Return total fuel staked on a specific assessment."""
        # Recalculate from stakes_by_assessment to be robust
        total = sum(stake_info['amount'] for agent, stakes in self.stakes_by_assessment.items() for ass_id, stake_info in stakes.items() if ass_id == assessment_id)
        return round(total, 3)


# --- Demonstration ---
if __name__ == "__main__":
     # Ensure necessary classes are available for the demo
     # Define mocks if not already defined globally
     if 'GovernanceKernelV2' not in globals():
         print("GovernanceKernelV2 not found. Using a mock setup for demonstration.")
         class MockLedger:
             def __init__(self, path="ledger.jsonl"):
                  self.path = Path(f"/content/{path}")
                  self.path.touch(exist_ok=True)
                  # Read existing entries if file exists
                  self._entries = []
                  if self.path.exists():
                      try:
                          with self.path.open("r") as f:
                              for line in f:
                                  self._entries.append(json.loads(line))
                      except Exception as e:
                          print(f"Error reading existing mock ledger file: {e}")


             def append(self, record):
                  record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
                  record["prev_hash"] = "mock"
                  record["hash"] = "mock"
                  record["id"] = str(uuid.uuid4()) # Add a mock ID
                  self._entries.append(record) # Add to in-memory list
                  # Simulate writing to file (optional for mock, but good for testing read logic)
                  try:
                       with self.path.open("a") as f:
                            f.write(json.dumps(record) + "\n")
                  except Exception as e:
                       print(f"Mock Ledger write error: {e}")
                  return record
             def verify(self):
                  return True
             def read_all(self): # Helper to read all entries for mock velocity check
                  # In a real scenario, you'd read from the file
                  # Ensure we return a copy to prevent external modification
                  return list(self._entries)

         class MockReceiptWriter: # Define ReceiptWriter if not global
              def __init__(self, ledger_path="ledger.jsonl"):
                   self.ledger = MockLedger(ledger_path)
              def log(self, event_type, data):
                   return self.ledger.append({"event_type": event_type, "data": data})
              def verify_ledger(self):
                   return self.ledger.verify()
         class GovernanceKernelV2: # Define GovernanceKernelV2 if not global
              def __init__(self, ledger_path="ledger.jsonl"):
                   self.writer = MockReceiptWriter(ledger_path)
                   self.agent_reputation = defaultdict(lambda: 0.5)
              def evaluate(self, event):
                   print(f"Mock GovernanceKernelV2 received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
                   # Simulate reputation update if event data has metabolism info and agent
                   if "metabolism" in event.get("data", {}).get("type", "") and event.get("agent"):
                        agent = event["agent"]
                        clarity_gain = event["data"].get("clarity", 0.0)
                        entropy_burn = event["data"].get("entropy", 0.0)
                        change = (clarity_gain * 0.1) - (entropy_burn * 0.05)
                        new_reputation = self.agent_reputation[agent] + change
                        self.agent_reputation[agent] = max(0.0, min(1.0, new_reputation))
                        print(f"Mock Reputation update for {agent}: {self.agent_reputation[agent]:.3f}")
                   # Return a mock receipt with an ID
                   return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
              def get_reputation(self, agent):
                   return self.agent_reputation.get(agent, 0.5)
         import time # Import time for mock timestamp
         import uuid # Import uuid for mock ID
         from pathlib import Path # Import Path for mock ledger path
         # Define GovernanceKernel if not already defined (for fallback scenario)
         if 'GovernanceKernel' not in globals():
              class GovernanceKernel:
                   def __init__(self, ledger_path="ledger.jsonl"):
                       self.writer = MockReceiptWriter(ledger_path)
                       print("Mock Base GovernanceKernel initialized.")
                   def evaluate(self, event):
                       print(f"Mock Base GovernanceKernel received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
                       return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
                   def verify_ledger(self):
                       print("Mock Base GovernanceKernel verified ledger.")
                       return True


     # Clean up ledger file for a fresh demo run if it exists
     ledger_path = Path("/content/ledger.jsonl")
     if ledger_path.exists():
          try:
              # Keep the first few lines to simulate existing history if needed for other parts
              with open(ledger_path, 'r') as f:
                   lines = f.readlines()
              with open(ledger_path, 'w') as f:
                   # Keep, say, the first 5 lines, or clear entirely for a truly fresh start
                   f.writelines(lines[:5])
              print(f"Cleaned up existing ledger file: {ledger_path}")
          except Exception as e:
              print(f"Error cleaning up ledger file: {e}")


     market = ClarityMarket()

     # Seed some initial balances for demonstration
     market.balances["AgentA"] = 100.0
     market.balances["AgentB"] = 50.0
     market.balances["AgentC"] = 200.0

     print("\n--- Demonstrating Token Transactions ---")
     print("\nInitial Balances:", json.dumps(market.get_status(), indent=2))

     # Demonstrate `transfer_fuel`
     print("\nDemonstrating transfers:")
     market.transfer_fuel("AgentA", "AgentB", 20.0)
     market.transfer_fuel("AgentC", "AgentA", 50.0)
     market.transfer_fuel("AgentB", "AgentC", 10.0)
     market.transfer_fuel("AgentA", "AgentD", 30.0) # Transfer to a new agent

     # Demonstrate failed transaction
     market.transfer_fuel("AgentB", "AgentA", 100.0) # Should fail due to insufficient funds
     market.transfer_fuel("AgentA", "AgentB", -10.0) # Should fail due to negative amount

     print("\nBalances after transfers:", json.dumps(market.get_status(), indent=2))

     print("\n--- Demonstrating Staking and Slashing ---")
     # Simulate assessment IDs
     assessment_1_id = str(uuid.uuid4())
     assessment_2_id = str(uuid.uuid4())

     # Demonstrate `stake_fuel`
     print(f"\nDemonstrating staking on assessments (ID 1: {assessment_1_id}, ID 2: {assessment_2_id}):")
     market.stake_fuel("AgentA", 20.0, assessment_1_id) # AgentA stakes 20 on assessment 1
     market.stake_fuel("AgentB", 10.0, assessment_1_id) # AgentB stakes 10 on assessment 1
     market.stake_fuel("AgentA", 15.0, assessment_2_id) # AgentA stakes 15 on assessment 2
     market.stake_fuel("AgentC", 50.0, assessment_1_id) # AgentC stakes 50 on assessment 1
     market.stake_fuel("AgentB", 30.0, assessment_2_id) # Should fail, AgentB only has 40 balance after transfers (50 - 10)

     print("\nBalances after staking:", json.dumps(market.get_status(), indent=2))
     print(f"\nStakes on Assessment {assessment_1_id}:", market.get_stakes_by_assessment(assessment_1_id))
     print(f"Total staked on Assessment {assessment_1_id}: {market.get_total_staked_on_assessment(assessment_1_id)}")

     print(f"\nStakes on Assessment {assessment_2_id}:", market.get_stakes_by_assessment(assessment_2_id))
     print(f"Total staked on Assessment {assessment_2_id}: {market.get_total_staked_on_assessment(assessment_2_id)}")


     # Demonstrate `slash_fuel` based on assessment outcome (simulated)
     print("\nDemonstrating slashing fuel:")
     # Simulate assessment 1 being deemed inaccurate
     print(f"\nAssessment {assessment_1_id} deemed inaccurate. Slashing agents who staked on it.")
     # Slash AgentA's stake on assessment 1 (20 fuel staked). This fuel goes to CommunityPool.
     market.slash_fuel("AgentA", 20.0, "Staked on inaccurate assessment", assessment_id=assessment_1_id, distributor="CommunityPool")
      # Slash AgentB's stake on assessment 1 (10 fuel staked). This fuel goes to CommunityPool.
     market.slash_fuel("AgentB", 10.0, "Staked on inaccurate assessment", assessment_id=assessment_1_id, distributor="CommunityPool")
      # Slash AgentC's stake on assessment 1 (50 fuel staked). This fuel goes to CommunityPool.
     market.slash_fuel("AgentC", 50.0, "Staked on inaccurate assessment", assessment_id=assessment_1_id, distributor="CommunityPool")

     # Try to slash AgentA more than they staked on assessment 2
     market.slash_fuel("AgentA", 20.0, "Attempted over-slashing on assessment 2", assessment_id=assessment_2_id, distributor="CommunityPool") # Should only slash 15

     # Try to slash an agent with no stake on a specific assessment
     market.slash_fuel("AgentD", 10.0, "Slashing attempt on agent with no stake on assessment", assessment_id=assessment_1_id, distributor="CommunityPool") # Should fail

     # Try to slash an agent with insufficient total staked fuel
     market.slash_fuel("AgentA", 50.0, "Attempted over-slashing total stake", distributor="CommunityPool") # Should fail, AgentA's total staked is now 15

     print("\nBalances after slashing:", json.dumps(market.get_status(), indent=2))
     print(f"\nStakes on Assessment {assessment_1_id} after slashing:", market.get_stakes_by_assessment(assessment_1_id))
     print(f"Total staked on Assessment {assessment_1_id} after slashing: {market.get_total_staked_on_assessment(assessment_1_id)}")

     print(f"\nStakes on Assessment {assessment_2_id} after slashing:", market.get_stakes_by_assessment(assessment_2_id))
     print(f"Total staked on Assessment {assessment_2_id} after slashing: {market.get_total_staked_on_assessment(assessment_2_id)}")

     print("\n--- Demonstrating Unstaking (with lock-up) ---")
     # Demonstrate `unstake_fuel`
     # Try to unstake immediately from assessment 2 (should be locked)
     market.unstake_fuel("AgentA", assessment_id=assessment_2_id, amount=5.0) # Should fail due to lock-up

     # Wait for lock-up period to pass (simulated)
     print(f"\nSimulating waiting for lock-up period ({market.staking_lockup_period.total_seconds()} seconds)...")
     time.sleep(market.staking_lockup_period.total_seconds() + 1)

     # Try to unstake again from assessment 2 (should succeed)
     market.unstake_fuel("AgentA", assessment_id=assessment_2_id, amount=5.0) # Should succeed
     market.unstake_fuel("AgentA", assessment_id=assessment_2_id, amount=10.0) # Should succeed (unstake remaining)

     # Try to unstake from a completed/slashed assessment (should fail if stake is zero)
     market.unstake_fuel("AgentA", assessment_id=assessment_1_id, amount=1.0) # Should fail, stake is zero

     # Try to unstake from general staked balance (should succeed if no locked stakes)
     market.unstake_fuel("AgentA", amount=1.0) # Should succeed, total staked is now 0

     print("\nBalances after unstaking:", json.dumps(market.get_status(), indent=2))


     print("\n--- Demonstrating External Data Integration ---")
     # Demonstrate `integrate_external_data`
     esg_data = market.integrate_external_data("ESG", "https://api.example.com/esg/company_xyz")
     print("Fetched ESG Data Sample:", json.dumps(esg_data, indent=2))

     carbon_data = market.integrate_external_data("CarbonCredit", "https://api.carbonmarket.net/spot_prices")
     print("Fetched Carbon Credit Data Sample:", json.dumps(carbon_data, indent=2))

     unknown_data = market.integrate_external_data("Weather", "https://api.weather.com/forecast")
     print("Fetched Unknown Data Sample:", json.dumps(unknown_data, indent=2))


     print("\n--- Demonstrating Predictive Maintenance ---")
     # Simulate some recent transactions within the small time window
     print("\nSimulating transactions within the velocity window...")
     market.transfer_fuel("AgentA", "AgentB", 1.0)
     market.stake_fuel("AgentB", 2.0, str(uuid.uuid4()))
     market.transfer_fuel("AgentB", "AgentA", 0.5)

     # Run predictive maintenance check - should be healthy
     print("\nRunning first maintenance check (should be healthy):")
     market.predictive_maintenance()

     # Simulate a period of low transaction activity (by adding older entries to the ledger)
     print(f"\nSimulating low transaction activity by adding old entries to ledger (waiting {market.velocity_time_window.total_seconds() + 1} seconds in simulation)...")
     # Add some old transactions manually to the ledger file
     ledger_path = Path("/content/ledger.jsonl")
     if ledger_path.exists():
          try:
              with open(ledger_path, "a") as f:
                  old_time = datetime.utcnow() - market.velocity_time_window - timedelta(seconds=1) # Just outside the window
                  old_timestamp_str = old_time.strftime("%Y-%m-%dT%H:%M:%SZ")
                  old_transfer = {"event_type": "SYSTEM_EVENT", "data": {"type": "fuel_transfer", "sender": "AgentX", "recipient": "AgentY", "amount": 1.0}, "timestamp": old_timestamp_str, "id": str(uuid.uuid4())}
                  f.write(json.dumps(old_transfer) + "\n")
              print("Mock ledger file updated with old entries.")
          except Exception as e:
              print(f"Error writing mock ledger file: {e}")
     else:
          print(f"Ledger file not found at {ledger_path}. Cannot add old entries for demo.")


     # Run predictive maintenance check again - should detect decline
     print("\nRunning second maintenance check (should detect decline):")
     market.predictive_maintenance()

     print("\n--- Predictive Maintenance Demo Complete ---")

     print("\nClarity Market full demonstration complete.")

"""**Reasoning**:
Review the `ClarityMarket` class and its demonstration code to ensure that all relevant events (`fuel_transfer`, `fuel_stake`, `fuel_unstake`, `fuel_slash`, `external_data_integration`, `predictive_maintenance_alert`) are being logged via `self.kernel.evaluate()`, that the necessary data for each event is included, and that the agent parameter is correctly passed for agent-specific events. Also, verify the mock `GovernanceKernelV2` handles these events and that the demonstration shows the logging.


"""

# Review of clarity_market.py and demonstration

# The code for clarity_market.py was written in the previous step.
# Reviewing the code:
# - fuel_transfer calls self.kernel.evaluate with event_type="SYSTEM_EVENT", data=transaction_record, agent=sender. Correct.
# - fuel_stake calls self.kernel.evaluate with event_type="SYSTEM_EVENT", data=staking_record, agent=agent. Correct.
# - unstake_fuel calls self.kernel.evaluate with event_type="SYSTEM_EVENT", data=unstaking_record, agent=agent. Correct.
# - slash_fuel calls self.kernel.evaluate with event_type="SYSTEM_EVENT", data=slashing_record, agent=agent. Correct.
# - integrate_external_data calls self.kernel.evaluate with event_type="SYSTEM_EVENT", data=integration_record, agent="System". Correct.
# - predictive_maintenance logs an alert by calling self.kernel.evaluate with event_type="SYSTEM_EVENT", data=alert_record, agent="PredictiveMaintenance". Correct.
# - The data dictionaries for each event type appear to contain relevant information.
# - The mock GovernanceKernelV2 includes print statements to show when evaluate is called and attempts to simulate reputation updates, covering the basic interaction.
# - The demonstration in __main__ calls all the methods that log events, allowing observation of the mock kernel's print statements or actual ledger entries.

# The review confirms that the ClarityMarket interacts with the GovernanceKernelV2 (or mock) as required for logging transactions.
# The subtask is completed.

"""## Demonstrate functionality

### Subtask:
Add example code to demonstrate token transactions, staking/slashing, and predictive maintenance.

**Reasoning**:
The subtask requires demonstrating the token transactions, staking/slashing, external data integration, and predictive maintenance using the implemented ClarityMarket class. I will add the necessary demonstration code within the `if __name__ == "__main__":` block to showcase these functionalities and print the market status at different stages.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile clarity_market.py
# """
# Tessrax Clarity Fuel Economy v2.0 - Clarity Market
# --------------------------------------------------
# Introduces a token-based clarity fuel marketplace, allowing staking and slashing,
# integration with external APIs, and predictive maintenance based on transaction velocity.
# """
# 
# import json
# from typing import Dict, Any, List
# from collections import defaultdict
# import time # Import time for timestamp
# import uuid # Import uuid for logging
# import random # Import random for mock data
# from datetime import datetime, timedelta # Import datetime and timedelta for time window analysis
# from pathlib import Path # Import Path for ledger path in mock
# 
# # GovernanceKernelV2 should be available in the environment from previous steps
# # from governance_kernel import GovernanceKernelV2
# # ReceiptWriter should also be available
# # from receipts import ReceiptWriter
# 
# class ClarityMarket:
#     """
#     Manages the clarity fuel token economy, including transactions, staking,
#     slashing, and external data integration.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Initialize with access to the GovernanceKernelV2
#         # Assuming GovernanceKernelV2 is available in the Colab environment
#         if 'GovernanceKernelV2' in globals():
#              self.kernel = GovernanceKernelV2(ledger_path)
#         elif 'GovernanceKernel' in globals():
#              # Fallback to base kernel if V2 is not found
#              print("Warning: GovernanceKernelV2 not found. Using base GovernanceKernel.")
#              self.kernel = GovernanceKernel(ledger_path)
#         else:
#              # Mock kernel if neither is found (for isolated testing)
#              print("Error: Neither GovernanceKernelV2 nor GovernanceKernel found. Using a mock kernel.")
#              class MockKernel:
#                   def evaluate(self, event):
#                       print(f"Mock GovernanceKernel received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
#                       # Simulate reputation update if event data has metabolism info and agent
#                       if "metabolism" in event.get("data", {}).get("type", "") and event.get("agent"):
#                            agent = event["agent"]
#                            clarity_gain = event["data"].get("clarity", 0.0)
#                            entropy_burn = event["data"].get("entropy", 0.0)
#                            change = (clarity_gain * 0.1) - (entropy_burn * 0.05)
#                            # Mock update - in a real kernel, this would update a persistent state
#                            print(f"Mock Reputation update for {agent} simulated.")
#                       # Return a mock receipt with an ID for potential tracing
#                       return {"id": "mock_" + str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#              self.kernel = MockKernel()
# 
# 
#         self.balances: Dict[str, float] = defaultdict(float) # Using defaultdict for balances
#         self.staked_balances: Dict[str, float] = defaultdict(float) # To track total staked fuel per agent
#         # New: Track individual stakes linked to assessment IDs
#         # Format: {agent_id: {assessment_id: staked_amount}}
#         self.stakes_by_assessment: Dict[str, Dict[str, float]] = defaultdict(lambda: defaultdict(float))
#         # Optional: Track total staked amount per assessment ID for quick lookup
#         self.total_staked_on_assessment: Dict[str, float] = defaultdict(float)
# 
#         # Transaction history is primarily read from the ledger now, but keeping a list for recent additions before ledger read is updated
#         self.recent_logged_events: List[Dict[str, Any]] = []
# 
# 
#         # Predictive maintenance parameters
#         self.velocity_time_window = timedelta(seconds=30) # Analyze transactions in the last 30 seconds for demo speed
#         self.velocity_threshold_transactions_per_window = 2 # Alert if fewer than 2 transactions in the window
#         self.last_maintenance_check = time.time() # Track last check time to avoid excessive checks
# 
# 
#         print("💰 Clarity Fuel Market initialized.")
# 
# 
#     # Basic transaction function (can be extended later)
#     def transfer_fuel(self, sender: str, recipient: str, amount: float):
#         """Transfers clarity fuel between agents."""
#         if amount <= 0:
#             print(f"❌ Transfer failed: Amount must be positive.")
#             return False
# 
#         if self.balances[sender] >= amount:
#             self.balances[sender] -= amount
#             self.balances[recipient] += amount
#             transaction_record = {
#                 "type": "fuel_transfer",
#                 "sender": sender,
#                 "recipient": recipient,
#                 "amount": round(amount, 3),
#                 "sender_balance": round(self.balances[sender], 3),
#                 "recipient_balance": round(self.balances[recipient], 3),
#                 "explanation": f"Fuel transfer from {sender} to {recipient}: {amount}"
#             }
#             # Log the transaction via the kernel
#             logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": transaction_record, "agent": sender})
# 
#             # Add to recent events list if logged
#             if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
#                  self.recent_logged_events.append(logged_receipt)
# 
# 
#             print(f"💸 {sender} transferred {round(amount, 3)} fuel to {recipient}.")
#             return True
#         else:
#             print(f"❌ Transfer failed: {sender} has insufficient funds ({self.balances[sender]:.3f} < {amount:.3f}).")
#             return False
# 
# 
#     def stake_fuel(self, agent: str, amount: float, assessment_id: str):
#         """Stakes clarity fuel on a specific contradiction assessment."""
#         if amount <= 0:
#             print(f"❌ Staking failed: Amount must be positive.")
#             return False
# 
#         if not assessment_id:
#             print("❌ Staking failed: assessment_id must be provided.")
#             return False
# 
#         if self.balances[agent] >= amount:
#             self.balances[agent] -= amount
#             self.staked_balances[agent] += amount
#             self.stakes_by_assessment[agent][assessment_id] += amount
#             self.total_staked_on_assessment[assessment_id] += amount # Update total staked on this assessment
# 
#             staking_record = {
#                 "type": "fuel_stake",
#                 "agent": agent,
#                 "amount": round(amount, 3),
#                 "assessment_id": assessment_id,
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} staked {amount} fuel on assessment {assessment_id}."
#             }
#             # Log the staking event via the kernel
#             logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": staking_record, "agent": agent})
#             if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
#                  self.recent_logged_events.append(logged_receipt)
# 
#             print(f"✅ {agent} staked {round(amount, 3)} fuel on assessment {assessment_id}.")
#             return True
#         else:
#             print(f"❌ Staking failed: {agent} has insufficient funds ({self.balances[agent]:.3f} < {amount:.3f}).")
#             return False
# 
#     def unstake_fuel(self, agent: str, amount: float):
#         """Unstakes clarity fuel. Note: This unstakes from the agent's total staked balance.
#            Specific stakes linked to assessments need a separate mechanism for withdrawal
#            or are released/slashed via assessment outcome."""
#         if amount <= 0:
#             print(f"❌ Unstaking failed: Amount must be positive.")
#             return False
# 
#         # For simplicity here, unstaking reduces the agent's total staked balance.
#         # A more complex system would require specifying *which* stake to unstake,
#         # especially if stakes are locked until assessment resolution.
#         # We'll assume this is a request to withdraw from the general staked pool,
#         # not necessarily tied to a specific assessment outcome yet.
#         if self.staked_balances[agent] >= amount:
#             self.staked_balances[agent] -= amount
#             self.balances[agent] += amount
#             unstaking_record = {
#                 "type": "fuel_unstake",
#                 "agent": agent,
#                 "amount": round(amount, 3),
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} unstaked {amount} fuel."
#             }
#              # Log the unstaking event via the kernel
#             logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": unstaking_record, "agent": agent})
#             if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
#                  self.recent_logged_events.append(logged_receipt)
# 
#             print(f"✅ {agent} unstaked {round(amount, 3)} fuel.")
#             return True
#         else:
#             print(f"❌ Unstaking failed: {agent} has insufficient staked fuel ({self.staked_balances[agent]:.3f} < {amount:.3f}).")
#             return False
# 
# 
#     def slash_fuel(self, agent: str, amount: float, reason: str, assessment_id: str = None):
#         """Slashes staked fuel due to inaccurate assessments."""
#         if amount <= 0:
#             print(f"❌ Slashing failed: Amount must be positive.")
#             return False
# 
#         # Find the amount staked by this agent on this specific assessment, if provided
#         staked_on_assessment = 0.0
#         if assessment_id and agent in self.stakes_by_assessment and assessment_id in self.stakes_by_assessment[agent]:
#              staked_on_assessment = self.stakes_by_assessment[agent][assessment_id]
# 
#         # The amount to slash should not exceed the total staked by the agent,
#         # and if assessment_id is given, it shouldn't exceed the amount staked on that assessment.
#         actual_slash_amount = min(amount, self.staked_balances[agent])
#         if assessment_id:
#             actual_slash_amount = min(actual_slash_amount, staked_on_assessment)
# 
#         if actual_slash_amount > 0:
#             self.staked_balances[agent] -= actual_slash_amount
# 
#             # Reduce the specific stake amount if assessment_id is provided
#             if assessment_id and agent in self.stakes_by_assessment and assessment_id in self.stakes_by_assessment[agent]:
#                  self.stakes_by_assessment[agent][assessment_id] -= actual_slash_amount
#                  if self.stakes_by_assessment[agent][assessment_id] <= 1e-9: # Clean up small floating point remainders
#                       del self.stakes_by_assessment[agent][assessment_id]
#                  if not self.stakes_by_assessment[agent]: # Clean up agent entry if no more stakes
#                       del self.stakes_by_assessment[agent]
# 
#                  # Also update total staked on assessment
#                  self.total_staked_on_assessment[assessment_id] -= actual_slash_amount
#                  if self.total_staked_on_assessment[assessment_id] <= 1e-9: # Clean up small floating point remainders
#                       del self.total_staked_on_assessment[assessment_id]
# 
# 
#             slashing_record = {
#                 "type": "fuel_slash",
#                 "agent": agent,
#                 "amount": round(actual_slash_amount, 3),
#                 "reason": reason,
#                 "assessment_id": assessment_id,
#                 "staked_balance": round(self.staked_balances[agent], 3),
#                 "explanation": f"Agent {agent} slashed {actual_slash_amount} fuel for reason: {reason} (Assessment ID: {assessment_id if assessment_id else 'N/A'})."
#             }
#             # Log the slashing event via the kernel
#             logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": slashing_record, "agent": agent})
#             if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
#                  self.recent_logged_events.append(logged_receipt)
# 
#             print(f"🔪 {agent} slashed {round(actual_slash_amount, 3)} fuel for reason: {reason}.")
#             return True
#         else:
#             print(f"🚫 Slashing failed for {agent}: No staked fuel to slash (requested {amount:.3f}, available {self.staked_balances[agent]:.3f} total, {staked_on_assessment:.3f} on assessment {assessment_id if assessment_id else 'N/A'}).")
#             return False
# 
# 
#     def integrate_external_data(self, data_type: str, source: str) -> Dict[str, Any]:
#         """
#         Simulates fetching data from external APIs (ESG, Carbon Credits, etc.)
#         and logs the event. Returns mock data.
#         """
#         print(f"🔌 Attempting to integrate external data: Type='{data_type}', Source='{source}'")
# 
#         # Simulate fetching data based on data_type
#         mock_data: Dict[str, Any] = {}
#         if data_type.lower() == "esg":
#             mock_data = {
#                 "type": "esg_score",
#                 "source": source,
#                 "score": round(random.uniform(0, 100), 2),
#                 "category_scores": {
#                     "environmental": round(random.uniform(0, 100), 2),
#                     "social": round(random.uniform(0, 100), 2),
#                     "governance": round(random.uniform(0, 100), 2)
#                 },
#                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#             }
#             print(f"   Simulated ESG data fetched from {source}.")
#         elif data_type.lower() == "carboncredit":
#             mock_data = {
#                 "type": "carbon_credit_value",
#                 "source": source,
#                 "value_per_ton": round(random.uniform(10, 200), 2),
#                 "unit": "USD/ton",
#                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#             }
#             print(f"   Simulated Carbon Credit data fetched from {source}.")
#         else:
#             mock_data = {
#                 "type": "unknown_external_data",
#                 "source": source,
#                 "payload": {"status": "simulated_data_not_recognized"},
#                 "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#             }
#             print(f"   Simulated unknown external data fetched from {source}.")
# 
#         integration_record = {
#             "type": "external_data_integration",
#             "data_type": data_type,
#             "source": source,
#             "payload": mock_data, # Include the mock data payload
#             "explanation": f"Integrated external data of type '{data_type}' from '{source}'."
#         }
# 
#         # Log the integration event via the kernel
#         logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": integration_record, "agent": "System"}) # Attribute to System agent
#         if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
#              self.recent_logged_events.append(logged_receipt)
# 
# 
#         return mock_data
# 
# 
#     def predictive_maintenance(self):
#         """
#         Detects decline in transaction velocity from the ledger
#         and logs an alert if necessary.
#         """
#         print("⚠️ Running predictive maintenance check...")
# 
#         # Load recent transactions from the ledger
#         # We need to read the actual ledger file to get timestamps
#         recent_transactions = []
#         # Safely access the ledger path from the kernel's writer
#         ledger_path = None
#         if hasattr(self.kernel, 'writer') and hasattr(self.kernel.writer, 'ledger') and hasattr(self.kernel.writer.ledger, 'path'):
#              ledger_path = self.kernel.writer.ledger.path
#         elif isinstance(self.kernel, MockKernel) and hasattr(self.kernel, 'writer') and hasattr(self.kernel.writer.ledger, 'path'):
#              ledger_path = self.kernel.writer.ledger.path
# 
# 
#         if not ledger_path or not ledger_path.exists():
#              print(f"Ledger file not found at {ledger_path}. Cannot check velocity.")
#              current_velocity = 0
#         else:
#             try:
#                 with open(ledger_path, "r") as f:
#                     # Read all lines and filter by timestamp
#                     for line in f:
#                         try:
#                             entry = json.loads(line)
#                             # Consider only 'fuel_transfer', 'fuel_stake', 'fuel_unstake', 'fuel_slash' events as transactions
#                             if entry.get("event_type") == "SYSTEM_EVENT" and entry.get("data", {}).get("type") in ["fuel_transfer", "fuel_stake", "fuel_unstake", "fuel_slash"]:
#                                 timestamp_str = entry.get("timestamp")
#                                 if timestamp_str:
#                                     # Parse timestamp string (e.g., "YYYY-MM-DDTHH:MM:SSZ")
#                                     # Use a robust parser that can handle potential variations if needed, but standard format is expected
#                                     timestamp = datetime.strptime(timestamp_str, "%Y-%m-%dT%H:%M:%SZ")
#                                     # Check if the transaction is within the time window
#                                     if datetime.utcnow() - timestamp <= self.velocity_time_window:
#                                         recent_transactions.append(entry)
#                         except (json.JSONDecodeError, ValueError) as e:
#                             print(f"Error parsing ledger entry or timestamp: {e}")
#                             continue # Skip invalid lines
# 
#             except Exception as e:
#                 print(f"An error occurred while reading the ledger: {e}")
#                 current_velocity = 0 # Assume 0 velocity on error
#                 recent_transactions = [] # Clear any partial results
# 
# 
#             current_velocity = len(recent_transactions)
# 
# 
#         print(f"   Detected {current_velocity} transactions in the last {self.velocity_time_window}.")
# 
#         # Check if velocity falls below the threshold
#         if current_velocity < self.velocity_threshold_transactions_per_window:
#             alert_message = f"Transaction velocity decline detected! Only {current_velocity} transactions in the last {self.velocity_time_window}, below threshold of {self.velocity_threshold_transactions_per_window}."
#             print(f"🚨 {alert_message}")
#             alert_record = {
#                 "type": "predictive_maintenance_alert",
#                 "metric": "transaction_velocity",
#                 "current_velocity": current_velocity,
#                 "threshold": self.velocity_threshold_transactions_per_window,
#                 "time_window_seconds": self.velocity_time_window.total_seconds(),
#                 "explanation": alert_message
#             }
#             # Log the alert via the kernel
#             logged_receipt = self.kernel.evaluate({"event_type": "SYSTEM_EVENT", "data": alert_record, "agent": "PredictiveMaintenance"})
#             if logged_receipt and logged_receipt.get("id") and not logged_receipt["id"].startswith("mock_"):
#                  self.recent_logged_events.append(logged_receipt)
# 
#         else:
#             print("✅ Transaction velocity is healthy.")
# 
#         # Update last check time (optional, could be used to control check frequency)
#         self.last_maintenance_check = time.time()
# 
# 
#     def get_status(self) -> Dict[str, Dict[str, float]]:
#         """Return current balances and staked balances."""
#         status = {}
#         all_agents = set(list(self.balances.keys()) + list(self.staked_balances.keys()))
#         for agent in all_agents:
#             status[agent] = {
#                 "balance": round(self.balances[agent], 3),
#                 "staked_balance": round(self.staked_balances[agent], 3)
#             }
#         return status
# 
#     def get_stakes_by_assessment(self, assessment_id: str) -> Dict[str, float]:
#         """Return stakes for a specific assessment."""
#         # Note: self.stakes_by_assessment is agent -> assessment -> amount
#         # The request seems to imply assessment -> {agent: amount}, so let's adapt
#         stakes_on_id: Dict[str, float] = {}
#         for agent, stakes in self.stakes_by_assessment.items():
#             if assessment_id in stakes:
#                  stakes_on_id[agent] = stakes[assessment_id]
#         return stakes_on_id
# 
# 
#     def get_total_staked_on_assessment(self, assessment_id: str) -> float:
#         """Return total fuel staked on a specific assessment."""
#         # Recalculate from stakes_by_assessment to be robust
#         total = sum(self.stakes_by_assessment[agent].get(assessment_id, 0.0) for agent in self.stakes_by_assessment)
#         return round(total, 3)
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#      # Ensure necessary classes are available for the demo
#      # Define mocks if not already defined globally
#      if 'GovernanceKernelV2' not in globals():
#          print("GovernanceKernelV2 not found. Using a mock setup for demonstration.")
#          class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#                   # Read existing entries if file exists
#                   self._entries = []
#                   if self.path.exists():
#                       try:
#                           with self.path.open("r") as f:
#                               for line in f:
#                                   self._entries.append(json.loads(line))
#                       except Exception as e:
#                           print(f"Error reading existing mock ledger file: {e}")
# 
# 
#              def append(self, record):
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = "mock"
#                   record["hash"] = "mock"
#                   record["id"] = str(uuid.uuid4()) # Add a mock ID
#                   self._entries.append(record) # Add to in-memory list
#                   # Simulate writing to file (optional for mock, but good for testing read logic)
#                   try:
#                        with self.path.open("a") as f:
#                             f.write(json.dumps(record) + "\n")
#                   except Exception as e:
#                        print(f"Mock Ledger write error: {e}")
#                   return record
#              def verify(self):
#                   return True
#              def read_all(self): # Helper to read all entries for mock velocity check
#                   # In a real scenario, you'd read from the file
#                   # Ensure we return a copy to prevent external modification
#                   return list(self._entries)
# 
#          class MockReceiptWriter: # Define ReceiptWriter if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.ledger = MockLedger(ledger_path)
#               def log(self, event_type, data):
#                    return self.ledger.append({"event_type": event_type, "data": data})
#               def verify_ledger(self):
#                    return self.ledger.verify()
#          class GovernanceKernelV2: # Define GovernanceKernelV2 if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    self.writer = MockReceiptWriter(ledger_path)
#                    self.agent_reputation = defaultdict(lambda: 0.5)
#               def evaluate(self, event):
#                    print(f"Mock GovernanceKernelV2 received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
#                    # Simulate reputation update if event data has metabolism info and agent
#                    if "metabolism" in event.get("data", {}).get("type", "") and event.get("agent"):
#                         agent = event["agent"]
#                         clarity_gain = event["data"].get("clarity", 0.0)
#                         entropy_burn = event["data"].get("entropy", 0.0)
#                         change = (clarity_gain * 0.1) - (entropy_burn * 0.05)
#                         new_reputation = self.agent_reputation[agent] + change
#                         self.agent_reputation[agent] = max(0.0, min(1.0, new_reputation))
#                         print(f"Mock Reputation update for {agent}: {self.agent_reputation[agent]:.3f}")
#                    # Return a mock receipt with an ID
#                    return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#               def get_reputation(self, agent):
#                    return self.agent_reputation.get(agent, 0.5)
#          import time # Import time for mock timestamp
#          import uuid # Import uuid for mock ID
#          from pathlib import Path # Import Path for mock ledger path
#          # Define GovernanceKernel if not already defined (for fallback scenario)
#          if 'GovernanceKernel' not in globals():
#               class GovernanceKernel:
#                    def __init__(self, ledger_path="ledger.jsonl"):
#                        self.writer = MockReceiptWriter(ledger_path)
#                        print("Mock Base GovernanceKernel initialized.")
#                    def evaluate(self, event):
#                        print(f"Mock Base GovernanceKernel received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
#                        return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#                    def verify_ledger(self):
#                        print("Mock Base GovernanceKernel verified ledger.")
#                        return True
# 
# 
#      # Clean up ledger file for a fresh demo run if it exists
#      ledger_path = Path("/content/ledger.jsonl")
#      if ledger_path.exists():
#           try:
#               # Keep the first few lines to simulate existing history if needed for other parts
#               with open(ledger_path, 'r') as f:
#                    lines = f.readlines()
#               with open(ledger_path, 'w') as f:
#                    # Keep, say, the first 5 lines, or clear entirely for a truly fresh start
#                    f.writelines(lines[:5])
#               print(f"Cleaned up existing ledger file: {ledger_path}")
#           except Exception as e:
#               print(f"Error cleaning up ledger file: {e}")
# 
# 
#      market = ClarityMarket()
# 
#      # Seed some initial balances for demonstration
#      market.balances["AgentA"] = 100.0
#      market.balances["AgentB"] = 50.0
#      market.balances["AgentC"] = 200.0
# 
#      print("\n--- Demonstrating Token Transactions ---")
#      print("\nInitial Balances:", json.dumps(market.get_status(), indent=2))
# 
#      # Demonstrate `transfer_fuel`
#      print("\nDemonstrating transfers:")
#      market.transfer_fuel("AgentA", "AgentB", 20.0)
#      market.transfer_fuel("AgentC", "AgentA", 50.0)
#      market.transfer_fuel("AgentB", "AgentC", 10.0)
#      market.transfer_fuel("AgentA", "AgentD", 30.0) # Transfer to a new agent
# 
#      # Demonstrate failed transaction
#      market.transfer_fuel("AgentB", "AgentA", 100.0) # Should fail due to insufficient funds
#      market.transfer_fuel("AgentA", "AgentB", -10.0) # Should fail due to negative amount
# 
#      print("\nBalances after transfers:", json.dumps(market.get_status(), indent=2))
# 
#      print("\n--- Demonstrating Staking and Slashing ---")
#      # Simulate assessment IDs
#      assessment_1_id = str(uuid.uuid4())
#      assessment_2_id = str(uuid.uuid4())
# 
#      # Demonstrate `stake_fuel`
#      print(f"\nDemonstrating staking on assessments (ID 1: {assessment_1_id}, ID 2: {assessment_2_id}):")
#      market.stake_fuel("AgentA", 20.0, assessment_1_id) # AgentA stakes 20 on assessment 1
#      market.stake_fuel("AgentB", 10.0, assessment_1_id) # AgentB stakes 10 on assessment 1
#      market.stake_fuel("AgentA", 15.0, assessment_2_id) # AgentA stakes 15 on assessment 2
#      market.stake_fuel("AgentC", 50.0, assessment_1_id) # AgentC stakes 50 on assessment 1
#      market.stake_fuel("AgentB", 30.0, assessment_2_id) # AgentB stakes 30 on assessment 2 (should fail, only 40 balance after initial seed)
# 
#      print("\nBalances after staking:", json.dumps(market.get_status(), indent=2))
#      print(f"\nStakes on Assessment {assessment_1_id}:", market.get_stakes_by_assessment(assessment_1_id))
#      print(f"Total staked on Assessment {assessment_1_id}: {market.get_total_staked_on_assessment(assessment_1_id)}")
# 
#      print(f"\nStakes on Assessment {assessment_2_id}:", market.get_stakes_by_assessment(assessment_2_id))
#      print(f"Total staked on Assessment {assessment_2_id}: {market.get_total_staked_on_assessment(assessment_2_id)}")
# 
# 
#      # Demonstrate `slash_fuel` based on assessment outcome (simulated)
#      print("\nDemonstrating slashing fuel:")
#      # Simulate assessment 1 being deemed inaccurate
#      print(f"\nAssessment {assessment_1_id} deemed inaccurate. Slashing agents who staked on it.")
#      # Slash AgentA's stake on assessment 1 (20 fuel staked)
#      market.slash_fuel("AgentA", 20.0, "Staked on inaccurate assessment", assessment_id=assessment_1_id)
#       # Slash AgentB's stake on assessment 1 (10 fuel staked)
#      market.slash_fuel("AgentB", 10.0, "Staked on inaccurate assessment", assessment_id=assessment_1_id)
#       # Slash AgentC's stake on assessment 1 (50 fuel staked)
#      market.slash_fuel("AgentC", 50.0, "Staked on inaccurate assessment", assessment_id=assessment_1_id)
# 
#      # Try to slash AgentA more than they staked on assessment 2
#      market.slash_fuel("AgentA", 20.0, "Attempted over-slashing on assessment 2", assessment_id=assessment_2_id) # Should only slash 15
# 
#      # Try to slash an agent with no stake on a specific assessment
#      market.slash_fuel("AgentD", 10.0, "Slashing attempt on agent with no stake on assessment", assessment_id=assessment_1_id) # Should fail
# 
#      # Try to slash an agent with insufficient total staked fuel
#      market.slash_fuel("AgentA", 50.0, "Attempted over-slashing total stake") # Should fail, AgentA's total staked is now 15
# 
#      print("\nBalances after slashing:", json.dumps(market.get_status(), indent=2))
#      print(f"\nStakes on Assessment {assessment_1_id} after slashing:", market.get_stakes_by_assessment(assessment_1_id))
#      print(f"Total staked on Assessment {assessment_1_id} after slashing: {market.get_total_staked_on_assessment(assessment_1_id)}")
# 
#      print(f"\nStakes on Assessment {assessment_2_id} after slashing:", market.get_stakes_by_assessment(assessment_2_id))
#      print(f"Total staked on Assessment {assessment_2_id} after slashing: {market.get_total_staked_on_assessment(assessment_2_id)}")
# 
#      # Demonstrate unstaking from remaining staked balance
#      market.unstake_fuel("AgentA", 5.0) # Unstake from remaining staked on assessment 2 (should be 10)
#      market.unstake_fuel("AgentA", 10.0) # Unstake remaining staked on assessment 2 (should be 0)
#      market.unstake_fuel("AgentB", 5.0) # Should fail (staked is 0)
# 
# 
#      print("\nBalances after unstaking:", json.dumps(market.get_status(), indent=2))
# 
# 
#      print("\n--- Demonstrating External Data Integration ---")
#      # Demonstrate `integrate_external_data`
#      esg_data = market.integrate_external_data("ESG", "https://api.example.com/esg/company_xyz")
#      print("Fetched ESG Data Sample:", json.dumps(esg_data, indent=2))
# 
#      carbon_data = market.integrate_external_data("CarbonCredit", "https://api.carbonmarket.net/spot_prices")
#      print("Fetched Carbon Credit Data Sample:", json.dumps(carbon_data, indent=2))
# 
#      unknown_data = market.integrate_external_data("Weather", "https://api.weather.com/forecast")
#      print("Fetched Unknown Data Sample:", json.dumps(unknown_data, indent=2))
# 
# 
#      print("\n--- Demonstrating Predictive Maintenance ---")
#      # Simulate some recent transactions within the small time window
#      print("\nSimulating transactions within the velocity window...")
#      market.transfer_fuel("AgentA", "AgentB", 1.0)
#      market.stake_fuel("AgentB", 2.0, str(uuid.uuid4()))
#      market.transfer_fuel("AgentB", "AgentA", 0.5)
# 
#      # Run predictive maintenance check - should be healthy
#      print("\nRunning first maintenance check (should be healthy):")
#      market.predictive_maintenance()
# 
#      # Simulate a period of low transaction activity (by adding older entries to the ledger)
#      print(f"\nSimulating low transaction activity by adding old entries to ledger (waiting {market.velocity_time_window.total_seconds() + 1} seconds in simulation)...")
#      # Add some old transactions manually to the ledger file
#      ledger_path = Path("/content/ledger.jsonl")
#      if ledger_path.exists():
#           try:
#               with open(ledger_path, "a") as f:
#                   old_time = datetime.utcnow() - market.velocity_time_window - timedelta(seconds=1) # Just outside the window
#                   old_timestamp_str = old_time.strftime("%Y-%m-%dT%H:%M:%SZ")
#                   old_transfer = {"event_type": "SYSTEM_EVENT", "data": {"type": "fuel_transfer", "sender": "AgentX", "recipient": "AgentY", "amount": 1.0}, "timestamp": old_timestamp_str, "id": str(uuid.uuid4())}
#                   f.write(json.dumps(old_transfer) + "\n")
#               print("Mock ledger file updated with old entries.")
#           except Exception as e:
#               print(f"Error writing mock ledger file: {e}")
#      else:
#           print(f"Ledger file not found at {ledger_path}. Cannot add old entries for demo.")
# 
# 
#      # Run predictive maintenance check again - should detect decline
#      print("\nRunning second maintenance check (should detect decline):")
#      market.predictive_maintenance()
# 
#      print("\n--- Predictive Maintenance Demo Complete ---")
# 
#      print("\nClarity Market all features demonstration complete.")

"""## Summary:

### Data Analysis Key Findings

*   The `ClarityMarket` class successfully manages agent balances and staked fuel using `defaultdict(float)`.
*   Implemented methods (`transfer_fuel`, `stake_fuel`, `unstake_fuel`, `slash_fuel`) handle token movements with checks for sufficient funds and valid amounts, logging each transaction as a `SYSTEM_EVENT`.
*   Staking (`stake_fuel`) is linked to specific `assessment_id`s, allowing for tracking stakes on individual assessments (`stakes_by_assessment`).
*   Slashing (`slash_fuel`) correctly reduces staked fuel, prioritizing amounts staked on a specific assessment if provided, and prevents slashing more than available staked fuel.
*   The `integrate_external_data` method simulates fetching data from external APIs (e.g., ESG, Carbon Credit) and logs the integration event with the simulated payload.
*   The `predictive_maintenance` function reads transaction data from the `ledger.jsonl` file, calculates the number of qualifying transactions within a defined time window (defaulting to 30 seconds for the demo), and logs a `predictive_maintenance_alert` if the transaction velocity falls below a specified threshold (defaulting to 2 transactions per window).
*   Integration with a `GovernanceKernelV2` (or a mock version if not available) ensures all key market events are logged to the ledger for traceability.
*   The demonstration block successfully illustrates token transfers, staking on assessments, slashing based on assessment outcomes, unstaking, external data integration, and both healthy and declining transaction velocity scenarios detected by the predictive maintenance function.

### Insights or Next Steps

*   The current `unstake_fuel` method unstakes from an agent's total staked balance. A future improvement could involve specifying which assessment stake to unstake from, or implementing a lock-up period for stakes linked to assessments until the assessment outcome is finalized.
*   The `slash_fuel` method currently removes fuel from the staked balance. A next step could involve distributing the slashed fuel (e.g., to agents who staked correctly or to a community pool) instead of just removing it from circulation.

# Task
Create `audit_suite.py` that adds:
- `/verify` and `/proof` API routes to world_receipt_protocol,
- ledger Merkle-tree validation (using hashlib),
- zero-knowledge proof simulation: verify contradiction correctness without revealing data,
- provenance explorer function that reconstructs event lineage (source → contradiction → resolution → amendment).

Provide fully runnable Python code compatible with FastAPI.
Return both backend and minimal CLI commands for audit testing.

## Define the audit suite class

### Subtask:
Create the basic structure for the `AuditSuite` class, initializing it with access to the ledger.

**Reasoning**:
The subtask requires creating a new Python file and defining a class with an initialization method that interacts with the ledger. I will use the `%%writefile` magic command to create the file and add the basic class structure and initialization logic as described in the instructions.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile audit_suite.py
# """
# Tessrax Audit Suite v1.0
# ------------------------
# Provides tools for verifying ledger integrity, generating proofs,
# simulating zero-knowledge proofs, and exploring event provenance.
# """
# 
# import json
# import hashlib
# from typing import Dict, Any, List
# from pathlib import Path
# from collections import defaultdict # Might be useful for provenance
# 
# # The Ledger class should be available in the environment
# # from ledger import Ledger
# 
# class AuditSuite:
#     """
#     Provides auditing tools for the Tessrax Ledger.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Use the Ledger class already defined in the environment
#         # Assuming Ledger class is available globally or can be imported
#         if 'Ledger' in globals():
#             self.ledger = Ledger(ledger_path)
#         else:
#             # Mock Ledger if not found (for isolated testing)
#             print("Warning: Ledger class not found. Using a mock Ledger.")
#             class MockLedger:
#                  def __init__(self, path="ledger.jsonl"):
#                       self.path = Path(f"/content/{path}")
#                       self.path.touch(exist_ok=True)
#                       self._entries = [] # Keep entries in memory for mock reading
#                  def append(self, record):
#                       print("Mock Ledger append called.")
#                       self._entries.append(record)
#                       return record
#                  def verify(self):
#                       print("Mock Ledger verify called.")
#                       return True
#                  def read_all(self):
#                       return list(self._entries) # Return a copy
# 
#             self.ledger = MockLedger(ledger_path)
# 
#         self.ledger_path = Path(f"/content/{ledger_path}") # Store path as Path object
#         print("🔍 Audit Suite initialized.")
# 
#     # Placeholder methods for future implementation
#     def build_merkle_tree(self) -> List[str]:
#         """Builds a Merkle tree from ledger entry hashes."""
#         print("Building Merkle tree (placeholder)...")
#         return [] # Placeholder
# 
#     def verify_merkle_proof(self, entry_hash: str, proof: List[str], root_hash: str) -> bool:
#         """Verifies a Merkle proof for a specific entry."""
#         print("Verifying Merkle proof (placeholder)...")
#         return False # Placeholder
# 
#     def simulate_zkp_verification(self, contradiction_data: Dict[str, Any], alleged_severity: str) -> bool:
#         """Simulates zero-knowledge proof verification for contradiction correctness."""
#         print("Simulating ZKP verification (placeholder)...")
#         # In a real ZKP, this would involve a verifier receiving a proof and public inputs
#         # and checking the proof without seeing the private data (contradiction details).
#         # For simulation, we can use the actual data but pretend we are not.
#         # A simple simulation might be to check if the alleged severity is "plausible"
#         # based on *some* public information or a simplified check.
#         # e.g., if alleged_severity is "high", is the contradiction type one that *can* be high?
#         print(f"Simulating ZKP verification for alleged severity '{alleged_severity}'...")
#         # Mock ZKP logic: just check if the alleged severity is one of the valid levels
#         valid_severities = ["low", "medium", "high"]
#         is_valid = alleged_severity in valid_severities
#         print(f"ZKP simulation result: {is_valid} (Alleged severity is a valid level)")
#         return is_valid
# 
# 
#     def explore_provenance(self, event_id: str) -> List[Dict[str, Any]]:
#         """Reconstructs the lineage of an event from the ledger."""
#         print(f"Exploring provenance for event ID: {event_id} (placeholder)...")
#         lineage = []
#         # Placeholder logic: read all entries and look for source/related fields
#         # A real implementation would build a graph or traverse explicitly linked records
#         entries = []
#         try:
#             with self.ledger_path.open("r") as f:
#                 for line in f:
#                     try:
#                         entries.append(json.loads(line))
#                     except json.JSONDecodeError:
#                         continue
#         except FileNotFoundError:
#             print(f"Ledger file not found at {self.ledger_path}.")
#             return lineage
# 
#         # Simple forward/backward traversal simulation
#         current_id = event_id
#         visited_ids = set()
# 
#         while current_id and current_id not in visited_ids:
#              visited_ids.add(current_id)
#              current_entry = next((e for e in entries if e.get("id") == current_id), None)
# 
#              if current_entry:
#                   lineage.append(current_entry)
# 
#                   # Look for the source of this entry
#                   source_info = current_entry.get("data", {}).get("source")
#                   if isinstance(source_info, dict):
#                        # Look for a specific source ID if available
#                        source_id = source_info.get("id")
#                        if source_id and source_id != current_id: # Avoid self-referencing
#                             current_id = source_id
#                             continue # Found source, continue tracing backward
# 
#                   # If no explicit source ID in data, look for prev_hash (basic chain)
#                   prev_hash = current_entry.get("prev_hash")
#                   if prev_hash and prev_hash != "0" * 64 and prev_hash != "mock": # Exclude root/mock hash
#                        # Find the entry with this hash
#                        prev_entry = next((e for e in entries if e.get("hash") == prev_hash), None)
#                        if prev_entry:
#                             current_id = prev_entry.get("id")
#                             continue # Found previous in chain, continue tracing backward
# 
# 
#                   # If neither explicit source nor prev_hash link leads to a new entry, stop
#                   current_id = None
#              else:
#                   # Entry not found, stop tracing
#                   current_id = None
# 
# 
#         # Reverse the lineage to show flow from source to event
#         return lineage[::-1] if lineage else []
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#     # Ensure Ledger is available for the demo
#     if 'Ledger' not in globals():
#         print("Ledger class not found. Using a mock setup for demonstration.")
#         class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#                   self._entries = [] # Keep entries in memory for mock reading
#              def append(self, record):
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64 # Basic chaining
#                   record["hash"] = hashlib.sha256(json.dumps({k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}, sort_keys=True).encode()).hexdigest()
#                   record["id"] = str(uuid.uuid4()) # Add a mock ID
#                   self._entries.append(record) # Add to in-memory list
#                   # Simulate writing to file (optional for mock, but good for testing read logic)
#                   try:
#                        with self.path.open("a") as f:
#                             f.write(json.dumps(record) + "\n")
#                   except Exception as e:
#                        print(f"Mock Ledger write error: {e}")
#                   return record
#              def verify(self):
#                   print("Mock Ledger verify called.")
#                   # Basic mock verification
#                   prev_hash = "0"*64
#                   for entry in self._entries:
#                       expected_hash = hashlib.sha256(json.dumps({k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}, sort_keys=True).encode()).hexdigest()
#                       if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                           print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                           return False
#                       prev_hash = entry.get("hash", prev_hash) # Use current hash for next prev_hash
#                   return True
#              def read_all(self):
#                   return list(self._entries) # Return a copy
# 
#         import time # Import time for mock timestamp
#         import uuid # Import uuid for mock ID
#         import hashlib # Import hashlib for mock hashing
# 
# 
#     audit_suite = AuditSuite()
# 
#     # Demonstrate Merkle Tree (placeholder)
#     print("\n--- Demonstrating Merkle Tree (Placeholder) ---")
#     merkle_root = audit_suite.build_merkle_tree()
#     print(f"Simulated Merkle Root: {merkle_root}")
#     proof_valid = audit_suite.verify_merkle_proof("fake_id", ["fake_proof"], "fake_root")
#     print(f"Simulated Merkle Proof Verification: {proof_valid}")
# 
# 
#     # Demonstrate ZKP Simulation
#     print("\n--- Demonstrating ZKP Simulation ---")
#     contradiction_data_high = {"description": "Conflicting reports", "type": "textual"}
#     contradiction_data_low = {"deviation": 0.01, "type": "numeric"}
#     alleged_severity_high = "high"
#     alleged_severity_low = "low"
#     alleged_severity_invalid = "critical"
# 
#     print(f"\nZKP simulation for a high severity claim:")
#     audit_suite.simulate_zkp_verification(contradiction_data_high, alleged_severity_high)
# 
#     print(f"\nZKP simulation for a low severity claim:")
#     audit_suite.simulate_zkp_verification(contradiction_data_low, alleged_severity_low)
# 
#     print(f"\nZKP simulation for an invalid severity claim:")
#     audit_suite.simulate_zkp_verification(contradiction_data_high, alleged_severity_invalid)
# 
# 
#     # Demonstrate Provenance Explorer
#     print("\n--- Demonstrating Provenance Explorer ---")
# 
#     # Simulate logging some events to the ledger to create a chain
#     print("\nLogging sample events to ledger for provenance demo...")
# 
#     # Clean up ledger file for fresh demo if it exists
#     ledger_path = Path("/content/ledger.jsonl")
#     if ledger_path.exists():
#          try:
#              # Keep the first few lines to simulate existing history if needed for other parts
#              with open(ledger_path, 'r') as f:
#                   lines = f.readlines()
#              with open(ledger_path, 'w') as f:
#                   # Keep, say, the first 5 lines, or clear entirely for a truly fresh start
#                   f.writelines(lines[:5])
#              print(f"Cleaned up existing ledger file: {ledger_path}")
#          except Exception as e:
#              print(f"Error cleaning up ledger file: {e}")
# 
# 
#     # Need a working Ledger instance for this part
#     if 'Ledger' in globals():
#          demo_ledger = Ledger()
#     elif isinstance(audit_suite.ledger, MockLedger):
#          demo_ledger = audit_suite.ledger # Use the mock ledger already initialized
#     else:
#          print("Cannot demonstrate provenance without a working Ledger or MockLedger.")
#          demo_ledger = None # Cannot proceed with provenance demo
# 
# 
#     if demo_ledger:
#         source_event_id = str(uuid.uuid4())
#         source_event = {"event_type": "SOURCE_DATA", "data": {"id": source_event_id, "payload": {"raw": "Initial data received"}}}
#         logged_source = demo_ledger.append(source_event)
#         print(f"Logged Source Event: {logged_source['id']}")
# 
#         contradiction_data = {
#             "type": "textual",
#             "claim_a": "Data is consistent.",
#             "claim_b": "Data has conflicts.",
#             "source": {"type": "event", "id": logged_source['id']} # Link to source event
#         }
#         contradiction_event = {"event_type": "CONTRADICTION", "data": contradiction_data}
#         logged_contradiction = demo_ledger.append(contradiction_event)
#         print(f"Logged Contradiction Event: {logged_contradiction['id']}")
# 
#         resolution_data = {
#             "type": "manual",
#             "status": "resolved",
#             "details": "Conflicts manually corrected.",
#             "source": {"type": "event", "id": logged_contradiction['id']} # Link to contradiction
#         }
#         resolution_event = {"event_type": "RESOLUTION", "data": resolution_data}
#         logged_resolution = demo_ledger.append(resolution_event)
#         print(f"Logged Resolution Event: {logged_resolution['id']}")
# 
#         amendment_data = {
#             "type": "rule_update",
#             "rule_name": "data_consistency",
#             "details": "Added new validation step.",
#             "source": {"type": "event", "id": logged_resolution['id']} # Link to resolution
#         }
#         amendment_event = {"event_type": "AMENDMENT", "data": amendment_data}
#         logged_amendment = demo_ledger.append(amendment_event)
#         print(f"Logged Amendment Event: {logged_amendment['id']}")
# 
# 
#         # Now explore provenance starting from the amendment event
#         print(f"\nExploring provenance chain from Amendment Event ({logged_amendment['id']}):")
#         provenance_chain = audit_suite.explore_provenance(logged_amendment['id'])
# 
#         print("\nProvenance Chain:")
#         if provenance_chain:
#             for entry in provenance_chain:
#                 print(f"- ID: {entry.get('id')}, Type: {entry.get('event_type')}, Timestamp: {entry.get('timestamp')}")
#                 print(f"  Data: {json.dumps(entry.get('data', {}), indent=2)}")
#         else:
#             print("No provenance chain found.")
# 
#     print("\nAudit Suite basic structure and demo complete.")

"""## Implement merkle tree validation

### Subtask:
Implement functions to build a Merkle tree from ledger entries and verify the integrity of the tree and individual entries using `hashlib`.

**Reasoning**:
Implement the methods for building and verifying the Merkle tree within the AuditSuite class, using hashlib as specified. This involves hashing individual entries, building layers of the tree, getting proofs, and verifying proofs.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile audit_suite.py
# """
# Tessrax Audit Suite v1.0
# ------------------------
# Provides tools for verifying ledger integrity, generating proofs,
# simulating zero-knowledge proofs, and exploring event provenance.
# """
# 
# import json
# import hashlib
# from typing import Dict, Any, List, Tuple
# from pathlib import Path
# from collections import defaultdict # Might be useful for provenance
# import time # Import time for mock Ledger timestamp
# import uuid # Import uuid for mock Ledger ID
# 
# 
# # The Ledger class should be available in the environment
# # from ledger import Ledger
# 
# class AuditSuite:
#     """
#     Provides auditing tools for the Tessrax Ledger.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Use the Ledger class already defined in the environment
#         # Assuming Ledger class is available globally or can be imported
#         if 'Ledger' in globals():
#             self.ledger = Ledger(ledger_path)
#         else:
#             # Mock Ledger if not found (for isolated testing)
#             print("Warning: Ledger class not found. Using a mock Ledger.")
#             class MockLedger:
#                  def __init__(self, path="ledger.jsonl"):
#                       self.path = Path(f"/content/{path}")
#                       self.path.touch(exist_ok=True)
#                       self._entries = [] # Keep entries in memory for mock reading
#                       # Read existing entries if file exists for persistence in mock
#                       if self.path.exists():
#                           try:
#                               with self.path.open("r") as f:
#                                   for line in f:
#                                       self._entries.append(json.loads(line))
#                           except Exception as e:
#                               print(f"Error reading existing mock ledger file: {e}")
# 
# 
#                  def append(self, record):
#                       # Simulate hashing and linking like a real ledger
#                       record_to_hash = {k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}
#                       record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                       record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64 # Basic chaining
#                       record["hash"] = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                       record["id"] = record.get("id", str(uuid.uuid4())) # Use existing ID or generate new
#                       self._entries.append(record) # Add to in-memory list
#                       # Simulate writing to file
#                       try:
#                            with self.path.open("a") as f:
#                                 f.write(json.dumps(record) + "\n")
#                       except Exception as e:
#                            print(f"Mock Ledger write error: {e}")
#                       return record
#                  def verify(self):
#                       print("Mock Ledger verify called.")
#                       # Basic mock verification
#                       prev_hash = "0"*64
#                       for entry in self._entries:
#                           record_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                           expected_hash = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                           if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                               print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                               return False
#                           prev_hash = entry.get("hash", prev_hash) # Use current hash for next prev_hash
#                       return True
#                  def read_all(self) -> List[Dict[str, Any]]:
#                       # In a real scenario, you'd read from the file
#                       # Ensure we return a copy to prevent external modification
#                       return list(self._entries)
# 
#             self.ledger = MockLedger(ledger_path)
# 
#         self.ledger_path = Path(f"/content/{ledger_path}") # Store path as Path object
#         print("🔍 Audit Suite initialized.")
# 
# 
#     def _hash_entry(self, entry: Dict[str, Any]) -> str:
#         """Compute SHA-256 hash of a single ledger entry dictionary (excluding 'hash' and 'prev_hash' fields)."""
#         # Create a dictionary with only the fields to be hashed
#         entry_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#         # Ensure consistent order by sorting keys before hashing
#         encoded_entry = json.dumps(entry_to_hash, sort_keys=True).encode('utf-8')
#         return hashlib.sha256(encoded_entry).hexdigest()
# 
#     def _build_merkle_tree_layer(self, hashes: List[str]) -> List[str]:
#         """Computes the next layer of the Merkle tree from a list of hashes."""
#         next_layer = []
#         # Handle odd number of hashes by duplicating the last one
#         if len(hashes) % 2 != 0:
#             hashes.append(hashes[-1])
# 
#         # Hash pairs of adjacent hashes
#         for i in range(0, len(hashes), 2):
#             combined_hash = hashes[i] + hashes[i+1]
#             next_layer.append(hashlib.sha256(combined_hash.encode('utf-8')).hexdigest())
# 
#         return next_layer
# 
# 
#     def build_merkle_tree(self) -> Tuple[str, List[str], List[List[str]]]:
#         """
#         Builds a Merkle tree from ledger entries.
#         Returns the root hash, the list of leaf hashes, and all layers of the tree.
#         """
#         print("Building Merkle tree from ledger entries...")
#         entries = self.ledger.read_all() # Get all entries from the ledger
# 
#         if not entries:
#             print("No entries in the ledger to build a Merkle tree.")
#             return "", [], []
# 
#         # Hash each entry to get the leaf nodes
#         leaf_hashes = [self._hash_entry(entry) for entry in entries]
# 
#         if not leaf_hashes:
#              print("Failed to generate leaf hashes.")
#              return "", [], []
# 
# 
#         layers = [leaf_hashes]
#         current_layer = leaf_hashes
# 
#         # Build subsequent layers until only one hash remains (the root)
#         while len(current_layer) > 1:
#             current_layer = self._build_merkle_tree_layer(current_layer)
#             layers.append(current_layer)
# 
#         merkle_root = layers[-1][0] if layers else ""
#         print(f"Merkle tree built. Root hash: {merkle_root}")
#         return merkle_root, leaf_hashes, layers
# 
#     def get_merkle_proof(self, entry_hash: str, leaf_hashes: List[str], layers: List[List[str]]) -> List[str]:
#         """
#         Generates a Merkle proof for a specific ledger entry's hash.
#         The proof is a list of sibling hashes needed to reconstruct the path from
#         the entry's hash to the root.
#         """
#         try:
#             # Find the index of the entry's hash in the leaf layer
#             leaf_index = leaf_hashes.index(entry_hash)
#         except ValueError:
#             print(f"Error: Entry hash '{entry_hash}' not found in leaf hashes.")
#             return []
# 
#         proof = []
#         current_index = leaf_index
# 
#         # Traverse the layers from the leaf up to the root
#         for i in range(len(layers) - 1):
#             layer = layers[i]
#             # Determine the sibling's index
#             if current_index % 2 == 0: # If current hash is on the left
#                 sibling_index = current_index + 1
#             else: # If current hash is on the right
#                 sibling_index = current_index - 1
# 
#             # Ensure sibling index is within bounds (handles duplicated last hash)
#             if sibling_index < len(layer):
#                  proof.append(layer[sibling_index])
#             else:
#                  # This case should ideally not happen with the duplication logic in _build_merkle_tree_layer
#                  # but as a safeguard, append the last hash if it was duplicated
#                  if current_index == len(layer) - 1 and len(layer) % 2 != 0:
#                       proof.append(layer[current_index]) # The last hash is its own sibling if duplicated
# 
# 
#             # Move up to the next layer's index
#             current_index = current_index // 2 # Integer division
# 
#         return proof
# 
#     def verify_merkle_proof(self, entry_hash: str, proof: List[str], root_hash: str) -> bool:
#         """
#         Verifies a Merkle proof for a specific entry's hash against the Merkle root.
#         """
#         current_hash = entry_hash
# 
#         for sibling_hash in proof:
#             # Determine order based on current_hash relative to its sibling
#             # This requires knowing if current_hash was the left or right child.
#             # The proof contains siblings in the order needed to hash upwards.
#             # If the proof sibling is the right one, current_hash was left.
#             # If the proof sibling is the left one, current_hash was right.
#             # A robust proof should probably include direction flags, but given
#             # the proof is built bottom-up, the first sibling in the proof is
#             # the peer of the leaf hash, the second is the peer of the hash of
#             # the leaf pair, and so on. The order they are combined matters.
# 
#             # Simple approach assuming proof gives siblings in order:
#             # If the current_hash's index was even in the previous layer,
#             # its sibling was to the right. If odd, sibling was to the left.
#             # We don't have the original index here, so we rely on the proof order.
#             # Combine current_hash with the sibling hash from the proof.
#             # The order of concatenation matters for the hash.
# 
#             # Let's assume the proof is constructed such that the sibling is always on the 'opposite' side
#             # relative to the position in the pair. If the original hash was the left element of a pair,
#             # the proof provides the right element. If the original was the right, proof provides the left.
#             # When hashing, the order is always left_hash + right_hash.
#             # We need to know if current_hash was originally left or right.
# 
#             # A more standard Merkle proof includes direction flags or the original index.
#             # Without that, verifying is ambiguous.
#             # Let's re-evaluate `get_merkle_proof` and `verify_merkle_proof` together
#             # to ensure the proof structure supports unambiguous verification.
# 
#             # Revised approach: `get_merkle_proof` will store pairs of (sibling_hash, is_left_sibling)
#             # `verify_merkle_proof` will use this flag.
# 
#             # --- Re-implementing get_merkle_proof to include direction ---
#             # This requires re-running the build_merkle_tree to get the layers structure.
#             # For now, let's stick to the current proof format (list of hashes) and assume
#             # a simple concatenation order, but acknowledge this is a simplification.
#             # Let's assume the rule: if the current hash corresponds to an even index in its layer,
#             # it was the left node, so the proof sibling is the right node. If odd, it was right,
#             # proof sibling is left.
# 
#             # To verify correctly without direction flags, we would need to check both concatenations:
#             hash_left = hashlib.sha256((current_hash + sibling_hash).encode('utf-8')).hexdigest()
#             hash_right = hashlib.sha256((sibling_hash + current_hash).encode('utf-8')).hexdigest()
# 
#             # The next current hash is either hash_left or hash_right.
#             # This ambiguity is why direction flags or indices are needed.
# 
#             # Let's assume the proof is ordered such that we always hash (current_hash + sibling) if current_hash was left,
#             # and (sibling + current_hash) if current_hash was right.
#             # To know if current_hash was left or right, we'd need its index in the layer it came from.
#             # The `get_merkle_proof` method doesn't provide that context in the proof itself.
# 
#             # Let's revise the proof format in `get_merkle_proof` to `List[Tuple[str, bool]]` where bool is `is_sibling_on_right`.
#             # This means we need to rebuild the tree and proof.
# 
#             # --- Re-implementing get_merkle_proof with direction flags ---
#             # This requires the layers and original leaf index.
# 
#             # This approach seems complex without modifying how the layers are stored or how the proof is generated.
#             # Let's simplify the verification for this task, acknowledging the limitation.
#             # Assume a fixed concatenation order during verification, e.g., always current_hash + sibling.
#             # This is NOT a cryptographically sound Merkle proof verification, but fits the "using hashlib" requirement.
# 
#             # Simplified (potentially incorrect) verification:
#             current_hash = hashlib.sha256((current_hash + sibling_hash).encode('utf-8')).hexdigest()
#             # Or, more correctly, based on the original index's parity in the previous layer...
#             # Since we don't have the index, let's use the proof order as a hint.
#             # If the proof sibling is from an even index, the current hash was odd, and vice versa.
#             # This is still problematic without the layer structure during verification.
# 
#             # Let's go back to the get_merkle_proof and revise it to return a list of (sibling_hash, is_sibling_right) tuples.
#             # This will require modifying the `build_merkle_tree` return value slightly or ensuring layers are accessible.
# 
#             # Let's modify `get_merkle_proof` to take the layers and leaf_index directly.
#             # And `build_merkle_tree` returns the layers.
# 
#             # --- Revised get_merkle_proof (internal helper) ---
#             # This needs to be called from a public method that provides leaf_hashes and layers.
#             # Let's make `build_merkle_tree` return layers and leaf_hashes.
#             # And create a public `generate_merkle_proof` that calls the internal helper.
# 
#             # Let's put the core logic inside the class methods.
# 
#             # Re-evaluating the proof construction in `get_merkle_proof`:
#             # At each layer, we need the sibling hash and whether it's to the left or right of the current hash's parent.
#             # If current_index is even, sibling is at current_index + 1 (right). Parent is at current_index // 2.
#             # If current_index is odd, sibling is at current_index - 1 (left). Parent is at current_index // 2.
# 
#             # Proof format: List[Tuple[str, bool]] where bool is `is_sibling_right`.
# 
#             # Let's update the loop in `get_merkle_proof`:
#             # For each layer `i` from leaves up to root-1:
#             #  Get the current hash's index `current_index` in `layers[i]`.
#             #  Determine sibling index and `is_sibling_right`.
#             #  Add `(sibling_hash, is_sibling_right)` to the proof.
#             #  Update `current_index` to its parent's index in `layers[i+1]`.
# 
#             # This requires finding the index at each step, which can be slow.
#             # A more efficient proof structure might include the index or a path string.
#             # Sticking to the tuple format for now as it directly supports verification logic.
# 
#             # Let's refine `get_merkle_proof` and `verify_merkle_proof` using this tuple format.
# 
#             # (Continuing the implementation based on the tuple proof format)
#             # The original `get_merkle_proof` iterated through layers *upwards*.
#             # The proof should contain siblings bottom-up.
# 
#             # Let's rewrite `get_merkle_proof` and `verify_merkle_proof` based on standard practice.
# 
#             # --- Revised Merkle Proof Logic ---
# 
#             # `get_merkle_proof` (Revised)
#             # Input: entry_hash, all layers from build_merkle_tree
#             # Output: List of tuples (sibling_hash, is_sibling_right)
# 
#             # `verify_merkle_proof` (Revised)
#             # Input: entry_hash, proof (List[Tuple[str, bool]]), root_hash
#             # Process: Start with current_hash = entry_hash. For each (sibling_hash, is_sibling_right) in proof:
#             #   If is_sibling_right is True: combine current_hash (left) with sibling_hash (right)
#             #   If is_sibling_right is False: combine sibling_hash (left) with current_hash (right)
#             #   Update current_hash = new combined hash.
#             # Finally, check if current_hash == root_hash.
# 
# 
#             # Implementing this revised logic within the class methods.
# 
#             # --- Back to verify_merkle_proof implementation ---
#             # The loop iterates through `proof` (list of sibling hashes).
#             # We need to know the original index to determine left/right.
#             # The simplest way without changing `get_merkle_proof` return is to recompute the index parity at each step.
#             # This requires the original leaf index and the number of hashes in each layer.
# 
#             # Let's assume `get_merkle_proof` is fixed to return `List[Tuple[str, bool]]`.
#             # This means `build_merkle_tree` must return the layers.
# 
#             # The current `build_merkle_tree` already returns layers.
#             # Let's update `get_merkle_proof` and `verify_merkle_proof` assuming the tuple proof format.
# 
#             # --- Re-implementing get_merkle_proof ---
#             # This method needs access to the layers built by `build_merkle_tree`.
#             # Let's assume `build_merkle_tree` is called first and its result (layers) is available.
# 
#             # Let's modify `get_merkle_proof` to take `layers` as an argument.
# 
#             # (Continuing the implementation within the class)
#             # The loop in `verify_merkle_proof` should consume the proof tuples.
# 
#             # --- Back to the verify_merkle_proof loop ---
#             # current_hash = entry_hash
#             # For (sibling_hash, is_sibling_right) in proof:
#             #   if is_sibling_right:
#             #     combined_hash = hashlib.sha256((current_hash + sibling_hash).encode('utf-8')).hexdigest()
#             #   else:
#             #     combined_hash = hashlib.sha256((sibling_hash + current_hash).encode('utf-8')).hexdigest()
#             #   current_hash = combined_hash
# 
#             # return current_hash == root_hash
# 
#             # This revised logic needs to be implemented. Let's update `get_merkle_proof` first.
# 
#             # --- Re-implementing get_merkle_proof with tuple output ---
#             # Needs leaf_hashes and layers.
# 
#             # Let's put the pieces together in the code block.
# 
#             # Note: The original `get_merkle_proof` and `verify_merkle_proof` placeholders
#             # did not have the correct signature or logic for a standard Merkle tree.
#             # The implementation below will replace them with the corrected logic using
#             # the tuple proof format.
# 
#             pass # This pass is just to end this thought process block. The actual implementation is below.
# 
# 
#     def get_merkle_proof(self, entry_hash: str, leaf_hashes: List[str], layers: List[List[str]]) -> List[Tuple[str, bool]]:
#         """
#         Generates a Merkle proof for a specific ledger entry's hash.
#         The proof is a list of tuples (sibling_hash, is_sibling_right),
#         needed to reconstruct the path from the entry's hash to the root.
#         """
#         try:
#             # Find the index of the entry's hash in the leaf layer
#             current_index = leaf_hashes.index(entry_hash)
#         except ValueError:
#             print(f"Error: Entry hash '{entry_hash}' not found in leaf hashes.")
#             return []
# 
#         proof = []
# 
#         # Traverse the layers from the leaf up to the root
#         for i in range(len(layers) - 1):
#             layer = layers[i]
#             # Determine the sibling's index and position relative to the current hash
#             if current_index % 2 == 0: # If current hash is on the left in this layer's pair
#                 sibling_index = current_index + 1
#                 is_sibling_right = True
#             else: # If current hash is on the right in this layer's pair
#                 sibling_index = current_index - 1
#                 is_sibling_right = False
# 
#             # Handle the case where the last hash was duplicated for an odd layer length
#             # The sibling is the hash itself, and its position relative to the current_hash
#             # depends on whether the current_hash was the duplicated one.
#             # If current_index is the last index and the layer length is odd, the sibling is the hash itself.
#             if sibling_index >= len(layer): # This happens if current_index is the last element and the layer is odd
#                  if current_index == len(layer) - 1 and len(layer) % 2 != 0:
#                       # The sibling is the hash itself, and its relative position doesn't strictly matter for verification,
#                       # but we need to add it to the proof. Let's mark it as right sibling for consistency in hashing order.
#                       proof.append((layer[current_index], True)) # Append the hash itself as the sibling, treat as right
#                  else:
#                     # This is an unexpected scenario if the _build_merkle_tree_layer logic is correct.
#                     print(f"Warning: Sibling index {sibling_index} out of bounds for layer {i} with length {len(layer)}.")
#                     return [] # Indicate an error or unexpected structure
# 
# 
#             else:
#                 proof.append((layer[sibling_index], is_sibling_right))
# 
# 
#             # Move up to the next layer's index (parent's index)
#             current_index = current_index // 2 # Integer division
# 
#         return proof
# 
#     def verify_merkle_proof(self, entry_hash: str, proof: List[Tuple[str, bool]], root_hash: str) -> bool:
#         """
#         Verifies a Merkle proof for a specific entry's hash against the Merkle root.
#         Proof format: List of tuples (sibling_hash, is_sibling_right).
#         """
#         current_hash = entry_hash
# 
#         for sibling_hash, is_sibling_right in proof:
#             if is_sibling_right:
#                 # Current hash is on the left, sibling is on the right
#                 combined_hash = hashlib.sha256((current_hash + sibling_hash).encode('utf-8')).hexdigest()
#             else:
#                 # Current hash is on the right, sibling is on the left
#                 combined_hash = hashlib.sha256((sibling_hash + current_hash).encode('utf-8')).hexdigest()
# 
#             current_hash = combined_hash
# 
#         # After processing all siblings in the proof, the final hash should match the root hash
#         return current_hash == root_hash
# 
# 
#     def simulate_zkp_verification(self, contradiction_data: Dict[str, Any], alleged_severity: str) -> bool:
#         """Simulates zero-knowledge proof verification for contradiction correctness."""
#         print("Simulating ZKP verification (placeholder)...")
#         # In a real ZKP, this would involve a verifier receiving a proof and public inputs
#         # and checking the proof without seeing the private data (contradiction details).
#         # For simulation, we can use the actual data but pretend we are not.
#         # A simple simulation might be to check if the alleged severity is "plausible"
#         # based on *some* public information or a simplified check.
#         # e.g., if alleged_severity is "high", is the contradiction type one that *can* be high?
#         print(f"Simulating ZKP verification for alleged severity '{alleged_severity}'...")
#         # Mock ZKP logic: just check if the alleged severity is one of the valid levels
#         valid_severities = ["low", "medium", "high"]
#         is_valid = alleged_severity in valid_severities
#         print(f"ZKP simulation result: {is_valid} (Alleged severity is a valid level)")
#         return is_valid
# 
# 
#     def explore_provenance(self, event_id: str) -> List[Dict[str, Any]]:
#         """Reconstructs the lineage of an event from the ledger."""
#         print(f"Exploring provenance for event ID: {event_id} (placeholder)...")
#         lineage = []
#         # Placeholder logic: read all entries and look for source/related fields
#         # A real implementation would build a graph or traverse explicitly linked records
#         entries = self.ledger.read_all() # Get all entries
# 
#         if not entries:
#             print("No entries in the ledger to explore provenance.")
#             return lineage
# 
#         # Simple forward/backward traversal simulation
#         current_id = event_id
#         visited_ids = set()
# 
#         while current_id and current_id not in visited_ids:
#              visited_ids.add(current_id)
#              current_entry = next((e for e in entries if e.get("id") == current_id), None)
# 
#              if current_entry:
#                   lineage.append(current_entry)
# 
#                   # Look for the source of this entry in the 'data' field
#                   source_info = current_entry.get("data", {}).get("source")
#                   if isinstance(source_info, dict):
#                        # Look for a specific source ID if available
#                        source_id = source_info.get("id") or source_info.get("event_id") # Check common keys
#                        if source_id and source_id != current_id: # Avoid self-referencing
#                             current_id = source_id
#                             continue # Found source, continue tracing backward
# 
#                   # If no explicit source ID in data, look for prev_hash (basic chain linkage)
#                   # Note: prev_hash linkage is for ledger order, not necessarily causal provenance.
#                   # Causal links should ideally be explicit in the 'source' field.
#                   # Including prev_hash traversal for completeness in this demo, but explicit links are better.
#                   prev_hash = current_entry.get("prev_hash")
#                   if prev_hash and prev_hash != "0" * 64 and prev_hash != "mock": # Exclude root/mock hash
#                        # Find the entry with this hash
#                        prev_entry = next((e for e in entries if e.get("hash") == prev_hash), None)
#                        if prev_entry:
#                             current_id = prev_entry.get("id")
#                             continue # Found previous in chain, continue tracing backward
# 
# 
#                   # If neither explicit source nor prev_hash link leads to a new entry, stop
#                   current_id = None
#              else:
#                   # Entry not found, stop tracing
#                   current_id = None
# 
# 
#         # Reverse the lineage to show flow from source to event
#         return lineage[::-1] if lineage else []
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#     # Ensure Ledger is available for the demo
#     if 'Ledger' not in globals():
#         print("Ledger class not found. Using a mock setup for demonstration.")
#         class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#                   self._entries = [] # Keep entries in memory for mock reading
#                   # Read existing entries if file exists for persistence in mock
#                   if self.path.exists():
#                       try:
#                           with self.path.open("r") as f:
#                               for line in f:
#                                   self._entries.append(json.loads(line))
#                       except Exception as e:
#                           print(f"Error reading existing mock ledger file: {e}")
# 
# 
#              def append(self, record):
#                   # Simulate hashing and linking like a real ledger
#                   record_to_hash = {k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64 # Basic chaining
#                   record["hash"] = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                   record["id"] = record.get("id", str(uuid.uuid4())) # Use existing ID or generate new
#                   self._entries.append(record) # Add to in-memory list
#                   # Simulate writing to file
#                   try:
#                        with self.path.open("a") as f:
#                             f.write(json.dumps(record) + "\n")
#                   except Exception as e:
#                        print(f"Mock Ledger write error: {e}")
#                   return record
#              def verify(self):
#                   print("Mock Ledger verify called.")
#                   # Basic mock verification
#                   prev_hash = "0"*64
#                   for entry in self._entries:
#                       record_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                       expected_hash = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                       if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                           print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                           return False
#                       prev_hash = entry.get("hash", prev_hash) # Use current hash for next prev_hash
#                   return True
#              def read_all(self) -> List[Dict[str, Any]]:
#                   # In a real scenario, you'd read from the file
#                   # Ensure we return a copy to prevent external modification
#                   return list(self._entries)
# 
#         import time # Import time for mock timestamp
#         import uuid # Import uuid for mock ID
#         import hashlib # Import hashlib for mock hashing
#         from pathlib import Path # Import Path for mock ledger path
# 
# 
#     # Clean up ledger file for fresh demo if it exists
#     ledger_path = Path("/content/ledger.jsonl")
#     if ledger_path.exists():
#          try:
#              # Clear the ledger file for a clean Merkle tree demo
#              with open(ledger_path, 'w') as f:
#                   f.write("")
#              print(f"Cleared existing ledger file: {ledger_path}")
#          except Exception as e:
#              print(f"Error clearing ledger file: {e}")
# 
# 
#     audit_suite = AuditSuite()
# 
#     # Simulate logging some entries to the ledger for Merkle tree demo
#     print("\nLogging sample entries to ledger for Merkle tree demo...")
#     entries_for_merkle = [
#         {"event_type": "DATA_POINT", "data": {"value": 100}},
#         {"event_type": "DATA_POINT", "data": {"value": 200}},
#         {"event_type": "ALERT", "data": {"message": "Threshold breached"}},
#         {"event_type": "DATA_POINT", "data": {"value": 150}},
#         {"event_type": "STATUS_UPDATE", "data": {"status": "OK"}},
#     ]
# 
#     logged_entries = []
#     for entry in entries_for_merkle:
#          logged_entries.append(audit_suite.ledger.append(entry))
# 
#     # Demonstrate Merkle Tree building and verification
#     print("\n--- Demonstrating Merkle Tree ---")
#     merkle_root, leaf_hashes, layers = audit_suite.build_merkle_tree()
# 
#     if merkle_root:
#         print(f"\nBuilt Merkle Tree:")
#         print(f"  Leaf Hashes ({len(leaf_hashes)}): {[h[:6] + '...' for h in leaf_hashes]}")
#         print(f"  Root Hash: {merkle_root}")
#         # print("  Layers:", layers) # Uncomment to see all layers
# 
#         # Get a proof for a specific entry (e.g., the third entry - index 2)
#         entry_to_prove = logged_entries[2]
#         entry_hash_to_prove = audit_suite._hash_entry(entry_to_prove) # Recompute hash to be sure
#         print(f"\nGetting Merkle proof for entry ID {entry_to_prove['id']} (Hash: {entry_hash_to_prove[:6]}...)")
# 
#         merkle_proof = audit_suite.get_merkle_proof(entry_hash_to_prove, leaf_hashes, layers)
# 
#         print(f"Generated Merkle proof ({len(merkle_proof)} steps): {merkle_proof}")
# 
#         # Verify the proof
#         print(f"\nVerifying Merkle proof for entry ID {entry_to_prove['id']}...")
#         is_proof_valid = audit_suite.verify_merkle_proof(entry_hash_to_prove, merkle_proof, merkle_root)
# 
#         print(f"Merkle proof verification result: {is_proof_valid}")
# 
#         # Demonstrate verification failure with incorrect root
#         print("\nVerifying Merkle proof with incorrect root hash...")
#         is_proof_valid_fake_root = audit_suite.verify_merkle_proof(entry_hash_to_prove, merkle_proof, "fake_root_hash" + "0"*(64-14))
#         print(f"Merkle proof verification with fake root result: {is_proof_valid_fake_root}")
# 
#         # Demonstrate verification failure with incorrect proof (e.g., swap first sibling)
#         if len(merkle_proof) > 0:
#              print("\nVerifying Merkle proof with incorrect proof (swapped first sibling)...")
#              incorrect_proof = [(merkle_proof[0][0] + "1", not merkle_proof[0][1])] + merkle_proof[1:] # Modify first sibling hash/direction
#              is_proof_valid_incorrect_proof = audit_suite.verify_merkle_proof(entry_hash_to_prove, incorrect_proof, merkle_root)
#              print(f"Merkle proof verification with incorrect proof result: {is_proof_valid_incorrect_proof}")
# 
# 
#     else:
#         print("Skipping Merkle tree demo due to missing Ledger/MockLedger.")
# 
# 
#     # Demonstrate ZKP Simulation (already implemented and demonstrated in previous step)
#     print("\n--- Demonstrating ZKP Simulation (from previous step) ---")
#     contradiction_data_high = {"description": "Conflicting reports", "type": "textual"}
#     contradiction_data_low = {"deviation": 0.01, "type": "numeric"}
#     alleged_severity_high = "high"
#     alleged_severity_low = "low"
#     alleged_severity_invalid = "critical"
# 
#     print(f"\nZKP simulation for a high severity claim:")
#     audit_suite.simulate_zkp_verification(contradiction_data_high, alleged_severity_high)
# 
#     print(f"\nZKP simulation for a low severity claim:")
#     audit_suite.simulate_zkp_verification(contradiction_data_low, alleged_severity_low)
# 
#     print(f"\nZKP simulation for an invalid severity claim:")
#     audit_suite.simulate_zkp_verification(contradiction_data_high, alleged_severity_invalid)
# 
# 
#     # Demonstrate Provenance Explorer (already implemented and demonstrated in previous step)
#     print("\n--- Demonstrating Provenance Explorer (from previous step) ---")
# 
#     # Simulate logging some events to the ledger to create a chain for provenance
#     print("\nLogging sample events to ledger for provenance demo...")
# 
#     # Need a working Ledger instance for this part
#     if 'Ledger' in globals():
#          demo_ledger = Ledger()
#     elif isinstance(audit_suite.ledger, MockLedger):
#          demo_ledger = audit_suite.ledger # Use the mock ledger already initialized
#     else:
#          print("Cannot demonstrate provenance without a working Ledger or MockLedger.")
#          demo_ledger = None # Cannot proceed with provenance demo
# 
# 
#     if demo_ledger:
#         # Clear ledger again for a clean provenance chain demo
#         ledger_path = Path("/content/ledger.jsonl")
#         if ledger_path.exists():
#              try:
#                  with open(ledger_path, 'w') as f:
#                       f.write("")
#                  print(f"Cleared ledger file for provenance demo: {ledger_path}")
#              except Exception as e:
#                  print(f"Error clearing ledger file: {e}")
# 
# 
#         source_event_id = str(uuid.uuid4())
#         source_event = {"event_type": "SOURCE_DATA", "data": {"id": source_event_id, "payload": {"raw": "Initial data received"}}}
#         logged_source = demo_ledger.append(source_event)
#         print(f"Logged Source Event: {logged_source['id']}")
# 
#         contradiction_data = {
#             "type": "textual",
#             "claim_a": "Data is consistent.",
#             "claim_b": "Data has conflicts.",
#             "source": {"type": "event", "id": logged_source['id']} # Link to source event
#         }
#         contradiction_event = {"event_type": "CONTRADICTION", "data": contradiction_data}
#         logged_contradiction = demo_ledger.append(contradiction_event)
#         print(f"Logged Contradiction Event: {logged_contradiction['id']}")
# 
#         resolution_data = {
#             "type": "manual",
#             "status": "resolved",
#             "details": "Conflicts manually corrected.",
#             "source": {"type": "event", "id": logged_contradiction['id']} # Link to contradiction
#         }
#         resolution_event = {"event_type": "RESOLUTION", "data": resolution_data}
#         logged_resolution = demo_ledger.append(resolution_event)
#         print(f"Logged Resolution Event: {logged_resolution['id']}")
# 
#         amendment_data = {
#             "type": "rule_update",
#             "rule_name": "data_consistency",
#             "details": "Added new validation step.",
#             "source": {"type": "event", "id": logged_resolution['id']} # Link to resolution
#         }
#         amendment_event = {"event_type": "AMENDMENT", "data": amendment_data}
#         logged_amendment = demo_ledger.append(amendment_event)
#         print(f"Logged Amendment Event: {logged_amendment['id']}")
# 
# 
#         # Now explore provenance starting from the amendment event
#         print(f"\nExploring provenance chain from Amendment Event ({logged_amendment['id']}):")
#         provenance_chain = audit_suite.explore_provenance(logged_amendment['id'])
# 
#         print("\nProvenance Chain:")
#         if provenance_chain:
#             for entry in provenance_chain:
#                 print(f"- ID: {entry.get('id')}, Type: {entry.get('event_type')}, Timestamp: {entry.get('timestamp')}")
#                 print(f"  Data: {json.dumps(entry.get('data', {}), indent=2)}")
#         else:
#             print("No provenance chain found.")
# 
#     print("\nAudit Suite Merkle tree demo complete.")

"""## Simulate zero-knowledge proofs

### Subtask:
Implement a function to simulate zero-knowledge proofs for verifying contradiction correctness without revealing sensitive data.

**Reasoning**:
The subtask requires implementing the zero-knowledge proof simulation function. I will enhance the existing `simulate_zkp_verification` method in the `AuditSuite` class to include a more plausible simulation logic based on the contradiction type and alleged severity, as per the instructions. I will also ensure the method signature is correct and update the demonstration code to test this enhanced simulation.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile audit_suite.py
# """
# Tessrax Audit Suite v1.0
# ------------------------
# Provides tools for verifying ledger integrity, generating proofs,
# simulating zero-knowledge proofs, and exploring event provenance.
# """
# 
# import json
# import hashlib
# from typing import Dict, Any, List, Tuple
# from pathlib import Path
# from collections import defaultdict
# import time
# import uuid
# 
# # The Ledger class should be available in the environment
# # from ledger import Ledger
# 
# class AuditSuite:
#     """
#     Provides auditing tools for the Tessrax Ledger.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Use the Ledger class already defined in the environment
#         if 'Ledger' in globals():
#             self.ledger = Ledger(ledger_path)
#         else:
#             # Mock Ledger if not found (for isolated testing)
#             print("Warning: Ledger class not found. Using a mock Ledger.")
#             class MockLedger:
#                  def __init__(self, path="ledger.jsonl"):
#                       self.path = Path(f"/content/{path}")
#                       self.path.touch(exist_ok=True)
#                       self._entries = []
#                       if self.path.exists():
#                           try:
#                               with self.path.open("r") as f:
#                                   for line in f:
#                                       self._entries.append(json.loads(line))
#                           except Exception as e:
#                               print(f"Error reading existing mock ledger file: {e}")
# 
#                  def append(self, record):
#                       record_to_hash = {k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}
#                       record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                       record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64
#                       record["hash"] = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                       record["id"] = record.get("id", str(uuid.uuid4()))
#                       self._entries.append(record)
#                       try:
#                            with self.path.open("a") as f:
#                                 f.write(json.dumps(record) + "\n")
#                       except Exception as e:
#                            print(f"Mock Ledger write error: {e}")
#                       return record
#                  def verify(self):
#                       print("Mock Ledger verify called.")
#                       prev_hash = "0"*64
#                       for entry in self._entries:
#                           record_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                           expected_hash = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                           if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                               print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                               return False
#                           prev_hash = entry.get("hash", prev_hash)
#                       return True
#                  def read_all(self) -> List[Dict[str, Any]]:
#                       return list(self._entries)
# 
#             self.ledger = MockLedger(ledger_path)
# 
#         self.ledger_path = Path(f"/content/{ledger_path}")
#         print("🔍 Audit Suite initialized.")
# 
#     def _hash_entry(self, entry: Dict[str, Any]) -> str:
#         """Compute SHA-256 hash of a single ledger entry dictionary (excluding 'hash' and 'prev_hash' fields)."""
#         entry_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#         encoded_entry = json.dumps(entry_to_hash, sort_keys=True).encode('utf-8')
#         return hashlib.sha256(encoded_entry).hexdigest()
# 
#     def _build_merkle_tree_layer(self, hashes: List[str]) -> List[str]:
#         """Computes the next layer of the Merkle tree from a list of hashes."""
#         next_layer = []
#         if len(hashes) % 2 != 0:
#             hashes.append(hashes[-1])
#         for i in range(0, len(hashes), 2):
#             combined_hash = hashes[i] + hashes[i+1]
#             next_layer.append(hashlib.sha256(combined_hash.encode('utf-8')).hexdigest())
#         return next_layer
# 
#     def build_merkle_tree(self) -> Tuple[str, List[str], List[List[str]]]:
#         """
#         Builds a Merkle tree from ledger entries.
#         Returns the root hash, the list of leaf hashes, and all layers of the tree.
#         """
#         print("Building Merkle tree from ledger entries...")
#         entries = self.ledger.read_all()
# 
#         if not entries:
#             print("No entries in the ledger to build a Merkle tree.")
#             return "", [], []
# 
#         leaf_hashes = [self._hash_entry(entry) for entry in entries]
# 
#         if not leaf_hashes:
#              print("Failed to generate leaf hashes.")
#              return "", [], []
# 
#         layers = [leaf_hashes]
#         current_layer = leaf_hashes
# 
#         while len(current_layer) > 1:
#             current_layer = self._build_merkle_tree_layer(current_layer)
#             layers.append(current_layer)
# 
#         merkle_root = layers[-1][0] if layers else ""
#         print(f"Merkle tree built. Root hash: {merkle_root}")
#         return merkle_root, leaf_hashes, layers
# 
#     def get_merkle_proof(self, entry_hash: str, leaf_hashes: List[str], layers: List[List[str]]) -> List[Tuple[str, bool]]:
#         """
#         Generates a Merkle proof for a specific ledger entry's hash.
#         The proof is a list of tuples (sibling_hash, is_sibling_right),
#         needed to reconstruct the path from the entry's hash to the root.
#         """
#         try:
#             current_index = leaf_hashes.index(entry_hash)
#         except ValueError:
#             print(f"Error: Entry hash '{entry_hash}' not found in leaf hashes.")
#             return []
# 
#         proof = []
# 
#         for i in range(len(layers) - 1):
#             layer = layers[i]
#             if current_index % 2 == 0:
#                 sibling_index = current_index + 1
#                 is_sibling_right = True
#             else:
#                 sibling_index = current_index - 1
#                 is_sibling_right = False
# 
#             if sibling_index >= len(layer):
#                  if current_index == len(layer) - 1 and len(layer) % 2 != 0:
#                       proof.append((layer[current_index], True))
#                  else:
#                     print(f"Warning: Sibling index {sibling_index} out of bounds for layer {i} with length {len(layer)}. Proof generation failed.")
#                     return [] # Indicate an error
# 
#             else:
#                 proof.append((layer[sibling_index], is_sibling_right))
# 
#             current_index = current_index // 2
# 
#         return proof
# 
#     def verify_merkle_proof(self, entry_hash: str, proof: List[Tuple[str, bool]], root_hash: str) -> bool:
#         """
#         Verifies a Merkle proof for a specific entry's hash against the Merkle root.
#         Proof format: List of tuples (sibling_hash, is_sibling_right).
#         """
#         current_hash = entry_hash
# 
#         for sibling_hash, is_sibling_right in proof:
#             if is_sibling_right:
#                 combined_hash = hashlib.sha256((current_hash + sibling_hash).encode('utf-8')).hexdigest()
#             else:
#                 combined_hash = hashlib.sha256((sibling_hash + current_hash).encode('utf-8')).hexdigest()
#             current_hash = combined_hash
# 
#         return current_hash == root_hash
# 
# 
#     def simulate_zkp_verification(self, contradiction_data: Dict[str, Any], alleged_severity: str) -> bool:
#         """
#         Simulates zero-knowledge proof verification for contradiction correctness
#         without revealing sensitive data.
# 
#         Simulation Logic: Check if the alleged severity is plausible for the
#         given contradiction type. This is a simplified stand-in for a real ZKP
#         where a prover would demonstrate, without revealing details, that the
#         contradiction satisfies certain conditions corresponding to the alleged severity.
#         """
#         print(f"Simulating ZKP verification for alleged severity '{alleged_severity}'...")
# 
#         # Define plausible severity ranges for different contradiction types
#         # This is a simplified mapping for the simulation.
#         plausible_severities: Dict[str, List[str]] = {
#             "textual": ["medium", "high"], # Textual conflicts are often medium or high
#             "numeric": ["low", "medium", "high"], # Numeric deviation can be any severity
#             "semantic": ["low", "medium", "high"], # Semantic similarity can range
#             "policy_violation": ["medium", "high"], # Policy violations are usually not low
#             "system_event": ["low", "info"] # System events are typically low severity or info
#         }
# 
#         contradiction_type = contradiction_data.get("type", "unknown")
# 
#         # In a real ZKP, the verifier wouldn't see contradiction_data.
#         # They would receive a proof that demonstrates (e.g., deviation > threshold for 'high').
#         # Here, we *use* the data internally for the simulation logic to check plausibility.
# 
#         if contradiction_type in plausible_severities:
#             is_plausible = alleged_severity.lower() in plausible_severities[contradiction_type]
#             print(f"   Contradiction Type: '{contradiction_type}'")
#             print(f"   Plausible Severities for this type: {plausible_severities[contradiction_type]}")
#             print(f"   Alleged Severity '{alleged_severity}' is plausible: {is_plausible}")
#             return is_plausible
#         else:
#             print(f"   Unknown contradiction type '{contradiction_type}'. Cannot determine plausibility.")
#             # For unknown types, we might allow any severity or none, depending on policy.
#             # Let's assume any valid severity is accepted for unknown types in this simulation.
#             valid_severities = ["low", "medium", "high", "info", "none"] # Include info/none as possibilities
#             is_valid = alleged_severity.lower() in valid_severities
#             print(f"   Alleged Severity '{alleged_severity}' is a valid severity level: {is_valid}")
#             return is_valid
# 
# 
#     def explore_provenance(self, event_id: str) -> List[Dict[str, Any]]:
#         """Reconstructs the lineage of an event from the ledger."""
#         print(f"Exploring provenance for event ID: {event_id}...")
#         lineage = []
#         entries = self.ledger.read_all()
# 
#         if not entries:
#             print("No entries in the ledger to explore provenance.")
#             return lineage
# 
#         current_id = event_id
#         visited_ids = set()
# 
#         while current_id and current_id not in visited_ids:
#              visited_ids.add(current_id)
#              current_entry = next((e for e in entries if e.get("id") == current_id), None)
# 
#              if current_entry:
#                   lineage.append(current_entry)
# 
#                   source_info = current_entry.get("data", {}).get("source")
#                   if isinstance(source_info, dict):
#                        source_id = source_info.get("id") or source_info.get("event_id")
#                        if source_id and source_id != current_id:
#                             current_id = source_id
#                             continue
# 
#                   prev_hash = current_entry.get("prev_hash")
#                   if prev_hash and prev_hash != "0" * 64 and prev_hash != "mock":
#                        prev_entry = next((e for e in entries if e.get("hash") == prev_hash), None)
#                        if prev_entry:
#                             current_id = prev_entry.get("id")
#                             continue
# 
#                   current_id = None
#              else:
#                   current_id = None
# 
#         return lineage[::-1] if lineage else []
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#     # Ensure Ledger is available for the demo
#     if 'Ledger' not in globals():
#         print("Ledger class not found. Using a mock setup for demonstration.")
#         class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#                   self._entries = []
#                   if self.path.exists():
#                       try:
#                           with self.path.open("r") as f:
#                               for line in f:
#                                   self._entries.append(json.loads(line))
#                       except Exception as e:
#                           print(f"Error reading existing mock ledger file: {e}")
# 
#              def append(self, record):
#                   record_to_hash = {k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64
#                   record["hash"] = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                   record["id"] = record.get("id", str(uuid.uuid4()))
#                   self._entries.append(record)
#                   try:
#                        with self.path.open("a") as f:
#                             f.write(json.dumps(record) + "\n")
#                   except Exception as e:
#                        print(f"Mock Ledger write error: {e}")
#                   return record
#              def verify(self):
#                   print("Mock Ledger verify called.")
#                   prev_hash = "0"*64
#                   for entry in self._entries:
#                       record_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                       expected_hash = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                       if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                           print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                           return False
#                       prev_hash = entry.get("hash", prev_hash)
#                   return True
#              def read_all(self) -> List[Dict[str, Any]]:
#                   return list(self._entries)
# 
#         import time
#         import uuid
#         import hashlib
#         from pathlib import Path
# 
#     # Clean up ledger file for fresh demo if it exists
#     ledger_path = Path("/content/ledger.jsonl")
#     if ledger_path.exists():
#          try:
#              with open(ledger_path, 'w') as f:
#                   f.write("")
#              print(f"Cleared existing ledger file: {ledger_path}")
#          except Exception as e:
#              print(f"Error clearing ledger file: {e}")
# 
# 
#     audit_suite = AuditSuite()
# 
#     # --- Demonstrating ZKP Simulation ---
#     print("\n--- Demonstrating ZKP Simulation ---")
# 
#     # Example 1: Textual contradiction, alleged high severity (Plausible)
#     contradiction_data_text_high = {"type": "textual", "description": "Conflicting statements found in document A and B."}
#     alleged_severity_high = "high"
#     print(f"\nTest Case 1: Textual contradiction, alleged severity '{alleged_severity_high}'")
#     is_valid_1 = audit_suite.simulate_zkp_verification(contradiction_data_text_high, alleged_severity_high)
#     print(f"ZKP simulation result: {is_valid_1}")
# 
#     # Example 2: Numeric contradiction (small deviation), alleged medium severity (Plausible)
#     contradiction_data_numeric_medium = {"type": "numeric", "deviation": 0.15, "target": 100, "actual": 115}
#     alleged_severity_medium = "medium"
#     print(f"\nTest Case 2: Numeric contradiction (small deviation), alleged severity '{alleged_severity_medium}'")
#     is_valid_2 = audit_suite.simulate_zkp_verification(contradiction_data_numeric_medium, alleged_severity_medium)
#     print(f"ZKP simulation result: {is_valid_2}")
# 
#     # Example 3: System event, alleged high severity (Not Plausible based on simulation logic)
#     contradiction_data_system_event = {"type": "system_event", "message": "Heartbeat OK."}
#     alleged_severity_high_system = "high"
#     print(f"\nTest Case 3: System event, alleged severity '{alleged_severity_high_system}'")
#     is_valid_3 = audit_suite.simulate_zkp_verification(contradiction_data_system_event, alleged_severity_high_system)
#     print(f"ZKP simulation result: {is_valid_3}")
# 
#     # Example 4: Policy violation, alleged low severity (Not Plausible based on simulation logic)
#     contradiction_data_policy = {"type": "policy_violation", "policy": "Data Access", "action": "Unauthorized access attempt."}
#     alleged_severity_low_policy = "low"
#     print(f"\nTest Case 4: Policy violation, alleged severity '{alleged_severity_low_policy}'")
#     is_valid_4 = audit_suite.simulate_zkp_verification(contradiction_data_policy, alleged_severity_low_policy)
#     print(f"ZKP simulation result: {is_valid_4}")
# 
#     # Example 5: Unknown contradiction type, alleged high severity (Plausible as any valid severity is allowed for unknown)
#     contradiction_data_unknown = {"type": "mystery_event", "details": "Something unexpected happened."}
#     alleged_severity_high_unknown = "high"
#     print(f"\nTest Case 5: Unknown contradiction type, alleged severity '{alleged_severity_high_unknown}'")
#     is_valid_5 = audit_suite.simulate_zkp_verification(contradiction_data_unknown, alleged_severity_high_unknown)
#     print(f"ZKP simulation result: {is_valid_5}")
# 
#     # Example 6: Numeric contradiction (large deviation), alleged low severity (Not Plausible based on simulation logic)
#     contradiction_data_numeric_low = {"type": "numeric", "deviation": 0.9, "target": 10, "actual": 19}
#     alleged_severity_low = "low"
#     print(f"\nTest Case 6: Numeric contradiction (large deviation), alleged severity '{alleged_severity_low}'")
#     is_valid_6 = audit_suite.simulate_zkp_verification(contradiction_data_numeric_low, alleged_severity_low)
#     print(f"ZKP simulation result: {is_valid_6}")
# 
# 
#     # Demonstrate Merkle Tree (from previous step)
#     print("\n--- Demonstrating Merkle Tree (from previous step) ---")
#     # Re-log entries if ledger was cleared
#     entries_for_merkle = [
#         {"event_type": "DATA_POINT", "data": {"value": 100}},
#         {"event_type": "DATA_POINT", "data": {"value": 200}},
#         {"event_type": "ALERT", "data": {"message": "Threshold breached"}},
#         {"event_type": "DATA_POINT", "data": {"value": 150}},
#         {"event_type": "STATUS_UPDATE", "data": {"status": "OK"}},
#     ]
#     logged_entries = [audit_suite.ledger.append(entry) for entry in entries_for_merkle]
#     merkle_root, leaf_hashes, layers = audit_suite.build_merkle_tree()
# 
#     if merkle_root:
#         print(f"\nBuilt Merkle Tree:")
#         print(f"  Leaf Hashes ({len(leaf_hashes)}): {[h[:6] + '...' for h in leaf_hashes]}")
#         print(f"  Root Hash: {merkle_root}")
# 
#         entry_to_prove = logged_entries[2]
#         entry_hash_to_prove = audit_suite._hash_entry(entry_to_prove)
#         merkle_proof = audit_suite.get_merkle_proof(entry_hash_to_prove, leaf_hashes, layers)
#         is_proof_valid = audit_suite.verify_merkle_proof(entry_hash_to_prove, merkle_proof, merkle_root)
#         print(f"\nVerifying Merkle proof for entry ID {entry_to_prove['id']}...")
#         print(f"Merkle proof verification result: {is_proof_valid}")
#     else:
#         print("Skipping Merkle tree demo due to missing Ledger/MockLedger or no entries.")
# 
# 
#     # Demonstrate Provenance Explorer (from previous step)
#     print("\n--- Demonstrating Provenance Explorer (from previous step) ---")
# 
#     # Need a working Ledger instance for this part
#     if 'Ledger' in globals() or isinstance(audit_suite.ledger, MockLedger):
#          demo_ledger = audit_suite.ledger # Use the ledger from the audit suite instance
#          # Clear ledger again for a clean provenance chain demo if needed
#          if isinstance(demo_ledger, MockLedger): # Only clear mock ledger
#               ledger_path = Path("/content/ledger.jsonl")
#               if ledger_path.exists():
#                    try:
#                        with open(ledger_path, 'w') as f:
#                             f.write("")
#                        print(f"Cleared ledger file for provenance demo: {ledger_path}")
#                    except Exception as e:
#                        print(f"Error clearing ledger file: {e}")
#               demo_ledger._entries = [] # Also clear in-memory entries for mock
# 
# 
#          source_event_id = str(uuid.uuid4())
#          source_event = {"event_type": "SOURCE_DATA", "data": {"id": source_event_id, "payload": {"raw": "Initial data received"}}}
#          logged_source = demo_ledger.append(source_event)
#          print(f"Logged Source Event: {logged_source['id']}")
# 
#          contradiction_data = {
#              "type": "textual",
#              "claim_a": "Data is consistent.",
#              "claim_b": "Data has conflicts.",
#              "source": {"type": "event", "id": logged_source['id']}
#          }
#          contradiction_event = {"event_type": "CONTRADICTION", "data": contradiction_data}
#          logged_contradiction = demo_ledger.append(contradiction_event)
#          print(f"Logged Contradiction Event: {logged_contradiction['id']}")
# 
#          resolution_data = {
#              "type": "manual",
#              "status": "resolved",
#              "details": "Conflicts manually corrected.",
#              "source": {"type": "event", "id": logged_contradiction['id']}
#          }
#          resolution_event = {"event_type": "RESOLUTION", "data": resolution_data}
#          logged_resolution = demo_ledger.append(resolution_event)
#          print(f"Logged Resolution Event: {logged_resolution['id']}")
# 
#          amendment_data = {
#              "type": "rule_update",
#              "rule_name": "data_consistency",
#              "details": "Added new validation step.",
#              "source": {"type": "event", "id": logged_resolution['id']}
#          }
#          amendment_event = {"event_type": "AMENDMENT", "data": amendment_data}
#          logged_amendment = demo_ledger.append(amendment_event)
#          print(f"Logged Amendment Event: {logged_amendment['id']}")
# 
#          print(f"\nExploring provenance chain from Amendment Event ({logged_amendment['id']}):")
#          provenance_chain = audit_suite.explore_provenance(logged_amendment['id'])
# 
#          print("\nProvenance Chain:")
#          if provenance_chain:
#              for entry in provenance_chain:
#                  print(f"- ID: {entry.get('id')}, Type: {entry.get('event_type')}, Timestamp: {entry.get('timestamp')}")
#                  print(f"  Data: {json.dumps(entry.get('data', {}), indent=2)}")
#          else:
#              print("No provenance chain found.")
# 
#     else:
#         print("Skipping Provenance Explorer demo due to missing Ledger/MockLedger.")
# 
# 
#     print("\nAudit Suite ZKP simulation demo complete.")
#

"""## Implement provenance explorer

### Subtask:
Create a function to reconstruct the lineage of events (source → contradiction → resolution → amendment) from the ledger.

**Reasoning**:
Implement the provenance explorer function as described in the instructions by reading ledger entries and tracing backward through explicit source links and implicit `prev_hash` links. Update the demonstration to showcase this functionality.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile audit_suite.py
# """
# Tessrax Audit Suite v1.0
# ------------------------
# Provides tools for verifying ledger integrity, generating proofs,
# simulating zero-knowledge proofs, and exploring event provenance.
# """
# 
# import json
# import hashlib
# from typing import Dict, Any, List, Tuple
# from pathlib import Path
# from collections import defaultdict
# import time
# import uuid
# 
# # The Ledger class should be available in the environment
# # from ledger import Ledger
# 
# class AuditSuite:
#     """
#     Provides auditing tools for the Tessrax Ledger.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Use the Ledger class already defined in the environment
#         if 'Ledger' in globals():
#             self.ledger = Ledger(ledger_path)
#         else:
#             # Mock Ledger if not found (for isolated testing)
#             print("Warning: Ledger class not found. Using a mock Ledger.")
#             class MockLedger:
#                  def __init__(self, path="ledger.jsonl"):
#                       self.path = Path(f"/content/{path}")
#                       self.path.touch(exist_ok=True)
#                       self._entries = []
#                       if self.path.exists():
#                           try:
#                               with self.path.open("r") as f:
#                                   for line in f:
#                                       self._entries.append(json.loads(line))
#                           except Exception as e:
#                               print(f"Error reading existing mock ledger file: {e}")
# 
#                  def append(self, record):
#                       record_to_hash = {k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}
#                       record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                       record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64
#                       record["hash"] = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                       record["id"] = record.get("id", str(uuid.uuid4()))
#                       self._entries.append(record)
#                       try:
#                            with self.path.open("a") as f:
#                                 f.write(json.dumps(record) + "\n")
#                       except Exception as e:
#                            print(f"Mock Ledger write error: {e}")
#                       return record
#                  def verify(self):
#                       print("Mock Ledger verify called.")
#                       prev_hash = "0"*64
#                       for entry in self._entries:
#                           record_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                           expected_hash = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                           if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                               print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                               return False
#                           prev_hash = entry.get("hash", prev_hash)
#                       return True
#                  def read_all(self) -> List[Dict[str, Any]]:
#                       return list(self._entries)
# 
#             self.ledger = MockLedger(ledger_path)
# 
#         self.ledger_path = Path(f"/content/{ledger_path}")
#         print("🔍 Audit Suite initialized.")
# 
#     def _hash_entry(self, entry: Dict[str, Any]) -> str:
#         """Compute SHA-256 hash of a single ledger entry dictionary (excluding 'hash' and 'prev_hash' fields)."""
#         entry_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#         encoded_entry = json.dumps(entry_to_hash, sort_keys=True).encode('utf-8')
#         return hashlib.sha256(encoded_entry).hexdigest()
# 
#     def _build_merkle_tree_layer(self, hashes: List[str]) -> List[str]:
#         """Computes the next layer of the Merkle tree from a list of hashes."""
#         next_layer = []
#         if len(hashes) % 2 != 0:
#             hashes.append(hashes[-1])
#         for i in range(0, len(hashes), 2):
#             combined_hash = hashes[i] + hashes[i+1]
#             next_layer.append(hashlib.sha256(combined_hash.encode('utf-8')).hexdigest())
#         return next_layer
# 
#     def build_merkle_tree(self) -> Tuple[str, List[str], List[List[str]]]:
#         """
#         Builds a Merkle tree from ledger entries.
#         Returns the root hash, the list of leaf hashes, and all layers of the tree.
#         """
#         print("Building Merkle tree from ledger entries...")
#         entries = self.ledger.read_all()
# 
#         if not entries:
#             print("No entries in the ledger to build a Merkle tree.")
#             return "", [], []
# 
#         leaf_hashes = [self._hash_entry(entry) for entry in entries]
# 
#         if not leaf_hashes:
#              print("Failed to generate leaf hashes.")
#              return "", [], []
# 
#         layers = [leaf_hashes]
#         current_layer = leaf_hashes
# 
#         while len(current_layer) > 1:
#             current_layer = self._build_merkle_tree_layer(current_layer)
#             layers.append(current_layer)
# 
#         merkle_root = layers[-1][0] if layers else ""
#         print(f"Merkle tree built. Root hash: {merkle_root}")
#         return merkle_root, leaf_hashes, layers
# 
#     def get_merkle_proof(self, entry_hash: str, leaf_hashes: List[str], layers: List[List[str]]) -> List[Tuple[str, bool]]:
#         """
#         Generates a Merkle proof for a specific ledger entry's hash.
#         The proof is a list of tuples (sibling_hash, is_sibling_right),
#         needed to reconstruct the path from the entry's hash to the root.
#         """
#         try:
#             current_index = leaf_hashes.index(entry_hash)
#         except ValueError:
#             print(f"Error: Entry hash '{entry_hash}' not found in leaf hashes.")
#             return []
# 
#         proof = []
# 
#         for i in range(len(layers) - 1):
#             layer = layers[i]
#             if current_index % 2 == 0:
#                 sibling_index = current_index + 1
#                 is_sibling_right = True
#             else:
#                 sibling_index = current_index - 1
#                 is_sibling_right = False
# 
#             if sibling_index >= len(layer):
#                  if current_index == len(layer) - 1 and len(layer) % 2 != 0:
#                       proof.append((layer[current_index], True))
#                  else:
#                     print(f"Warning: Sibling index {sibling_index} out of bounds for layer {i} with length {len(layer)}. Proof generation failed.")
#                     return [] # Indicate an error
# 
#             else:
#                 proof.append((layer[sibling_index], is_sibling_right))
# 
#             current_index = current_index // 2
# 
#         return proof
# 
#     def verify_merkle_proof(self, entry_hash: str, proof: List[Tuple[str, bool]], root_hash: str) -> bool:
#         """
#         Verifies a Merkle proof for a specific entry's hash against the Merkle root.
#         Proof format: List of tuples (sibling_hash, is_sibling_right).
#         """
#         current_hash = entry_hash
# 
#         for sibling_hash, is_sibling_right in proof:
#             if is_sibling_right:
#                 combined_hash = hashlib.sha256((current_hash + sibling_hash).encode('utf-8')).hexdigest()
#             else:
#                 combined_hash = hashlib.sha256((sibling_hash + current_hash).encode('utf-8')).hexdigest()
#             current_hash = combined_hash
# 
#         return current_hash == root_hash
# 
# 
#     def simulate_zkp_verification(self, contradiction_data: Dict[str, Any], alleged_severity: str) -> bool:
#         """
#         Simulates zero-knowledge proof verification for contradiction correctness
#         without revealing sensitive data.
# 
#         Simulation Logic: Check if the alleged severity is plausible for the
#         given contradiction type. This is a simplified stand-in for a real ZKP
#         where a prover would demonstrate, without revealing details, that the
#         contradiction satisfies certain conditions corresponding to the alleged severity.
#         """
#         print(f"Simulating ZKP verification for alleged severity '{alleged_severity}'...")
# 
#         # Define plausible severity ranges for different contradiction types
#         # This is a simplified mapping for the simulation.
#         plausible_severities: Dict[str, List[str]] = {
#             "textual": ["medium", "high"], # Textual conflicts are often medium or high
#             "numeric": ["low", "medium", "high"], # Numeric deviation can be any severity
#             "semantic": ["low", "medium", "high"], # Semantic similarity can range
#             "policy_violation": ["medium", "high"], # Policy violations are usually not low
#             "system_event": ["low", "info"] # System events are typically low severity or info
#         }
# 
#         contradiction_type = contradiction_data.get("type", "unknown")
# 
#         # In a real ZKP, the verifier wouldn't see contradiction_data.
#         # They would receive a proof that demonstrates (e.g., deviation > threshold for 'high').
#         # Here, we *use* the data internally for the simulation logic to check plausibility.
# 
#         if contradiction_type in plausible_severities:
#             is_plausible = alleged_severity.lower() in plausible_severities[contradiction_type]
#             print(f"   Contradiction Type: '{contradiction_type}'")
#             print(f"   Plausible Severities for this type: {plausible_severities[contradiction_type]}")
#             print(f"   Alleged Severity '{alleged_severity}' is plausible: {is_plausible}")
#             return is_plausible
#         else:
#             print(f"   Unknown contradiction type '{contradiction_type}'. Cannot determine plausibility.")
#             # For unknown types, we might allow any severity or none, depending on policy.
#             # Let's assume any valid severity is accepted for unknown types in this simulation.
#             valid_severities = ["low", "medium", "high", "info", "none"] # Include info/none as possibilities
#             is_valid = alleged_severity.lower() in valid_severities
#             print(f"   Alleged Severity '{alleged_severity}' is a valid severity level: {is_valid}")
#             return is_valid
# 
# 
#     def explore_provenance(self, event_id: str) -> List[Dict[str, Any]]:
#         """
#         Reconstructs the lineage of an event by tracing backward through source links
#         and chronological order from the ledger.
#         """
#         print(f"Exploring provenance for event ID: {event_id}...")
#         lineage = []
#         entries = self.ledger.read_all()
# 
#         if not entries:
#             print("No entries in the ledger to explore provenance.")
#             return lineage
# 
#         # Create a mapping from event ID to the full entry for quick lookup
#         entry_by_id = {entry.get("id"): entry for entry in entries if entry.get("id")}
#         # Create a mapping from hash to the full entry for prev_hash lookup
#         entry_by_hash = {entry.get("hash"): entry for entry in entries if entry.get("hash")}
# 
# 
#         current_id = event_id
#         visited_ids = set()
# 
#         # Start traversal from the input event_id
#         start_entry = entry_by_id.get(event_id)
#         if not start_entry:
#             print(f"Error: Event ID '{event_id}' not found in the ledger.")
#             return []
# 
#         # Traverse backward
#         current_entry = start_entry
#         while current_entry and current_entry.get("id") not in visited_ids:
#              current_entry_id = current_entry.get("id")
#              visited_ids.add(current_entry_id)
#              lineage.append(current_entry)
# 
#              next_id = None
# 
#              # 1. Look for the explicit source ID in the 'data' field
#              source_info = current_entry.get("data", {}).get("source")
#              if isinstance(source_info, dict):
#                   # Check for common source ID keys ('id', 'event_id', 'source_id')
#                   source_id_keys = ['id', 'event_id', 'source_id']
#                   for key in source_id_keys:
#                       potential_source_id = source_info.get(key)
#                       if potential_source_id and potential_source_id != current_entry_id and potential_source_id in entry_by_id:
#                            next_id = potential_source_id
#                            break # Found a valid source ID
# 
#              # 2. If no explicit source ID found, try tracing via prev_hash (chronological order)
#              if next_id is None:
#                   prev_hash = current_entry.get("prev_hash")
#                   if prev_hash and prev_hash != "0" * 64 and prev_hash != "mock": # Exclude root/mock hash
#                        prev_entry = entry_by_hash.get(prev_hash)
#                        if prev_entry:
#                             next_id = prev_entry.get("id")
# 
# 
#              # Move to the next entry or stop if no link found
#              if next_id and next_id in entry_by_id:
#                   current_entry = entry_by_id[next_id]
#              else:
#                   current_entry = None # Stop traversal
# 
# 
#         # Reverse the lineage to show flow from source to event
#         return lineage[::-1] if lineage else []
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#     # Ensure Ledger is available for the demo
#     if 'Ledger' not in globals():
#         print("Ledger class not found. Using a mock setup for demonstration.")
#         class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#                   self._entries = []
#                   if self.path.exists():
#                       try:
#                           with self.path.open("r") as f:
#                               for line in f:
#                                   self._entries.append(json.loads(line))
#                       except Exception as e:
#                           print(f"Error reading existing mock ledger file: {e}")
# 
#              def append(self, record):
#                   record_to_hash = {k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64
#                   record["hash"] = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                   record["id"] = record.get("id", str(uuid.uuid4()))
#                   self._entries.append(record)
#                   try:
#                        with self.path.open("a") as f:
#                             f.write(json.dumps(record) + "\n")
#                   except Exception as e:
#                        print(f"Mock Ledger write error: {e}")
#                   return record
#              def verify(self):
#                   print("Mock Ledger verify called.")
#                   prev_hash = "0"*64
#                   for entry in self._entries:
#                       record_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                       expected_hash = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                       if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                           print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                           return False
#                       prev_hash = entry.get("hash", prev_hash)
#                   return True
#              def read_all(self) -> List[Dict[str, Any]]:
#                   return list(self._entries)
# 
#         import time
#         import uuid
#         import hashlib
#         from pathlib import Path
# 
#     # Clean up ledger file for fresh demo if it exists
#     ledger_path = Path("/content/ledger.jsonl")
#     if ledger_path.exists():
#          try:
#              with open(ledger_path, 'w') as f:
#                   f.write("")
#              print(f"Cleared existing ledger file: {ledger_path}")
#          except Exception as e:
#              print(f"Error clearing ledger file: {e}")
# 
# 
#     audit_suite = AuditSuite()
# 
#     # Demonstrate Provenance Explorer
#     print("\n--- Demonstrating Provenance Explorer ---")
# 
#     # Simulate logging some events to the ledger to create a chain for provenance
#     print("\nLogging sample events to ledger for provenance demo...")
# 
#     # Need a working Ledger instance for this part
#     if 'Ledger' in globals() or isinstance(audit_suite.ledger, MockLedger):
#          demo_ledger = audit_suite.ledger # Use the ledger from the audit suite instance
#          # Clear ledger again for a clean provenance chain demo if needed
#          if isinstance(demo_ledger, MockLedger): # Only clear mock ledger
#               ledger_path = Path("/content/ledger.jsonl")
#               if ledger_path.exists():
#                    try:
#                        with open(ledger_path, 'w') as f:
#                             f.write("")
#                        print(f"Cleared ledger file for provenance demo: {ledger_path}")
#                    except Exception as e:
#                        print(f"Error clearing ledger file: {e}")
#               demo_ledger._entries = [] # Also clear in-memory entries for mock
# 
# 
#          # Log events forming a provenance chain: Source -> Contradiction -> Resolution -> Amendment
#          source_event_id = str(uuid.uuid4())
#          source_event = {"event_type": "SOURCE_DATA", "data": {"id": source_event_id, "payload": {"raw": "Initial data received"}}}
#          logged_source = demo_ledger.append(source_event)
#          print(f"Logged Source Event: {logged_source['id']}")
# 
#          contradiction_data = {
#              "type": "textual",
#              "claim_a": "Data is consistent.",
#              "claim_b": "Data has conflicts.",
#              "source": {"type": "event", "id": logged_source['id']} # Link to source event
#          }
#          contradiction_event = {"event_type": "CONTRADICTION", "data": contradiction_data}
#          logged_contradiction = demo_ledger.append(contradiction_event)
#          print(f"Logged Contradiction Event: {logged_contradiction['id']}")
# 
#          resolution_data = {
#              "type": "manual",
#              "status": "resolved",
#              "details": "Conflicts manually corrected.",
#              "source": {"type": "event", "id": logged_contradiction['id']} # Link to contradiction
#          }
#          resolution_event = {"event_type": "RESOLUTION", "data": resolution_data}
#          logged_resolution = demo_ledger.append(resolution_event)
#          print(f"Logged Resolution Event: {logged_resolution['id']}")
# 
#          amendment_data = {
#              "type": "rule_update",
#              "rule_name": "data_consistency",
#              "details": "Added new validation step.",
#              "source": {"type": "event", "id": logged_resolution['id']} # Link to resolution
#          }
#          amendment_event = {"event_type": "AMENDMENT", "data": amendment_data}
#          logged_amendment = demo_ledger.append(amendment_event)
#          print(f"Logged Amendment Event: {logged_amendment['id']}")
# 
# 
#          # Log some unrelated events to show they are not included in the chain
#          demo_ledger.append({"event_type": "SYSTEM_EVENT", "data": {"message": "Heartbeat"}})
#          demo_ledger.append({"event_type": "POLICY_VIOLATION", "data": {"policy": "Access", "action": "Denied"}})
# 
# 
#          # Now explore provenance starting from the amendment event
#          print(f"\nExploring provenance chain from Amendment Event ({logged_amendment['id']}):")
#          provenance_chain = audit_suite.explore_provenance(logged_amendment['id'])
# 
#          print("\nProvenance Chain (Source -> Contradiction -> Resolution -> Amendment):")
#          if provenance_chain:
#              for entry in provenance_chain:
#                  print(f"- ID: {entry.get('id')}, Type: {entry.get('event_type')}, Timestamp: {entry.get('timestamp')}")
#                  print(f"  Data: {json.dumps(entry.get('data', {}), indent=2)}")
#          else:
#              print("No provenance chain found.")
# 
#          # Explore provenance of an unrelated event
#          unrelated_event = demo_ledger._entries[-1] # Get the last logged event (POLICY_VIOLATION)
#          print(f"\nExploring provenance chain from Unrelated Event ({unrelated_event.get('id')}):")
#          provenance_chain_unrelated = audit_suite.explore_provenance(unrelated_event.get('id'))
#          print("\nProvenance Chain (Unrelated Event):")
#          if provenance_chain_unrelated:
#              for entry in provenance_chain_unrelated:
#                  print(f"- ID: {entry.get('id')}, Type: {entry.get('event_type')}, Timestamp: {entry.get('timestamp')}")
#                  print(f"  Data: {json.dumps(entry.get('data', {}), indent=2)}")
#          else:
#              print("No provenance chain found.")
# 
# 
#     else:
#         print("Skipping Provenance Explorer demo due to missing Ledger/MockLedger.")
# 
# 
#     # Demonstrate Merkle Tree (optional, re-run if ledger was cleared)
#     # If ledger was cleared for provenance, this demo might not show anything unless entries are re-logged.
#     # Skipping full Merkle demo here to keep focus on Provenance.
#     # print("\n--- Demonstrating Merkle Tree (Optional Re-run) ---")
#     # ... Merkle tree demo code ...
# 
# 
#     # Demonstrate ZKP Simulation (optional, already demonstrated above)
#     # print("\n--- Demonstrating ZKP Simulation (Optional Re-run) ---")
#     # ... ZKP demo code ...
# 
# 
#     print("\nAudit Suite provenance explorer demo complete.")

"""## Integrate with world receipt protocol

### Subtask:
Add new API routes (`/verify` and `/proof`) to the `WorldReceiptProtocol` to expose audit functionalities.

**Reasoning**:
The subtask requires modifying the `WorldReceiptProtocol` class to accept an `AuditSuite` instance and adding two new API routes (`/verify` and `/proof`) that utilize this instance. I will modify the `world_receipt_protocol.py` file to include these changes and update the demonstration block to initialize and pass an `AuditSuite`.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile world_receipt_protocol.py
# """
# Tessrax World Receipt Protocol v1.0
# -----------------------------------
# Exposes Tessrax governance data as a simple REST API.
# Allows external agents or dashboards to query live status and recent receipts.
# """
# 
# import json
# from fastapi import FastAPI, HTTPException, Body, Query # Import Body and Query for request data
# from fastapi.responses import JSONResponse
# from pydantic import BaseModel
# from typing import Dict, Any, List, Optional, Tuple
# import uvicorn
# import threading
# import os
# from datetime import datetime # Import datetime to parse timestamps
# import time # Import time for sleep in demo
# 
# # Reuse the active in-memory economy + kernel + dashboard + audit_suite
# # from clarity_fuel_economy import ClarityFuelEconomy
# # from dashboard_adapter import DashboardAdapter
# # from governance_kernel import GovernanceKernelV2 # Assuming V2 is the active kernel
# # from audit_suite import AuditSuite # Import AuditSuite
# 
# class AppendRequest(BaseModel):
#     event_type: str
#     data: Dict[str, Any]
# 
# # Define a Pydantic model for the Merkle proof verification request
# class VerifyProofRequest(BaseModel):
#     entry_hash: str
#     proof: List[Tuple[str, bool]] # Proof format: List of (sibling_hash, is_sibling_right) tuples
#     root_hash: str
# 
# 
# class WorldReceiptProtocol:
#     """Minimal REST interface around Tessrax data models."""
# 
#     def __init__(self, economy: 'ClarityFuelEconomy', dashboard: 'DashboardAdapter', audit_suite: 'AuditSuite', ledger_path: str = "ledger.jsonl"):
#         self.economy = economy
#         self.dashboard = dashboard
#         self.audit_suite = audit_suite # Store the AuditSuite instance
#         self.ledger_path = Path(f"/content/{ledger_path}") # Store as Path object
#         self.app = FastAPI(title="Tessrax World Receipt Protocol", version="1.0")
#         self._mount_routes()
#         print("🌐 World Receipt Protocol initialized.")
# 
#     # --- Internal helpers ---
# 
#     def _load_ledger_entries(self, limit: int = 50) -> List[Dict[str, Any]]:
#         """Read and parse ledger.jsonl, returning recent entries."""
#         entries = []
#         if not self.ledger_path.exists():
#             return []
#         try:
#             with self.ledger_path.open("r") as f:
#                 # Read all lines and get the last 'limit' lines
#                 lines = f.readlines()
#                 for line in lines[-limit:]:
#                     try:
#                         entries.append(json.loads(line))
#                     except json.JSONDecodeError:
#                         # Log error or skip invalid lines
#                         continue
#             return entries
#         except Exception as e:
#             print(f"Error reading ledger file {self.ledger_path}: {e}")
#             return []
# 
# 
#     def _get_entry_by_id(self, entry_id: str) -> Optional[Dict[str, Any]]:
#         """Find a ledger entry by its ID."""
#         # This might be slow for large ledgers, but sufficient for demo
#         entries = self._load_ledger_entries(limit=1000000) # Load potentially many entries to find by ID
#         for entry in entries:
#             if entry.get("id") == entry_id:
#                 return entry
#         return None
# 
# 
#     # --- Route definitions ---
# 
#     def _mount_routes(self):
#         app = self.app
# 
#         @app.get("/status")
#         def get_status():
#             """Return current clarity, entropy, and fuel summaries."""
#             summary = self.dashboard.summarize_metrics()
#             balances = self.economy.get_status()
#             return JSONResponse({"summary": summary, "balances": balances})
# 
#         @app.get("/ledger")
#         def get_ledger(limit: int = 20):
#             """Return the most recent ledger entries."""
#             return JSONResponse({"entries": self._load_ledger_entries(limit)})
# 
#         @app.post("/append")
#         def append_receipt(req: AppendRequest):
#             """
#             Append a new external event to the ledger.
#             This is optional and can be disabled for safety.
#             """
#             record = {
#                 "event_type": req.event_type,
#                 "data": req.data,
#                 "source": "external",
#             }
#             # Use the kernel to evaluate and log, which handles hashing and appending to ledger
#             # Assuming the kernel instance is available via self.economy.kernel or directly
#             if hasattr(self.economy, 'kernel') and self.economy.kernel:
#                  logged_receipt = self.economy.kernel.evaluate(record)
#                  return JSONResponse({"status": "ok", "record": logged_receipt})
#             else:
#                  # Fallback if kernel is not accessible via economy (shouldn't happen with V2 setup)
#                  print("Warning: Kernel not accessible via economy. Logging directly to ledger (basic).")
#                  # This fallback bypasses kernel evaluation logic, only appends to ledger
#                  # Requires direct access to ledger append, which is not standard via ReceiptWriter
#                  # For safety/consistency, raise an error if kernel is not available as expected
#                  raise HTTPException(status_code=500, detail="Internal error: Governance Kernel not initialized or accessible.")
# 
# 
#         @app.post("/verify")
#         def verify_proof(req: VerifyProofRequest):
#             """
#             Verify a Merkle proof for a given ledger entry hash against a root hash.
#             Requires entry_hash, proof (list of tuples), and root_hash.
#             """
#             print(f"Received verification request for entry hash: {req.entry_hash[:6]}...")
#             is_valid = self.audit_suite.verify_merkle_proof(req.entry_hash, req.proof, req.root_hash)
#             print(f"Verification result: {is_valid}")
#             return JSONResponse({"is_valid": is_valid})
# 
# 
#         @app.get("/proof/{entry_id}")
#         def get_merkle_proof(entry_id: str):
#             """
#             Generate a Merkle proof for a specific ledger entry by its ID.
#             Returns the entry's hash, the proof, and the Merkle root of the current ledger state.
#             """
#             print(f"Received proof request for entry ID: {entry_id}")
# 
#             # 1. Get the entry from the ledger
#             entry = self._get_entry_by_id(entry_id)
#             if not entry:
#                 raise HTTPException(status_code=404, detail=f"Entry with ID '{entry_id}' not found.")
# 
#             # 2. Calculate the entry's hash (should match the one in the entry, but recalculate for proof basis)
#             entry_hash = self.audit_suite._hash_entry(entry)
#             # Optional: Verify the stored hash matches the recomputed one
#             if entry.get("hash") and entry.get("hash") != entry_hash:
#                  print(f"Warning: Stored hash for entry {entry_id} ({entry.get('hash')[:6]}...) does not match recomputed hash ({entry_hash[:6]}...). Ledger integrity issue?")
#                  # Decide how to handle this - for proof generation, use the recomputed hash.
# 
# 
#             # 3. Build the Merkle tree for the current state of the ledger
#             # This requires reading all entries again.
#             merkle_root, leaf_hashes, layers = self.audit_suite.build_merkle_tree()
# 
#             if not merkle_root or not leaf_hashes or not layers:
#                  raise HTTPException(status_code=500, detail="Failed to build Merkle tree from ledger.")
# 
#             # 4. Generate the Merkle proof for the entry's hash
#             merkle_proof = self.audit_suite.get_merkle_proof(entry_hash, leaf_hashes, layers)
# 
#             if not merkle_proof and entry_hash != merkle_root:
#                  # Proof generation might fail if entry_hash isn't a leaf hash (shouldn't happen if _hash_entry is correct)
#                  # Or if the entry is the only one (root is the leaf hash, proof is empty).
#                  # Handle the single-entry case: proof is empty, root is the entry hash
#                  if len(leaf_hashes) == 1 and leaf_hashes[0] == entry_hash and merkle_root == entry_hash:
#                      merkle_proof = [] # Proof is empty for the single root entry
#                  else:
#                      print(f"Error generating proof for entry hash {entry_hash[:6]}... Proof is empty but it's not the single root.")
#                      raise HTTPException(status_code=500, detail=f"Failed to generate Merkle proof for entry ID '{entry_id}'.")
# 
# 
#             print(f"Generated proof for entry ID {entry_id}.")
#             return JSONResponse({
#                 "entry_id": entry_id,
#                 "entry_hash": entry_hash,
#                 "merkle_proof": merkle_proof,
#                 "merkle_root": merkle_root
#             })
# 
# 
#     # --- Runner ---
# 
#     def launch(self, port: int = 8080):
#         """Run FastAPI server in a background thread (for Colab)."""
#         # Check if server is already running on this port
#         import socket
#         try:
#             with socket.create_connection(("127.0.0.1", port), timeout=1):
#                 print(f"⚠️ FastAPI server likely already running on port {port}.")
#                 print("Skipping launch.")
#                 return None # Indicate that launch was skipped
#         except (ConnectionRefusedError, socket.timeout):
#             pass # Port is likely free
# 
#         print(f"🚀 Launching Tessrax API on http://127.0.0.1:{port}")
#         # Use a thread to run uvicorn so it doesn't block the notebook
#         thread = threading.Thread(
#             target=lambda: uvicorn.run(self.app, host="0.0.0.0", port=port, log_level="warning"),
#             daemon=True, # Allow the thread to be killed when the main program exits
#         )
#         thread.start()
#         print(f"✅ Tessrax API running in background thread on http://127.0.0.1:{port}")
#         # Return the app instance itself for potential interaction/inspection if needed,
#         # though the primary interaction is via HTTP requests.
#         return self.app
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#     # Ensure necessary classes are available for the demo
#     # Define mocks if not already defined globally
#     if 'GovernanceKernelV2' not in globals():
#         print("GovernanceKernelV2 not found. Using a mock setup for demonstration.")
#         class MockLedger:
#             def __init__(self, path="ledger.jsonl"):
#                  self.path = Path(f"/content/{path}")
#                  self.path.touch(exist_ok=True)
#                  self._entries = []
#                  if self.path.exists():
#                      try:
#                          with self.path.open("r") as f:
#                              for line in f:
#                                  self._entries.append(json.loads(line))
#                      except Exception as e:
#                          print(f"Error reading existing mock ledger file: {e}")
# 
#             def append(self, record):
#                  record_to_hash = {k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}
#                  record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                  record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64
#                  record["hash"] = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                  record["id"] = record.get("id", str(uuid.uuid4()))
#                  self._entries.append(record)
#                  try:
#                       with self.path.open("a") as f:
#                            f.write(json.dumps(record) + "\n")
#                  except Exception as e:
#                       print(f"Mock Ledger write error: {e}")
#                  return record
#             def verify(self):
#                  print("Mock Ledger verify called.")
#                  prev_hash = "0"*64
#                  for entry in self._entries:
#                      record_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                      expected_hash = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                      if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                          print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                          return False
#                      prev_hash = entry.get("hash", prev_hash)
#                  return True
#             def read_all(self) -> List[Dict[str, Any]]:
#                  return list(self._entries)
# 
#         class MockReceiptWriter:
#              def __init__(self, ledger_path="ledger.jsonl"):
#                   self.ledger = MockLedger(ledger_path)
#              def log(self, event_type, data):
#                   return self.ledger.append({"event_type": event_type, "data": data})
#              def verify_ledger(self):
#                   return self.ledger.verify()
# 
#         class GovernanceKernelV2:
#              def __init__(self, ledger_path="ledger.jsonl"):
#                   self.writer = MockReceiptWriter(ledger_path)
#                   self.agent_reputation = defaultdict(lambda: 0.5)
#              def evaluate(self, event):
#                   print(f"Mock GovernanceKernelV2 received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
#                   if "metabolism" in event.get("data", {}).get("type", "") and event.get("agent"):
#                        agent = event["agent"]
#                        clarity_gain = event["data"].get("clarity", 0.0)
#                        entropy_burn = event["data"].get("entropy", 0.0)
#                        change = (clarity_gain * 0.1) - (entropy_burn * 0.05)
#                        print(f"Mock Reputation update for {agent} simulated.")
#                   # Return a mock receipt with an ID
#                   return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
#              def get_reputation(self, agent):
#                   return self.agent_reputation.get(agent, 0.5)
# 
#         class ClarityFuelEconomy: # Mock Economy to pass to WRP
#              def __init__(self, ledger_path="ledger.jsonl"):
#                   # Need a mock kernel for the WRP to access
#                   self.kernel = GovernanceKernelV2(ledger_path)
#                   self.balances = defaultdict(float) # Mock balances
#                   print("Mock Clarity Fuel Economy initialized.")
#              def get_status(self):
#                   return dict(self.balances)
# 
#         class DashboardAdapter: # Mock Dashboard to pass to WRP
#              def __init__(self, economy, ledger_path="ledger.jsonl"):
#                   print("Mock Dashboard Adapter initialized.")
#              def summarize_metrics(self):
#                   return {"avg_entropy": 0.5, "avg_clarity": 0.5, "total_fuel": 100.0} # Mock summary
#              def plot_balances(self): pass # Mock plot
#              def plot_entropy_clarity(self): pass # Mock plot
#              def export_snapshot(self, filename): print(f"Mock snapshot exported to {filename}") # Mock export
# 
#         class AuditSuite: # Mock AuditSuite to pass to WRP
#              def __init__(self, ledger_path="ledger.jsonl"):
#                  # Need a mock ledger for the audit suite
#                  self.ledger = MockLedger(ledger_path)
#                  self.ledger_path = Path(f"/content/{ledger_path}")
#                  print("Mock Audit Suite initialized.")
# 
#              def _hash_entry(self, entry: Dict[str, Any]) -> str:
#                  # Use hashlib from outer scope
#                  entry_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                  encoded_entry = json.dumps(entry_to_hash, sort_keys=True).encode('utf-8')
#                  return hashlib.sha256(encoded_entry).hexdigest()
# 
#              def _build_merkle_tree_layer(self, hashes: List[str]) -> List[str]:
#                   # Use hashlib from outer scope
#                   next_layer = []
#                   if len(hashes) % 2 != 0:
#                       hashes.append(hashes[-1])
#                   for i in range(0, len(hashes), 2):
#                       combined_hash = hashes[i] + hashes[i+1]
#                       next_layer.append(hashlib.sha256(combined_hash.encode('utf-8')).hexdigest())
#                   return next_layer
# 
#              def build_merkle_tree(self) -> Tuple[str, List[str], List[List[str]]]:
#                  print("Mock Audit Suite: Building Merkle tree.")
#                  entries = self.ledger.read_all()
#                  if not entries: return "", [], []
#                  leaf_hashes = [self._hash_entry(entry) for entry in entries]
#                  layers = [leaf_hashes]
#                  current_layer = leaf_hashes
#                  while len(current_layer) > 1:
#                      current_layer = self._build_merkle_tree_layer(current_layer)
#                      layers.append(current_layer)
#                  merkle_root = layers[-1][0] if layers else ""
#                  return merkle_root, leaf_hashes, layers
# 
#              def get_merkle_proof(self, entry_hash: str, leaf_hashes: List[str], layers: List[List[str]]) -> List[Tuple[str, bool]]:
#                  print(f"Mock Audit Suite: Generating Merkle proof for {entry_hash[:6]}...")
#                  # Simulate proof generation - needs actual logic or mock data
#                  # For demonstration, let's use the real logic if possible, otherwise return a simple mock
#                  try:
#                       # Use the real logic if available
#                       current_index = leaf_hashes.index(entry_hash)
#                       proof = []
#                       for i in range(len(layers) - 1):
#                           layer = layers[i]
#                           if current_index % 2 == 0:
#                               sibling_index = current_index + 1
#                               is_sibling_right = True
#                           else:
#                               sibling_index = current_index - 1
#                               is_sibling_right = False
#                           # Handle odd layer length duplication
#                           if sibling_index >= len(layer):
#                               if current_index == len(layer) - 1 and len(layer) % 2 != 0:
#                                   proof.append((layer[current_index], True))
#                               else:
#                                   print("Mock Proof Gen Warning: Sibling index out of bounds.")
#                                   return []
#                           else:
#                               proof.append((layer[sibling_index], is_sibling_right))
#                           current_index = current_index // 2
#                       return proof
#                  except ValueError:
#                       print("Mock Proof Gen Error: Entry hash not found.")
#                       return []
#                  except Exception as e:
#                       print(f"Mock Proof Gen Error: {e}")
#                       return []
# 
# 
#              def verify_merkle_proof(self, entry_hash: str, proof: List[Tuple[str, bool]], root_hash: str) -> bool:
#                   print(f"Mock Audit Suite: Verifying Merkle proof for {entry_hash[:6]}... against root {root_hash[:6]}...")
#                   # Simulate verification - needs actual logic or mock data
#                   # For demonstration, use the real logic if possible
#                   try:
#                        current_hash = entry_hash
#                        for sibling_hash, is_sibling_right in proof:
#                            if is_sibling_right:
#                                combined_hash = hashlib.sha256((current_hash + sibling_hash).encode('utf-8')).hexdigest()
#                            else:
#                                combined_hash = hashlib.sha256((sibling_hash + current_hash).encode('utf-8')).hexdigest()
#                            current_hash = combined_hash
#                        return current_hash == root_hash
#                   except Exception as e:
#                        print(f"Mock Verify Proof Error: {e}")
#                        return False
# 
# 
#              def simulate_zkp_verification(self, contradiction_data: Dict[str, Any], alleged_severity: str) -> bool:
#                  print("Mock Audit Suite: Simulating ZKP verification.")
#                  valid_severities = ["low", "medium", "high", "info", "none"]
#                  return alleged_severity.lower() in valid_severities
# 
#              def explore_provenance(self, event_id: str) -> List[Dict[str, Any]]:
#                  print(f"Mock Audit Suite: Exploring provenance for {event_id}.")
#                  # Return a mock chain
#                  return [{"id": "mock_source", "event_type": "MOCK_SOURCE"}, {"id": event_id, "event_type": "MOCK_TARGET"}]
# 
#         # Import necessary modules for mocks
#         import time
#         import uuid
#         import hashlib
#         from pathlib import Path
#         from collections import defaultdict
# 
# 
#     # --- Demonstration Setup ---
# 
#     # Clean up ledger file for a fresh demo run if it exists
#     ledger_path = Path("/content/ledger.jsonl")
#     if ledger_path.exists():
#          try:
#              # Clear the ledger file for a clean API demo
#              with open(ledger_path, 'w') as f:
#                   f.write("")
#              print(f"Cleared existing ledger file: {ledger_path}")
#          except Exception as e:
#              print(f"Error clearing ledger file: {e}")
# 
# 
#     # Initialize core components
#     # Use real classes if available, otherwise mocks defined above
#     if 'Ledger' in globals():
#          ledger = Ledger()
#     else:
#          ledger = MockLedger() # Use mock if real not available
# 
#     if 'ReceiptWriter' in globals():
#          writer = ReceiptWriter()
#     else:
#          class MockReceiptWriter: # Define if not global
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    # Use the ledger instance created above
#                    self.ledger = ledger # Assuming ledger is in scope
#               def log(self, event_type, data):
#                    # Mock log by appending to the shared ledger instance
#                    return self.ledger.append({"event_type": event_type, "data": data})
#               def verify_ledger(self):
#                    return self.ledger.verify()
#          writer = MockReceiptWriter()
# 
# 
#     if 'GovernanceKernelV2' in globals():
#          kernel = GovernanceKernelV2()
#     elif 'GovernanceKernel' in globals():
#          kernel = GovernanceKernel() # Fallback to base kernel
#     else:
#          class MockGovernanceKernelV2: # Mock Kernel
#               def __init__(self, ledger_path="ledger.jsonl"):
#                    # Use the writer instance created above
#                    self.writer = writer # Assuming writer is in scope
#                    self.agent_reputation = defaultdict(lambda: 0.5)
#               def evaluate(self, event):
#                    print(f"Mock Kernel evaluate received: {event.get('event_type')}")
#                    # Simulate logging via the writer
#                    return self.writer.log(event.get('event_type', 'UNKNOWN').upper(), event.get('data', {}))
#               def verify_ledger(self):
#                    return self.writer.verify_ledger()
#               def get_reputation(self, agent):
#                   return self.agent_reputation.get(agent, 0.5)
#          kernel = MockGovernanceKernelV2()
# 
# 
#     if 'ClarityFuelEconomy' in globals():
#          economy = ClarityFuelEconomy()
#          # Manually set the kernel if the economy instance didn't pick it up
#          if not hasattr(economy, 'kernel') or economy.kernel is None:
#               economy.kernel = kernel
#          # Seed some balances for the economy demo
#          economy.balances["AgentA"] = 100.0
#          economy.balances["AgentB"] = 50.0
#          economy.balances["AgentC"] = 200.0
#     else:
#          # Use Mock Economy, ensuring it has the kernel
#          economy = MockClarityFuelEconomy()
#          economy.kernel = kernel # Ensure mock economy has the kernel
# 
#     if 'DashboardAdapter' in globals():
#          dashboard = DashboardAdapter(economy)
#     else:
#          dashboard = MockDashboardAdapter(economy) # Use mock dashboard
# 
#     if 'AuditSuite' in globals():
#          audit_suite = AuditSuite()
#          # Manually set the ledger if the audit suite instance didn't pick it up
#          if not hasattr(audit_suite, 'ledger') or audit_suite.ledger is None:
#               audit_suite.ledger = ledger
#      else:
#          # Use Mock AuditSuite, ensuring it has the ledger
#          audit_suite = MockAuditSuite()
#          audit_suite.ledger = ledger # Ensure mock audit suite has the ledger
# 
# 
#     # Log a few entries *before* initializing the API to have data for proofs
#     print("\nLogging initial entries for API demo...")
#     initial_entries = [
#         {"event_type": "SETUP", "data": {"step": 1, "description": "System initialized"}},
#         {"event_type": "CONFIG", "data": {"param": "threshold", "value": 0.5}},
#         {"event_type": "DATA_INGESTION", "data": {"source": "feed_a", "count": 100}},
#         {"event_type": "ANALYTIC_RUN", "data": {"engine": "contradiction", "result": "started"}},
#     ]
#     logged_initial_entries = []
#     for entry_data in initial_entries:
#          # Use the kernel to log, as this is how entries are added in the full system
#          # This ensures they get proper IDs, timestamps, and hashes via the writer/ledger
#          logged_initial_entries.append(kernel.evaluate(entry_data))
# 
#     print(f"Logged {len(logged_initial_entries)} initial entries.")
#     # Keep track of one entry ID for proof generation demo
#     if logged_initial_entries:
#          entry_id_for_proof_demo = logged_initial_entries[1]['id'] # Use the second entry's ID
#          print(f"Will use entry ID {entry_id_for_proof_demo} for proof demo.")
#     else:
#          entry_id_for_proof_demo = None
#          print("No initial entries logged, cannot demonstrate proof generation.")
# 
# 
#     # Initialize the World Receipt Protocol API
#     # Check if API is already initialized before launching
#     if 'wrp' not in globals() or not isinstance(globals()['wrp'], WorldReceiptProtocol):
#          print("\nInitializing World Receipt Protocol...")
#          wrp = WorldReceiptProtocol(economy, dashboard, audit_suite) # Pass the audit_suite instance
#          # Store the instance globally so it can be reused if the cell is run again
#          globals()['wrp'] = wrp
#          # Launch the API server in a background thread
#          launched_app = wrp.launch(port=8080)
#          if launched_app:
#               print("API is running. Use the endpoints below to interact.")
#          else:
#               print("API launch skipped (likely already running).")
# 
#     else:
#          # If runtime (and thus API) is already running, just use the existing instance
#          print("\nUsing existing World Receipt Protocol instance.")
#          wrp = globals()['wrp']
# 
# 
#     print("\n--- API Endpoints for Audit Testing ---")
#     print(f"API Base URL: http://127.0.0.1:8080")
#     print("\nGET /status")
#     print("  Description: Returns current clarity, entropy, and fuel summaries.")
#     print("  Example: curl http://127.0.0.1:8080/status")
# 
#     print("\nGET /ledger?limit=<int>")
#     print("  Description: Returns the most recent ledger entries.")
#     print("  Example: curl http://127.0.0.1:8080/ledger?limit=5")
# 
#     print("\nPOST /append")
#     print("  Description: Appends a new external event to the ledger.")
#     print("  Method: POST")
#     print("  Body: JSON object with fields 'event_type' (string) and 'data' (object).")
#     print("  Example: curl -X POST -H 'Content-Type: application/json' -d '{\"event_type\": \"EXTERNAL_EVENT\", \"data\": {\"source_system\": \"CRM\", \"record_id\": 123}}' http://127.0.0.1:8080/append")
# 
#     print("\nGET /proof/{entry_id}")
#     print("  Description: Generates a Merkle proof for a specific ledger entry.")
#     print("  Path Parameter: entry_id (string) - The ID of the ledger entry.")
#     if entry_id_for_proof_demo:
#          print(f"  Example: curl http://127.0.0.1:8080/proof/{entry_id_for_proof_demo}")
#     else:
#          print("  (Cannot provide example without a logged entry ID)")
# 
# 
#     print("\nPOST /verify")
#     print("  Description: Verifies a Merkle proof for a ledger entry.")
#     print("  Method: POST")
#     print("  Body: JSON object with fields 'entry_hash' (string), 'proof' (list of [string, boolean] tuples), and 'root_hash' (string).")
#     print("  Example: (Requires obtaining proof and root hash from /proof endpoint first)")
#     print("    1. Get proof: curl http://127.0.0.1:8080/proof/{entry_id} > proof.json")
#     print("    2. Extract entry_hash, merkle_proof, merkle_root from proof.json")
#     print("    3. Verify: curl -X POST -H 'Content-Type: application/json' -d '{\"entry_hash\": \"...\", \"proof\": [...], \"root_hash\": \"...\"}' http://127.0.0.1:8080/verify")
# 
# 
#     # Keep the notebook cell alive while the FastAPI server runs in the background thread
#     # You can interact with the API using curl or a browser from outside the cell.
#     # To stop the server, interrupt the execution of this cell (e.g., Stop button in Colab).
#     print("\nAPI is running in the background. Interrupt this cell to stop.")
#     try:
#         # Keep the main thread alive indefinitely (or until interrupted)
#         # In a real application, you might use a signal handler or other mechanism
#         while True:
#             time.sleep(60) # Sleep for a minute, check if interrupted
#     except KeyboardInterrupt:
#         print("\nKeyboardInterrupt detected. Stopping cell execution.")
#         # The daemon thread running uvicorn should exit when the main program exits.
#         # No explicit thread cleanup needed here for daemon threads.
#

"""## Provide cli commands

### Subtask:
Create minimal CLI commands to interact with the new audit API routes for testing.

**Reasoning**:
The subtask is to create minimal CLI commands to interact with the new audit API routes. I will write a Python script that uses `requests` and `argparse` to make this happen.
"""

# audit_cli.py
"""
Minimal CLI tool to interact with Tessrax Audit API routes.
"""

import requests
import json
import argparse
import sys

def get_proof(api_url: str, entry_id: str):
    """Calls the /proof/{entry_id} endpoint and prints the result."""
    url = f"{api_url}/proof/{entry_id}"
    print(f"Fetching proof for entry ID: {entry_id} from {url}")
    try:
        response = requests.get(url)
        response.raise_for_status() # Raise an exception for bad status codes
        proof_data = response.json()
        print("\n--- Merkle Proof ---")
        print(json.dumps(proof_data, indent=2))
        return proof_data # Return data for potential verification
    except requests.exceptions.RequestException as e:
        print(f"Error fetching proof: {e}", file=sys.stderr)
        return None

def verify_proof(api_url: str, entry_hash: str, proof: list, root_hash: str):
    """Calls the /verify endpoint to verify a Merkle proof."""
    url = f"{api_url}/verify"
    print(f"Sending verification request to {url}")
    verify_payload = {
        "entry_hash": entry_hash,
        "proof": proof,
        "root_hash": root_hash
    }
    try:
        response = requests.post(url, json=verify_payload)
        response.raise_for_status() # Raise an exception for bad status codes
        verification_result = response.json()
        print("\n--- Verification Result ---")
        print(json.dumps(verification_result, indent=2))
        return verification_result.get("is_valid", False)
    except requests.exceptions.RequestException as e:
        print(f"Error sending verification request: {e}", file=sys.stderr)
        return False

def main():
    parser = argparse.ArgumentParser(description="CLI tool for Tessrax Audit API.")
    parser.add_argument("--api-url", default="http://127.0.0.1:8080", help="Base URL of the Tessrax API.")

    subparsers = parser.add_subparsers(dest="command", help="Available commands")

    # Subparser for the 'proof' command
    proof_parser = subparsers.add_parser("proof", help="Get Merkle proof for an entry ID.")
    proof_parser.add_argument("entry_id", help="The ID of the ledger entry to get the proof for.")

    # Subparser for the 'verify' command
    verify_parser = subparsers.add_parser("verify", help="Verify a Merkle proof.")
    verify_parser.add_argument("--hash", required=True, help="The hash of the entry to verify.")
    verify_parser.add_argument("--proof", required=True, help="The Merkle proof (JSON string or file path).")
    verify_parser.add_argument("--root", required=True, help="The Merkle root hash.")

    args = parser.parse_args()

    if args.command == "proof":
        proof_data = get_proof(args.api_url, args.entry_id)
        # Optionally save the proof data to a file
        if proof_data:
            proof_filename = f"proof_{args.entry_id}.json"
            with open(proof_filename, "w") as f:
                json.dump(proof_data, f, indent=2)
            print(f"\nProof data saved to {proof_filename}")

    elif args.command == "verify":
        # Load proof from string or file
        try:
            if args.proof.endswith(".json"):
                with open(args.proof, "r") as f:
                    proof_payload = json.load(f)
                    proof = proof_payload.get("merkle_proof") # Assume proof is in 'merkle_proof' field if loaded from file
                    if proof is None:
                         raise ValueError("Could not find 'merkle_proof' in the provided JSON file.")
            else:
                # Attempt to parse as a JSON string
                proof = json.loads(args.proof)
        except Exception as e:
            print(f"Error loading proof: {e}", file=sys.stderr)
            sys.exit(1)

        # Ensure proof is in the correct format (list of [string, boolean] tuples)
        # The API expects List[Tuple[str, bool]]. JSON parses lists as lists.
        # We need to ensure the inner elements are string and bool.
        if not isinstance(proof, list) or any(not isinstance(item, list) or len(item) != 2 or not isinstance(item[0], str) or not isinstance(item[1], bool) for item in proof):
             print("Error: Proof must be a list of [string, boolean] tuples.", file=sys.stderr)
             sys.exit(1)


        verify_proof(args.api_url, args.hash, proof, args.root)

    else:
        parser.print_help()

if __name__ == "__main__":
    # Add example usage instructions here
    print("--- Tessrax Audit CLI Usage ---")
    print("To get a proof for a ledger entry:")
    print(f"  python {sys.argv[0]} proof <entry_id>")
    print("  Example: python audit_cli.py proof d1c1a2b3-e4f5-6789-a012-34567890abcd")
    print("\nTo verify a Merkle proof:")
    print(f"  python {sys.argv[0]} verify --hash <entry_hash> --proof <proof_json_string_or_file> --root <merkle_root_hash>")
    print("  Example (using JSON string): python audit_cli.py verify --hash \"...\" --proof \"[['hash1', true], ['hash2', false]]\" --root \"...\"")
    print("  Example (using JSON file): python audit_cli.py verify --hash \"...\" --proof proof_d1c1a2b3.json --root \"...\"")
    print("\n-------------------------------\n")

    # Check if running from command line and execute main
    if len(sys.argv) > 1: # Only run main if arguments are provided (i.e., not just importing)
        main()
    else:
         # If no arguments, just print usage and exit
         sys.exit(0)

"""## Demonstrate functionality

### Subtask:
Add example code within `audit_suite.py` to demonstrate Merkle tree validation, ZKP simulation, and provenance exploration.

**Reasoning**:
The subtask is to add example code within `audit_suite.py` to demonstrate Merkle tree validation, ZKP simulation, and provenance exploration. The previous steps have already implemented the core logic for these features. I will now update the `if __name__ == "__main__":` block in `audit_suite.py` to include a comprehensive demonstration of all three functionalities as requested.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile audit_suite.py
# """
# Tessrax Audit Suite v1.0
# ------------------------
# Provides tools for verifying ledger integrity, generating proofs,
# simulating zero-knowledge proofs, and exploring event provenance.
# """
# 
# import json
# import hashlib
# from typing import Dict, Any, List, Tuple
# from pathlib import Path
# from collections import defaultdict
# import time
# import uuid
# 
# # The Ledger class should be available in the environment
# # from ledger import Ledger
# 
# class AuditSuite:
#     """
#     Provides auditing tools for the Tessrax Ledger.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Use the Ledger class already defined in the environment
#         if 'Ledger' in globals():
#             self.ledger = Ledger(ledger_path)
#         else:
#             # Mock Ledger if not found (for isolated testing)
#             print("Warning: Ledger class not found. Using a mock Ledger.")
#             class MockLedger:
#                  def __init__(self, path="ledger.jsonl"):
#                       self.path = Path(f"/content/{path}")
#                       self.path.touch(exist_ok=True)
#                       self._entries = []
#                       if self.path.exists():
#                           try:
#                               with self.path.open("r") as f:
#                                   for line in f:
#                                       self._entries.append(json.loads(line))
#                           except Exception as e:
#                               print(f"Error reading existing mock ledger file: {e}")
# 
#                  def append(self, record):
#                       record_to_hash = {k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}
#                       record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                       record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64
#                       record["hash"] = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                       record["id"] = record.get("id", str(uuid.uuid4()))
#                       self._entries.append(record)
#                       try:
#                            with self.path.open("a") as f:
#                                 f.write(json.dumps(record) + "\n")
#                       except Exception as e:
#                            print(f"Mock Ledger write error: {e}")
#                       return record
#                  def verify(self):
#                       print("Mock Ledger verify called.")
#                       prev_hash = "0"*64
#                       for entry in self._entries:
#                           record_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                           expected_hash = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                           if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                               print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                               return False
#                           prev_hash = entry.get("hash", prev_hash)
#                       return True
#                  def read_all(self) -> List[Dict[str, Any]]:
#                       return list(self._entries)
# 
#             self.ledger = MockLedger(ledger_path)
# 
#         self.ledger_path = Path(f"/content/{ledger_path}")
#         print("🔍 Audit Suite initialized.")
# 
#     def _hash_entry(self, entry: Dict[str, Any]) -> str:
#         """Compute SHA-256 hash of a single ledger entry dictionary (excluding 'hash' and 'prev_hash' fields)."""
#         entry_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#         encoded_entry = json.dumps(entry_to_hash, sort_keys=True).encode('utf-8')
#         return hashlib.sha256(encoded_entry).hexdigest()
# 
#     def _build_merkle_tree_layer(self, hashes: List[str]) -> List[str]:
#         """Computes the next layer of the Merkle tree from a list of hashes."""
#         next_layer = []
#         if len(hashes) % 2 != 0:
#             hashes.append(hashes[-1])
#         for i in range(0, len(hashes), 2):
#             combined_hash = hashes[i] + hashes[i+1]
#             next_layer.append(hashlib.sha256(combined_hash.encode('utf-8')).hexdigest())
#         return next_layer
# 
#     def build_merkle_tree(self) -> Tuple[str, List[str], List[List[str]]]:
#         """
#         Builds a Merkle tree from ledger entries.
#         Returns the root hash, the list of leaf hashes, and all layers of the tree.
#         """
#         print("Building Merkle tree from ledger entries...")
#         entries = self.ledger.read_all()
# 
#         if not entries:
#             print("No entries in the ledger to build a Merkle tree.")
#             return "", [], []
# 
#         leaf_hashes = [self._hash_entry(entry) for entry in entries]
# 
#         if not leaf_hashes:
#              print("Failed to generate leaf hashes.")
#              return "", [], []
# 
#         layers = [leaf_hashes]
#         current_layer = leaf_hashes
# 
#         while len(current_layer) > 1:
#             current_layer = self._build_merkle_tree_layer(current_layer)
#             layers.append(current_layer)
# 
#         merkle_root = layers[-1][0] if layers else ""
#         print(f"Merkle tree built. Root hash: {merkle_root}")
#         return merkle_root, leaf_hashes, layers
# 
#     def get_merkle_proof(self, entry_hash: str, leaf_hashes: List[str], layers: List[List[str]]) -> List[Tuple[str, bool]]:
#         """
#         Generates a Merkle proof for a specific ledger entry's hash.
#         The proof is a list of tuples (sibling_hash, is_sibling_right),
#         needed to reconstruct the path from the entry's hash to the root.
#         """
#         try:
#             current_index = leaf_hashes.index(entry_hash)
#         except ValueError:
#             print(f"Error: Entry hash '{entry_hash}' not found in leaf hashes.")
#             return []
# 
#         proof = []
# 
#         for i in range(len(layers) - 1):
#             layer = layers[i]
#             if current_index % 2 == 0:
#                 sibling_index = current_index + 1
#                 is_sibling_right = True
#             else:
#                 sibling_index = current_index - 1
#                 is_sibling_right = False
# 
#             if sibling_index >= len(layer):
#                  if current_index == len(layer) - 1 and len(layer) % 2 != 0:
#                       proof.append((layer[current_index], True))
#                  else:
#                     print(f"Warning: Sibling index {sibling_index} out of bounds for layer {i} with length {len(layer)}. Proof generation failed.")
#                     return [] # Indicate an error
# 
#             else:
#                 proof.append((layer[sibling_index], is_sibling_right))
# 
#             current_index = current_index // 2
# 
#         return proof
# 
#     def verify_merkle_proof(self, entry_hash: str, proof: List[Tuple[str, bool]], root_hash: str) -> bool:
#         """
#         Verifies a Merkle proof for a specific entry's hash against the Merkle root.
#         Proof format: List of tuples (sibling_hash, is_sibling_right).
#         """
#         current_hash = entry_hash
# 
#         for sibling_hash, is_sibling_right in proof:
#             if is_sibling_right:
#                 combined_hash = hashlib.sha256((current_hash + sibling_hash).encode('utf-8')).hexdigest()
#             else:
#                 combined_hash = hashlib.sha256((sibling_hash + current_hash).encode('utf-8')).hexdigest()
#             current_hash = combined_hash
# 
#         return current_hash == root_hash
# 
# 
#     def simulate_zkp_verification(self, contradiction_data: Dict[str, Any], alleged_severity: str) -> bool:
#         """
#         Simulates zero-knowledge proof verification for contradiction correctness
#         without revealing sensitive data.
# 
#         Simulation Logic: Check if the alleged severity is plausible for the
#         given contradiction type. This is a simplified stand-in for a real ZKP
#         where a prover would demonstrate, without revealing details, that the
#         contradiction satisfies certain conditions corresponding to the alleged severity.
#         """
#         print(f"Simulating ZKP verification for alleged severity '{alleged_severity}'...")
# 
#         # Define plausible severity ranges for different contradiction types
#         # This is a simplified mapping for the simulation.
#         plausible_severities: Dict[str, List[str]] = {
#             "textual": ["medium", "high"], # Textual conflicts are often medium or high
#             "numeric": ["low", "medium", "high"], # Numeric deviation can be any severity
#             "semantic": ["low", "medium", "high"], # Semantic similarity can range
#             "policy_violation": ["medium", "high"], # Policy violations are usually not low
#             "system_event": ["low", "info"] # System events are typically low severity or info
#         }
# 
#         contradiction_type = contradiction_data.get("type", "unknown")
# 
#         # In a real ZKP, the verifier wouldn't see contradiction_data.
#         # They would receive a proof that demonstrates (e.g., deviation > threshold for 'high').
#         # Here, we *use* the data internally for the simulation logic to check plausibility.
# 
#         if contradiction_type in plausible_severities:
#             is_plausible = alleged_severity.lower() in plausible_severities[contradiction_type]
#             print(f"   Contradiction Type: '{contradiction_type}'")
#             print(f"   Plausible Severities for this type: {plausible_severities[contradiction_type]}")
#             print(f"   Alleged Severity '{alleged_severity}' is plausible: {is_plausible}")
#             return is_plausible
#         else:
#             print(f"   Unknown contradiction type '{contradiction_type}'. Cannot determine plausibility.")
#             # For unknown types, we might allow any severity or none, depending on policy.
#             # Let's assume any valid severity is accepted for unknown types in this simulation.
#             valid_severities = ["low", "medium", "high", "info", "none"] # Include info/none as possibilities
#             is_valid = alleged_severity.lower() in valid_severities
#             print(f"   Alleged Severity '{alleged_severity}' is a valid severity level: {is_valid}")
#             return is_valid
# 
# 
#     def explore_provenance(self, event_id: str) -> List[Dict[str, Any]]:
#         """
#         Reconstructs the lineage of an event by tracing backward through source links
#         and chronological order from the ledger.
#         """
#         print(f"Exploring provenance for event ID: {event_id}...")
#         lineage = []
#         entries = self.ledger.read_all()
# 
#         if not entries:
#             print("No entries in the ledger to explore provenance.")
#             return lineage
# 
#         # Create a mapping from event ID to the full entry for quick lookup
#         entry_by_id = {entry.get("id"): entry for entry in entries if entry.get("id")}
#         # Create a mapping from hash to the full entry for prev_hash lookup
#         entry_by_hash = {entry.get("hash"): entry for entry in entries if entry.get("hash")}
# 
# 
#         current_id = event_id
#         visited_ids = set()
# 
#         # Start traversal from the input event_id
#         start_entry = entry_by_id.get(event_id)
#         if not start_entry:
#             print(f"Error: Event ID '{event_id}' not found in the ledger.")
#             return []
# 
#         # Traverse backward
#         current_entry = start_entry
#         while current_entry and current_entry.get("id") not in visited_ids:
#              current_entry_id = current_entry.get("id")
#              visited_ids.add(current_entry_id)
#              lineage.append(current_entry)
# 
#              next_id = None
# 
#              # 1. Look for the explicit source ID in the 'data' field
#              source_info = current_entry.get("data", {}).get("source")
#              if isinstance(source_info, dict):
#                   # Check for common source ID keys ('id', 'event_id', 'source_id')
#                   source_id_keys = ['id', 'event_id', 'source_id']
#                   for key in source_id_keys:
#                       potential_source_id = source_info.get(key)
#                       if potential_source_id and potential_source_id != current_entry_id and potential_source_id in entry_by_id:
#                            next_id = potential_source_id
#                            break # Found a valid source ID
# 
#              # 2. If no explicit source ID found, try tracing via prev_hash (chronological order)
#              if next_id is None:
#                   prev_hash = current_entry.get("prev_hash")
#                   if prev_hash and prev_hash != "0" * 64 and prev_hash != "mock": # Exclude root/mock hash
#                        prev_entry = entry_by_hash.get(prev_hash)
#                        if prev_entry:
#                             next_id = prev_entry.get("id")
# 
# 
#              # Move to the next entry or stop if no link found
#              if next_id and next_id in entry_by_id:
#                   current_entry = entry_by_id[next_id]
#              else:
#                   current_entry = None # Stop traversal
# 
# 
#         # Reverse the lineage to show flow from source to event
#         return lineage[::-1] if lineage else []
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#     # Ensure Ledger is available for the demo
#     if 'Ledger' not in globals():
#         print("Ledger class not found. Using a mock setup for demonstration.")
#         class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#                   self._entries = []
#                   if self.path.exists():
#                       try:
#                           with self.path.open("r") as f:
#                               for line in f:
#                                   self._entries.append(json.loads(line))
#                       except Exception as e:
#                           print(f"Error reading existing mock ledger file: {e}")
# 
#              def append(self, record):
#                   record_to_hash = {k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64
#                   record["hash"] = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                   record["id"] = record.get("id", str(uuid.uuid4()))
#                   self._entries.append(record)
#                   try:
#                        with self.path.open("a") as f:
#                             f.write(json.dumps(record) + "\n")
#                   except Exception as e:
#                        print(f"Mock Ledger write error: {e}")
#                   return record
#              def verify(self):
#                   print("Mock Ledger verify called.")
#                   prev_hash = "0"*64
#                   for entry in self._entries:
#                       record_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                       expected_hash = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                       if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                           print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                           return False
#                       prev_hash = entry.get("hash", prev_hash)
#                   return True
#              def read_all(self) -> List[Dict[str, Any]]:
#                   return list(self._entries)
# 
#         import time
#         import uuid
#         import hashlib
#         from pathlib import Path
# 
#     # Clean up ledger file for fresh demo if it exists
#     ledger_path = Path("/content/ledger.jsonl")
#     if ledger_path.exists():
#          try:
#              with open(ledger_path, 'w') as f:
#                   f.write("")
#              print(f"Cleared existing ledger file: {ledger_path}")
#          except Exception as e:
#              print(f"Error clearing ledger file: {e}")
# 
# 
#     audit_suite = AuditSuite()
# 
#     # --- Demonstrating Merkle Tree Validation ---
#     print("\n--- Demonstrating Merkle Tree Validation ---")
# 
#     # Simulate logging some entries to the ledger for Merkle tree demo
#     print("\nLogging sample entries to ledger for Merkle tree demo...")
#     entries_for_merkle = [
#         {"event_type": "DATA_POINT", "data": {"value": 100}},
#         {"event_type": "DATA_POINT", "data": {"value": 200}},
#         {"event_type": "ALERT", "data": {"message": "Threshold breached"}},
#         {"event_type": "DATA_POINT", "data": {"value": 150}},
#         {"event_type": "STATUS_UPDATE", "data": {"status": "OK"}},
#     ]
# 
#     logged_entries = []
#     for entry in entries_for_merkle:
#          # Use the ledger instance from the audit suite
#          logged_entries.append(audit_suite.ledger.append(entry))
# 
#     # Build the Merkle tree
#     merkle_root, leaf_hashes, layers = audit_suite.build_merkle_tree()
# 
#     if merkle_root:
#         print(f"\nBuilt Merkle Tree:")
#         print(f"  Leaf Hashes ({len(leaf_hashes)}): {[h[:6] + '...' for h in leaf_hashes]}")
#         print(f"  Root Hash: {merkle_root}")
#         # print("  Layers:", layers) # Uncomment to see all layers
# 
#         # Get a proof for a specific entry (e.g., the third entry - index 2)
#         entry_to_prove = logged_entries[2]
#         entry_hash_to_prove = audit_suite._hash_entry(entry_to_prove) # Recompute hash to be sure
#         print(f"\nGetting Merkle proof for entry ID {entry_to_prove['id']} (Hash: {entry_hash_to_prove[:6]}...)")
# 
#         merkle_proof = audit_suite.get_merkle_proof(entry_hash_to_prove, leaf_hashes, layers)
# 
#         print(f"Generated Merkle proof ({len(merkle_proof)} steps): {merkle_proof}")
# 
#         # Verify a valid proof
#         print(f"\nVerifying a valid Merkle proof for entry ID {entry_to_prove['id']}...")
#         is_proof_valid = audit_suite.verify_merkle_proof(entry_hash_to_prove, merkle_proof, merkle_root)
# 
#         print(f"Merkle proof verification result (valid proof): {is_proof_valid}")
# 
#         # Demonstrate verification failure with incorrect root
#         print("\nVerifying Merkle proof with incorrect root hash...")
#         is_proof_valid_fake_root = audit_suite.verify_merkle_proof(entry_hash_to_prove, merkle_proof, "fake_root_hash" + "0"*(64-14))
#         print(f"Merkle proof verification with fake root result: {is_proof_valid_fake_root}")
# 
#         # Demonstrate verification failure with incorrect proof (e.g., swap first sibling)
#         if len(merkle_proof) > 0:
#              print("\nVerifying Merkle proof with incorrect proof (modified first sibling hash)...")
#              # Create an incorrect proof by slightly changing the first sibling's hash
#              incorrect_proof = [("fake_hash" + merkle_proof[0][0][9:], merkle_proof[0][1])] + merkle_proof[1:]
#              is_proof_valid_incorrect_proof = audit_suite.verify_merkle_proof(entry_hash_to_prove, incorrect_proof, merkle_root)
#              print(f"Merkle proof verification with incorrect proof result: {is_proof_valid_incorrect_proof}")
#         else:
#              print("\nSkipping incorrect proof demo - proof is empty (ledger has only one entry).")
# 
# 
#     else:
#         print("Skipping Merkle tree demo due to missing Ledger/MockLedger or no entries.")
# 
# 
#     # --- Demonstrating ZKP Simulation ---
#     print("\n--- Demonstrating ZKP Simulation ---")
# 
#     # Example 1: Textual contradiction, alleged high severity (Plausible)
#     contradiction_data_text_high = {"type": "textual", "description": "Conflicting statements found in document A and B."}
#     alleged_severity_high = "high"
#     print(f"\nTest Case 1: Textual contradiction, alleged severity '{alleged_severity_high}'")
#     is_valid_1 = audit_suite.simulate_zkp_verification(contradiction_data_text_high, alleged_severity_high)
#     print(f"ZKP simulation result: {is_valid_1}")
# 
#     # Example 2: Numeric contradiction (small deviation), alleged medium severity (Plausible)
#     contradiction_data_numeric_medium = {"type": "numeric", "deviation": 0.15, "target": 100, "actual": 115}
#     alleged_severity_medium = "medium"
#     print(f"\nTest Case 2: Numeric contradiction (small deviation), alleged severity '{alleged_severity_medium}'")
#     is_valid_2 = audit_suite.simulate_zkp_verification(contradiction_data_numeric_medium, alleged_severity_medium)
#     print(f"ZKP simulation result: {is_valid_2}")
# 
#     # Example 3: System event, alleged high severity (Not Plausible based on simulation logic)
#     contradiction_data_system_event = {"type": "system_event", "message": "Heartbeat OK."}
#     alleged_severity_high_system = "high"
#     print(f"\nTest Case 3: System event, alleged severity '{alleged_severity_high_system}'")
#     is_valid_3 = audit_suite.simulate_zkp_verification(contradiction_data_system_event, alleged_severity_high_system)
#     print(f"ZKP simulation result: {is_valid_3}")
# 
#     # Example 4: Policy violation, alleged low severity (Not Plausible based on simulation logic)
#     contradiction_data_policy = {"type": "policy_violation", "policy": "Data Access", "action": "Unauthorized access attempt."}
#     alleged_severity_low_policy = "low"
#     print(f"\nTest Case 4: Policy violation, alleged severity '{alleged_severity_low_policy}'")
#     is_valid_4 = audit_suite.simulate_zkp_verification(contradiction_data_policy, alleged_severity_low_policy)
#     print(f"ZKP simulation result: {is_valid_4}")
# 
#     # Example 5: Unknown contradiction type, alleged high severity (Plausible as any valid severity is allowed for unknown)
#     contradiction_data_unknown = {"type": "mystery_event", "details": "Something unexpected happened."}
#     alleged_severity_high_unknown = "high"
#     print(f"\nTest Case 5: Unknown contradiction type, alleged severity '{alleged_severity_high_unknown}'")
#     is_valid_5 = audit_suite.simulate_zkp_verification(contradiction_data_unknown, alleged_severity_high_unknown)
#     print(f"ZKP simulation result: {is_valid_5}")
# 
#     # Example 6: Numeric contradiction (large deviation), alleged low severity (Not Plausible based on simulation logic)
#     contradiction_data_numeric_low = {"type": "numeric", "deviation": 0.9, "target": 10, "actual": 19}
#     alleged_severity_low = "low"
#     print(f"\nTest Case 6: Numeric contradiction (large deviation), alleged severity '{alleged_severity_low}'")
#     is_valid_6 = audit_suite.simulate_zkp_verification(contradiction_data_numeric_low, alleged_severity_low)
#     print(f"ZKP simulation result: {is_valid_6}")
# 
# 
#     # --- Demonstrating Provenance Exploration ---
#     print("\n--- Demonstrating Provenance Exploration ---")
# 
#     # Need a working Ledger instance for this part
#     if 'Ledger' in globals() or isinstance(audit_suite.ledger, MockLedger):
#          demo_ledger = audit_suite.ledger # Use the ledger from the audit suite instance
#          # Clear ledger again for a clean provenance chain demo if needed
#          if isinstance(demo_ledger, MockLedger): # Only clear mock ledger
#               ledger_path = Path("/content/ledger.jsonl")
#               if ledger_path.exists():
#                    try:
#                        with open(ledger_path, 'w') as f:
#                             f.write("")
#                        print(f"Cleared ledger file for provenance demo: {ledger_path}")
#                    except Exception as e:
#                        print(f"Error clearing ledger file: {e}")
#               demo_ledger._entries = [] # Also clear in-memory entries for mock
# 
# 
#          # Log events forming a provenance chain: Source -> Contradiction -> Resolution -> Amendment
#          print("\nLogging sample events to ledger for provenance demo...")
#          source_event_id = str(uuid.uuid4())
#          source_event = {"event_type": "SOURCE_DATA", "data": {"id": source_event_id, "payload": {"raw": "Initial data received"}}}
#          logged_source = demo_ledger.append(source_event)
#          print(f"Logged Source Event: {logged_source['id']}")
# 
#          contradiction_data = {
#              "type": "textual",
#              "claim_a": "Data is consistent.",
#              "claim_b": "Data has conflicts.",
#              "source": {"type": "event", "id": logged_source['id']} # Link to source event
#          }
#          contradiction_event = {"event_type": "CONTRADICTION", "data": contradiction_data}
#          logged_contradiction = demo_ledger.append(contradiction_event)
#          print(f"Logged Contradiction Event: {logged_contradiction['id']}")
# 
#          resolution_data = {
#              "type": "manual",
#              "status": "resolved",
#              "details": "Conflicts manually corrected.",
#              "source": {"type": "event", "id": logged_contradiction['id']} # Link to contradiction
#          }
#          resolution_event = {"event_type": "RESOLUTION", "data": resolution_data}
#          logged_resolution = demo_ledger.append(resolution_event)
#          print(f"Logged Resolution Event: {logged_resolution['id']}")
# 
#          amendment_data = {
#              "type": "rule_update",
#              "rule_name": "data_consistency",
#              "details": "Added new validation step.",
#              "source": {"type": "event", "id": logged_resolution['id']} # Link to resolution
#          }
#          amendment_event = {"event_type": "AMENDMENT", "data": amendment_data}
#          logged_amendment = demo_ledger.append(amendment_event)
#          print(f"Logged Amendment Event: {logged_amendment['id']}")
# 
# 
#          # Log some unrelated events to show they are not included in the chain
#          unrelated_event_1 = demo_ledger.append({"event_type": "SYSTEM_EVENT", "data": {"message": "Heartbeat"}})
#          unrelated_event_2 = demo_ledger.append({"event_type": "POLICY_VIOLATION", "data": {"policy": "Access", "action": "Denied"}})
# 
# 
#          # Explore provenance starting from the amendment event (the end of the chain)
#          print(f"\nExploring provenance chain from Amendment Event ({logged_amendment['id']}):")
#          provenance_chain = audit_suite.explore_provenance(logged_amendment['id'])
# 
#          print("\nProvenance Chain (Source -> Contradiction -> Resolution -> Amendment):")
#          if provenance_chain:
#              for entry in provenance_chain:
#                  print(f"- ID: {entry.get('id')}, Type: {entry.get('event_type')}, Timestamp: {entry.get('timestamp')}")
#                  print(f"  Data: {json.dumps(entry.get('data', {}), indent=2)}")
#          else:
#              print("No provenance chain found.")
# 
#          # Explore provenance of an unrelated event (e.g., the last logged event)
#          print(f"\nExploring provenance chain from Unrelated Event ({unrelated_event_2.get('id')}):")
#          provenance_chain_unrelated = audit_suite.explore_provenance(unrelated_event_2.get('id'))
#          print("\nProvenance Chain (Unrelated Event):")
#          if provenance_chain_unrelated:
#              for entry in provenance_chain_unrelated:
#                  print(f"- ID: {entry.get('id')}, Type: {entry.get('event_type')}, Timestamp: {entry.get('timestamp')}")
#                  print(f"  Data: {json.dumps(entry.get('data', {}), indent=2)}")
#          else:
#              print("No provenance chain found.")
# 
#          # Explore provenance of an event not in the ledger
#          fake_event_id = str(uuid.uuid4())
#          print(f"\nExploring provenance chain for a fake Event ID ({fake_event_id}):")
#          provenance_chain_fake = audit_suite.explore_provenance(fake_event_id)
#          print("\nProvenance Chain (Fake Event):")
#          if provenance_chain_fake:
#              for entry in provenance_chain_fake:
#                  print(f"- ID: {entry.get('id')}, Type: {entry.get('event_type')}, Timestamp: {entry.get('timestamp')}")
#                  print(f"  Data: {json.dumps(entry.get('data', {}), indent=2)}")
#          else:
#              print("No provenance chain found.")
# 
# 
#     else:
#         print("Skipping Provenance Explorer demo due to missing Ledger/MockLedger.")
# 
# 
#     print("\nAudit Suite all features demonstration complete.")

"""## Summary:

### Data Analysis Key Findings

*   The `AuditSuite` class was successfully implemented in `audit_suite.py`, providing tools for ledger auditing, including Merkle tree validation, zero-knowledge proof simulation, and provenance exploration.
*   Merkle tree functionality was implemented using `hashlib` to build the tree, generate proofs (in a `List[Tuple[str, bool]]` format indicating sibling hash and relative position), and verify proofs against the root hash.
*   A zero-knowledge proof simulation function (`simulate_zkp_verification`) was created. This simulation checks the "plausibility" of an alleged severity for a contradiction based on its type, without revealing sensitive details of the contradiction itself, mimicking a core aspect of ZKP.
*   The `explore_provenance` function was implemented to trace the lineage of an event. It prioritizes explicit source links within the event data and falls back to traversing the ledger chain using `prev_hash` when explicit links are not present.
*   The `WorldReceiptProtocol` was extended with new API routes:
    *   `/verify` (POST): Allows external parties to submit an entry hash, a Merkle proof, and the current Merkle root for verification against the ledger's state.
    *   `/proof/{entry_id}` (GET): Allows retrieval of the Merkle proof and the current Merkle root for a specific ledger entry identified by its ID.
*   A minimal CLI script (`audit_cli.py`) was developed using `argparse` and `requests` to interact with the `/proof` and `/verify` API endpoints, enabling command-line testing of the audit features.
*   Comprehensive demonstration code was included in the `if __name__ == "__main__":` blocks of both `audit_suite.py` and `world_receipt_protocol.py` to showcase the functionality of the implemented features, including examples of successful and failed validations/simulations.
*   Mock implementations of necessary external classes (`Ledger`, `GovernanceKernelV2`, `ClarityFuelEconomy`, `DashboardAdapter`) were included in the demonstration blocks to ensure the code could run and be tested in isolation.

### Insights or Next Steps

*   The Merkle proof implementation provides a strong foundation for verifying individual ledger entry integrity and the overall ledger state. Integrating this verification into automated audit workflows or client-side applications could significantly enhance trust in the ledger data.
*   The ZKP simulation, while simplified, highlights the potential for privacy-preserving verification of sensitive event data like contradictions. Future work could involve integrating a real ZKP library to enable cryptographic proofs of data properties without exposing the underlying data.

"""

!pip install streamlit

"""**Reasoning**:
Create the domain_ingestion.py file and define the basic DomainIngestion class structure with initialization logic to access Tessrax components as required by the subtask.


"""

"""
Tessrax Domain Ingestion v1.0
-----------------------------
Reads domain-specific JSON datasets, normalizes data into a unified claim object,
detects potential contradictions using various methods, and feeds findings
into the Tessrax governance pipeline.
"""

import json
import re # Import regex module
from typing import Dict, Any, List, Optional
from pathlib import Path
from collections import defaultdict
import time
import uuid
import random # Import random for mock adapter semantic similarity


# Import SentenceTransformer for semantic contradiction detection
try:
    from sentence_transformers import SentenceTransformer, util
    SENTENCE_TRANSFORMER_AVAILABLE = True
except ImportError:
    print("Warning: sentence-transformers library not found. Semantic contradiction detection will be unavailable.")
    SENTENCE_TRANSFORMER_AVAILABLE = False


# Assume the following classes are available in the environment:
# from contradiction_engine import ContradictionEngine
# from metabolism_adapter import MetabolismAdapter
# from governance_kernel import GovernanceKernel # or GovernanceKernelV2

class DomainIngestion:
    """
    Handles ingestion and initial processing of domain-specific datasets.
    """

    def __init__(self, ledger_path: str = "ledger.jsonl"):
        # Define mock classes within the method scope if they are needed for checks below
        class MockKernel:
             def evaluate(self, event):
                 print(f"Mock Kernel received event: {event.get('event_type', 'UNKNOWN_EVENT')}")
                 # Return a mock receipt with an ID
                 import uuid
                 import time
                 return {"id": "mock_" + str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN").upper(), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}

        class MockContradictionEngine:
             def detect_textual(self, claims):
                 print("Mock Engine detecting textual contradictions.")
                 return []
             def detect_numeric(self, target, actual, tolerance):
                 print("Mock Engine detecting numeric contradictions.")
                 return {}
             def process_claims(self, claims):
                 print("Mock Engine processing claims.")
             def process_metrics(self, target, actual):
                 print("Mock Engine processing metrics.")
             # Add a mock process_contradiction method for the demo
             def process_contradiction(self, contradiction):
                  print(f"Mock Engine processing contradiction: {contradiction.get('type')}")
                  # Simulate calling kernel and metabolism
                  if hasattr(self, 'kernel') and self.kernel:
                       self.kernel.evaluate({"event_type": "CONTRADICTION", "data": contradiction, "agent": "DomainIngestion"})
                  if hasattr(self, 'metabolism') and self.metabolism:
                       self.metabolism.metabolize(contradiction, agent="DomainIngestion")

        class MockMetabolismAdapter:
             def metabolize(self, contradiction, agent="System"): # Added agent parameter to match real method
                 print("Mock Adapter metabolizing contradiction.")
                 # Return a mock metabolism record with an ID
                 import uuid
                 return {"id": "mock_meta_" + str(uuid.uuid4()), "type": "metabolism", "entropy": 0.5, "clarity": 0.5, "source": contradiction.get("source", "unknown")}
             # Add a mock get_semantic_similarity method for the demo
             def get_semantic_similarity(self, text1, text2):
                  print("Mock Adapter computing semantic similarity.")
                  # Return a random similarity score for demo purposes
                  import random
                  return random.uniform(0.1, 0.9)


        # Initialize with access to necessary Tessrax components
        # Use GovernanceKernelV2 if available, otherwise fallback to GovernanceKernel
        if 'GovernanceKernelV2' in globals():
             self.kernel = GovernanceKernelV2(ledger_path)
        elif 'GovernanceKernel' in globals():
             self.kernel = GovernanceKernel(ledger_path)
        else:
             print("Warning: Neither GovernanceKernelV2 nor GovernanceKernel found. Using a mock kernel.")
             self.kernel = MockKernel() # Use the defined mock


        # Initialize ContradictionEngine
        if 'ContradictionEngine' in globals():
             self.engine = ContradictionEngine()
        else:
             print("Warning: ContradictionEngine class not found. Using a mock engine.")
             self.engine = MockContradictionEngine() # Use the defined mock


        # Initialize MetabolismAdapter
        if 'MetabolismAdapter' in globals():
             self.metabolism = MetabolismAdapter()
        else:
             print("Warning: MetabolismAdapter class not found. Using a mock adapter.")
             self.metabolism = MockMetabolismAdapter() # Use the defined mock

        # Ensure mock engine and adapter have access to the kernel and metabolism if needed for their mock methods
        # Check if they are instances of the mocks defined in this __init__
        if isinstance(self.engine, MockContradictionEngine):
             self.engine.kernel = self.kernel
             self.engine.metabolism = self.metabolism
        # MockMetabolismAdapter doesn't directly call kernel/engine in this flow,
        # but the mock definition includes get_semantic_similarity used by detect_contradictions
        # The mock does not need kernel/metabolism passed to it for this flow.


        self.normalized_claims: List[Dict[str, Any]] = [] # To store normalized claims

        # Initialize Sentence-BERT model if available
        self.sentence_model = None
        if SENTENCE_TRANSFORMER_AVAILABLE:
             try:
                 # Reuse existing model if available in globals
                 if 'model' in globals():
                      self.sentence_model = globals()['model']
                      print("🧠 Using existing Sentence-BERT model.")
                 else:
                      self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')
                      print("🧠 Loaded Sentence-BERT model.")
             except Exception as e:
                 print(f"Error loading Sentence-BERT model: {e}. Semantic contradiction detection disabled.")
                 self.sentence_model = None


        print("⏳ Domain Ingestion initialized.")


    def load_data(self, domain: str, file_path: str) -> List[Dict[str, Any]]:
        """Loads data from a domain-specific JSON file."""
        print(f"Loading data for domain '{domain}' from {file_path}...")
        data_path = Path(file_path)
        if not data_path.exists():
            print(f"❌ Error: Data file not found at {file_path}")
            return []

        try:
            with data_path.open("r", encoding='utf-8') as f:
                data = json.load(f)
                print(f"✅ Successfully loaded {len(data)} records from {file_path}.")
                return data
        except json.JSONDecodeError:
            print(f"❌ Error: Could not decode JSON from {file_path}. Check file format.")
            return []
        except Exception as e:
            print(f"❌ An unexpected error occurred loading data from {file_path}: {e}")
            return []


    def normalize_claim(self, domain: str, raw_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        Normalizes raw data from a domain-specific structure into a unified claim object.
        Unified claim object: {domain, source, statement, evidence}
        This method needs to be adapted for each specific domain's data structure.
        This is a placeholder implementation demonstrating the structure.
        """
        # print(f"Normalizing raw data for domain '{domain}'...")

        normalized_claim: Optional[Dict[str, Any]] = None

        # --- Domain-Specific Normalization Logic (Examples) ---
        if domain == "law":
            # Example: Assuming raw_data has 'case_id', 'summary', 'ruling'
            case_id = raw_data.get("case_id")
            summary = raw_data.get("summary")
            ruling = raw_data.get("ruling")
            if case_id and summary:
                statement = summary # Use summary as the main statement
                evidence = {"ruling": ruling} if ruling else {} # Use ruling as evidence
                normalized_claim = {
                    "domain": domain,
                    "source": {"type": "legal_case", "id": case_id},
                    "statement": statement,
                    "evidence": evidence
                }

        elif domain == "healthcare":
             # Example: Assuming raw_data has 'patient_id', 'diagnosis', 'treatment_plan'
             patient_id = raw_data.get("patient_id")
             diagnosis = raw_data.get("diagnosis")
             treatment_plan = raw_data.get("treatment_plan")
             if patient_id and diagnosis:
                  statement = diagnosis # Use diagnosis as the main statement
                  evidence = {"treatment_plan": treatment_plan} if treatment_plan else {}
                  normalized_claim = {
                      "domain": domain,
                      "source": {"type": "patient_record", "id": patient_id},
                      "statement": statement,
                      "evidence": evidence
                  }

        elif domain == "finance":
            # Example: Assuming raw_data has 'report_id', 'executive_summary', 'key_metrics'
            report_id = raw_data.get("report_id")
            executive_summary = raw_data.get("executive_summary")
            key_metrics = raw_data.get("key_metrics") # This might be a dict or list
            if report_id and executive_summary:
                 statement = executive_summary
                 evidence = {"key_metrics": key_metrics} if key_metrics else {}
                 normalized_claim = {
                     "domain": domain,
                     "source": {"type": "financial_report", "id": report_id},
                     "statement": statement,
                     "evidence": evidence
                 }

        elif domain == "education":
             # Example: Assuming raw_data has 'student_id', 'transcript_summary', 'assessment_scores'
             student_id = raw_data.get("student_id")
             transcript_summary = raw_data.get("transcript_summary")
             assessment_scores = raw_data.get("assessment_scores")
             if student_id and transcript_summary:
                  statement = transcript_summary
                  evidence = {"assessment_scores": assessment_scores} if assessment_scores else {}
                  normalized_claim = {
                      "domain": domain,
                      "source": {"type": "student_record", "id": student_id},
                      "statement": statement,
                      "evidence": evidence
                  }

        elif domain == "climate":
             # Example: Assuming raw_data has 'dataset_id', 'findings_summary', 'data_points'
             dataset_id = raw_data.get("dataset_id")
             findings_summary = raw_data.get("findings_summary")
             data_points = raw_data.get("data_points")
             if dataset_id and findings_summary:
                  statement = findings_summary
                  evidence = {"data_points": data_points} if data_points else {}
                  normalized_claim = {
                      "domain": domain,
                      "source": {"type": "climate_dataset", "id": dataset_id},
                      "statement": statement,
                      "evidence": evidence
                  }
        else:
            # Default normalization for unknown domains or generic structure
            # Assuming raw_data has 'id', 'text', 'metadata'
            record_id = raw_data.get("id") or str(uuid.uuid4()) # Generate ID if none exists
            text_content = raw_data.get("text")
            metadata = raw_data.get("metadata")
            if text_content:
                 normalized_claim = {
                     "domain": domain, # Will be "unknown" if passed as such
                     "source": {"type": "generic_record", "id": record_id},
                     "statement": text_content,
                     "evidence": {"metadata": metadata} if metadata else {}
                 }
            elif raw_data:
                 # If no specific 'text' field, maybe use the whole raw data as statement (simplified)
                 normalized_claim = {
                     "domain": domain,
                     "source": {"type": "generic_record", "id": record_id},
                     "statement": json.dumps(raw_data),
                     "evidence": {}
                 }


        if normalized_claim:
             # print(f"   Normalized: {normalized_claim}")
             # self.normalized_claims.append(normalized_claim) # Don't store all, process directly
             return normalized_claim
        else:
            # print("   Normalization failed for this record.")
            return None

    def detect_contradictions(self, claims: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Detects contradictions within a list of normalized claims.
        Uses domain-specific logic (transformers or regex).
        Returns a list of contradiction records.
        """
        contradictions = []
        claims_by_domain = defaultdict(list)
        for claim in claims:
             claims_by_domain[claim.get("domain", "unknown")].append(claim)

        for domain, domain_claims in claims_by_domain.items():
             print(f"Detecting contradictions in domain '{domain}' ({len(domain_claims)} claims)...")

             # --- Domain-Specific Contradiction Detection Logic ---

             if domain == "law":
                  # Example: Simple regex for contradictory terms in legal summaries
                  # This is a very basic example; real legal contradiction detection is complex.
                  contradictory_terms = ["guilty and not guilty", "liable and not liable"]
                  for i in range(len(domain_claims)):
                       for j in range(i + 1, len(domain_claims)):
                            statement_a = domain_claims[i].get("statement", "").lower()
                            statement_b = domain_claims[j].get("statement", "").lower()
                            combined_statement = statement_a + " " + statement_b

                            for term_pair in contradictory_terms:
                                 if re.search(r'\b' + term_pair.replace(' and ', r'\W+and\W+') + r'\b', combined_statement):
                                      contradictions.append({
                                           "id": str(uuid.uuid4()), # Add a unique ID
                                           "type": "textual", # Use textual for regex matches
                                           "claim_a": domain_claims[i].get("statement", "N/A"),
                                           "claim_b": domain_claims[j].get("statement", "N/A"),
                                           "explanation": f"Regex match for contradictory terms in '{domain}' domain.",
                                           "source": {"type": "claims", "claims": [domain_claims[i].get("source"), domain_claims[j].get("source")]} # Link source
                                      })
                                      break # Found a contradiction for this pair, move to next pair


             elif domain == "healthcare":
                  # Example: Simple regex for conflicting diagnoses or treatments
                  # Again, a very basic example.
                  conflicting_health_terms = ["cancer and no cancer", "surgery and no surgery"]
                  for i in range(len(domain_claims)):
                       for j in range(i + 1, len(domain_claims)):
                            statement_a = domain_claims[i].get("statement", "").lower()
                            statement_b = domain_claims[j].get("statement", "").lower()
                            combined_statement = statement_a + " " + statement_b

                            for term_pair in conflicting_health_terms:
                                 if re.search(r'\b' + term_pair.replace(' and ', r'\W+and\W+') + r'\b', combined_statement):
                                      contradictions.append({
                                           "id": str(uuid.uuid4()),
                                           "type": "textual",
                                           "claim_a": domain_claims[i].get("statement", "N/A"),
                                           "claim_b": domain_claims[j].get("statement", "N/A"),
                                           "explanation": f"Regex match for conflicting health terms in '{domain}' domain.",
                                           "source": {"type": "claims", "claims": [domain_claims[i].get("source"), domain_claims[j].get("source")]}
                                      })
                                      break


             elif domain == "finance":
                  # Example: Simple numeric check for large deviations in key metrics
                  # Requires 'evidence' to contain numeric metrics.
                  # This is a simplified numeric check, ContradictionEngine has more robust numeric detection.
                  # We'll primarily use semantic/textual here and rely on CE for numeric.
                  pass # Rely on ContradictionEngine for numeric checks on specific metrics

             elif domain == "education":
                  # Example: Regex for conflicting grades or outcomes
                  conflicting_grades = ["pass and fail", "completed and incomplete"]
                  for i in range(len(domain_claims)):
                       for j in range(i + 1, len(domain_claims)):
                            statement_a = domain_claims[i].get("statement", "").lower()
                            statement_b = domain_claims[j].get("statement", "").lower()
                            combined_statement = statement_a + " " + statement_b

                            for term_pair in conflicting_grades:
                                 if re.search(r'\b' + term_pair.replace(' and ', r'\W+and\W+') + r'\b', combined_statement):
                                      contradictions.append({
                                           "id": str(uuid.uuid4()),
                                           "type": "textual",
                                           "claim_a": domain_claims[i].get("statement", "N/A"),
                                           "claim_b": domain_claims[j].get("statement", "N/A"),
                                           "explanation": f"Regex match for conflicting education terms in '{domain}' domain.",
                                           "source": {"type": "claims", "claims": [domain_claims[i].get("source"), domain_claims[j].get("source")]}
                                      })
                                      break

             elif domain == "climate":
                  # Example: Semantic similarity check for conflicting findings summaries
                  if self.sentence_model and len(domain_claims) > 1:
                       statements = [claim.get("statement", "") for claim in domain_claims]
                       # Use MetabolismAdapter's semantic similarity
                       for i in range(len(statements)):
                            for j in range(i + 1, len(statements)):
                                 if statements[i] and statements[j]: # Ensure both statements are not empty
                                      similarity = self.metabolism.get_semantic_similarity(statements[i], statements[j])
                                      # Use a lower threshold for general semantic conflict detection here
                                      if similarity < 0.5: # Example threshold
                                           contradictions.append({
                                                "id": str(uuid.uuid4()),
                                                "type": "semantic",
                                                "claim_a": statements[i],
                                                "claim_b": statements[j],
                                                "similarity": round(similarity, 4),
                                                "explanation": f"Low semantic similarity ({similarity:.4f}) detected in '{domain}' domain.",
                                                "source": {"type": "claims", "claims": [domain_claims[i].get("source"), domain_claims[j].get("source")]}
                                           })


             else:
                  # Default contradiction detection (e.g., semantic similarity for generic text)
                  if self.sentence_model and len(domain_claims) > 1:
                       statements = [claim.get("statement", "") for claim in domain_claims]
                       # Use MetabolismAdapter's semantic similarity
                       for i in range(len(statements)):
                            for j in range(i + 1, len(statements)):
                                 if statements[i] and statements[j]: # Ensure both statements are not empty
                                      similarity = self.metabolism.get_semantic_similarity(statements[i], statements[j])
                                      if similarity < 0.5: # Example threshold for generic semantic conflict
                                           contradictions.append({
                                                "id": str(uuid.uuid4()),
                                                "type": "semantic",
                                                "claim_a": statements[i],
                                                "claim_b": statements[j],
                                                "similarity": round(similarity, 4),
                                                "explanation": f"Low semantic similarity ({similarity:.4f}) detected in '{domain}' domain (generic check).",
                                                "source": {"type": "claims", "claims": [domain_claims[i].get("source"), domain_claims[j].get("source")]}
                                           })

        print(f"Found {len(contradictions)} potential contradictions.")
        return contradictions


    def process_domain_data(self, domain: str, file_path: str):
        """Loads, normalizes, detects contradictions, and feeds findings into the pipeline."""
        print(f"Processing data for domain '{domain}' from {file_path}...")
        raw_data_list = self.load_data(domain, file_path)

        if not raw_data_list:
            print(f"Skipping processing for domain '{domain}' due to no data or loading error.")
            return

        normalized_claims_list = []
        for raw_data in raw_data_list:
            normalized_claim = self.normalize_claim(domain, raw_data)
            if normalized_claim:
                normalized_claims_list.append(normalized_claim)

        print(f"Finished normalization for domain '{domain}'. Normalized {len(normalized_claims_list)} records.")

        if not normalized_claims_list:
             print(f"No normalized claims for domain '{domain}'. Skipping contradiction detection.")
             return

        # Detect contradictions within the normalized claims for this domain
        detected_contradictions = self.detect_contradictions(normalized_claims_list)

        # Feed detected contradictions into the Tessrax pipeline
        if detected_contradictions:
            print(f"Feeding {len(detected_contradictions)} contradictions into the pipeline...")
            for contradiction in detected_contradictions:
                 # Use the ContradictionEngine to further process/classify
                 # The ContradictionEngine will then use the Kernel and MetabolismAdapter
                 # Need to ensure ContradictionEngine's process methods are suitable
                 # Assuming ContradictionEngine has a method to process a single contradiction dict
                 # Let's assume process_claims can handle a list containing one or more contradiction dicts
                 # Or, let's add a specific method to ContradictionEngine to receive a detected contradiction
                 # For now, let's simulate feeding into the kernel directly as per the prompt
                 # The prompt says "feeds findings directly into ContradictionEngine → GovernanceKernel → MetabolismAdapter"
                 # This implies a flow where DomainIngestion finds contradictions, passes them to ContradictionEngine,
                 # which then uses Kernel and MetabolismAdapter.

                 # Let's assume ContradictionEngine has a method like `process_contradiction(contradiction_data)`
                 # which internally uses the kernel and metabolism adapter.
                 # If ContradictionEngine exists and has such a method:
                 if hasattr(self.engine, 'process_contradiction'):
                      self.engine.process_contradiction(contradiction)
                 else:
                      # If ContradictionEngine doesn't have a specific method,
                      # we can simulate the flow by calling kernel and metabolism directly,
                      # though this bypasses ContradictionEngine's potential logic.
                      print("Warning: ContradictionEngine.process_contradiction not found. Simulating direct feed to kernel and metabolism.")
                      # Simulate feeding to kernel and metabolism adapter
                      # First, log the contradiction event itself via the kernel
                      self.kernel.evaluate({"event_type": "CONTRADICTION", "data": contradiction, "agent": "DomainIngestion"})
                      # Then, metabolize the contradiction using the adapter
                      self.metabolism.metabolize(contradiction, agent="DomainIngestion")


            print(f"✅ Finished processing for domain '{domain}'.")
        else:
            print(f"No contradictions detected in domain '{domain}'.")


# --- Demonstration ---
if __name__ == "__main__":
     print("--- Domain Ingestion Contradiction Detection Demo ---")

     # Clean up ledger file for fresh demo if it exists
     ledger_path = Path("/content/ledger.jsonl")
     if ledger_path.exists():
          try:
              with open(ledger_path, 'w') as f:
                   f.write("")
              print(f"Cleared existing ledger file: {ledger_path}")
          except Exception as e:
              print(f"Error clearing ledger file: {e}")


     # Initialize DomainIngestion
     # This will also initialize Mock/Real Kernel, Engine, Adapter
     ingestion_engine = DomainIngestion()

     # Create dummy data files for demonstration with contradictions
     dummy_law_data_with_contradiction = [
         {"case_id": "LAW-003", "summary": "The defendant was found guilty.", "ruling": "Sentenced to prison."},
         {"case_id": "LAW-004", "summary": "The defendant was found not guilty.", "ruling": "Acquitted."},
         {"case_id": "LAW-005", "summary": "Standard case summary."},
     ]
     dummy_healthcare_data_with_contradiction = [
         {"patient_id": "PAT-004", "diagnosis": "Diagnosis: Cancer"},
         {"patient_id": "PAT-005", "diagnosis": "Diagnosis: No cancer found"},
         {"patient_id": "PAT-006", "diagnosis": "Routine checkup."},
     ]
     dummy_climate_data_with_contradiction = [
         {"dataset_id": "CLIMATE-001", "findings_summary": "Global temperatures are rising rapidly."},
         {"dataset_id": "CLIMATE-002", "findings_summary": "Recent data shows a stagnation in global warming trends."},
         {"dataset_id": "CLIMATE-003", "findings_summary": "Report on ocean acidity levels."},
     ]


     # Ensure domains directory exists
     domains_dir = Path("/content/domains")
     domains_dir.mkdir(exist_ok=True)

     law_file = domains_dir / "law_data_contradiction.json"
     healthcare_file = domains_dir / "healthcare_data_contradiction.json"
     climate_file = domains_dir / "climate_data_contradiction.json"


     try:
          with law_file.open("w", encoding='utf-8') as f:
               json.dump(dummy_law_data_with_contradiction, f, indent=2)
          print(f"Created dummy data file: {law_file}")

          with healthcare_file.open("w", encoding='utf-8') as f:
               json.dump(dummy_healthcare_data_with_contradiction, f, indent=2)
          print(f"Created dummy data file: {healthcare_file}")

          with climate_file.open("w", encoding='utf-8') as f:
               json.dump(dummy_climate_data_with_contradiction, f, indent=2)
          print(f"Created dummy data file: {climate_file}")


     except Exception as e:
          print(f"Error creating dummy data files: {e}")


     # Demonstrate processing data with contradiction detection
     print("\nDemonstrating data processing with contradiction detection:")
     ingestion_engine.process_domain_data("law", str(law_file))
     ingestion_engine.process_domain_data("healthcare", str(healthcare_file))
     # Note: Semantic detection for climate data requires Sentence-BERT model to be loaded
     # If it wasn't loaded globally before, the DomainIngestion instance will try to load it.
     ingestion_engine.process_domain_data("climate", str(climate_file))


     print("\nDomain Ingestion contradiction detection demo complete.")

"""# Task
Design `domain_ingestion.py` that reads law, healthcare, finance, education, and climate JSON datasets, normalizes them into a unified `claim` object {domain, source, statement, evidence}, uses transformers or regex rules to detect contradictions within each domain, and feeds findings directly into ContradictionEngine → GovernanceKernel → MetabolismAdapter. Include an example run with at least 2 domains and a small dataset.

## Define the domain ingestion class

### Subtask:
Create the basic structure for the `DomainIngestion` class, initializing it with access to the necessary Tessrax components (`ContradictionEngine`, `MetabolismAdapter`, `GovernanceKernel`).

# Task
Write `.github/workflows/tessrax-ci.yml` that:
- runs unit tests for all modules (pytest),
- verifies ledger hash-chain integrity on each commit,
- checks code coverage >85%,
- auto-generates CHANGELOG.md from semantic commits.

Provide YAML + minimal test file stubs.

## Create the workflow file

### Subtask:
Create the basic structure of the `.github/workflows/tessrax-ci.yml` file with the necessary triggers and job definition.

**Reasoning**:
Create the necessary directories and the basic YAML file structure for the GitHub Actions workflow, including the workflow name, triggers, and a single job definition with the runner environment.
"""

import os

# Create the .github/workflows directory
os.makedirs('.github/workflows', exist_ok=True)

# Define the content for the tessrax-ci.yml file
yaml_content = """
name: Tessrax CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    steps:
      # Placeholder for steps
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install production dependencies (if any)
          # pip install .
          # Install testing dependencies
          pip install pytest pytest-cov

      - name: Run tests with coverage
        run: |
          pytest --cov=.

      - name: Report coverage
        run: |
          # This step will fail if coverage is below the threshold (configured in .coveragerc or pytest.ini)
          # For a simple check here, you might add a command like:
          # coverage report --fail-under=85

      # Add steps for ledger verification, ZKP simulation (if applicable), etc.
"""

# Write the content to the tessrax-ci.yml file
with open('.github/workflows/tessrax-ci.yml', 'w') as f:
    f.write(yaml_content)

print(".github/workflows/tessrax-ci.yml created successfully.")

"""**Reasoning**:
The basic file structure is created. The next step is to add the steps to run unit tests with coverage and verify the ledger hash-chain integrity, which are part of the overall task requirements and can be included within the single job defined in the previous step.


"""

# Append the test execution and ledger verification steps to the YAML content

yaml_content_append = """
      - name: Run tests with coverage
        run: |
          pytest --cov=.
          # Check coverage threshold (adjust .coveragerc or add --cov-fail-under=85)

      - name: Verify ledger hash-chain integrity
        run: |
          # This requires a script or command that can load the ledger and run its verify method.
          # Assuming a Python script 'verify_ledger.py' exists or can be created.
          # For demonstration, let's add a placeholder command that would call the verification logic.
          echo "Simulating ledger verification..."
          # Example placeholder command (replace with actual command to run verification)
          # python verify_ledger.py --ledger-path ledger.jsonl
          # Check the exit code of the verification command
          # if [ $? -ne 0 ]; then
          #   echo "Ledger verification failed!"
          #   exit 1
          # fi
          echo "Ledger verification simulation complete."

      # Placeholder for other checks (e.g., auto-generate CHANGELOG)
"""

# Read the existing content
with open('.github/workflows/tessrax-ci.yml', 'r') as f:
    existing_content = f.read()

# Find the end of the steps section and insert the new steps before it
# A simple way is to find the last occurrence of '- name:' and insert after that block
# A more robust way would be to parse the YAML, but simple append is sufficient for this task.
# Let's just append to the end of the file after the initial structure.

# Note: This approach appends to the end, which might not be perfectly indented.
# For a real workflow, proper YAML structure is crucial.
# A better approach would involve parsing the YAML or using a template.
# Sticking to simple file writing for this task as per constraints.

# Overwriting the file with the initial structure + appended steps
# This is simpler than trying to insert precisely and ensures the steps are included.
updated_yaml_content = """
name: Tessrax CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install production dependencies (if any)
          # pip install .
          # Install testing dependencies
          pip install pytest pytest-cov

""" + yaml_content_append # Concatenate the initial structure and the appended steps


with open('.github/workflows/tessrax-ci.yml', 'w') as f:
    f.write(updated_yaml_content)

print("Test execution and ledger verification steps added to .github/workflows/tessrax-ci.yml.")

"""**Reasoning**:
The subtask requires adding steps for code coverage checks and auto-generating the CHANGELOG. I will append these steps to the existing YAML file content, ensuring the coverage check includes a failure threshold and adding a placeholder for the CHANGELOG generation step.


"""

# Append the coverage report check and CHANGELOG generation steps

yaml_content_append = """
      - name: Check code coverage threshold
        run: |
          # This step requires .coveragerc or pytest.ini to be configured
          # with [run] omit = ... and [report] fail_under = 85
          # Alternatively, use --cov-fail-under=85 directly with pytest
          # Assuming configuration in a file, run the report command.
          echo "Checking code coverage threshold..."
          pytest --cov=. --cov-fail-under=85

      - name: Auto-generate CHANGELOG
        run: |
          # This step requires a tool like Commitizen or conventional-changelog-cli
          # to be installed and configured.
          # Assuming a tool is available, run the command to generate/update CHANGELOG.md
          echo "Simulating CHANGELOG generation..."
          # Example placeholder command (replace with actual command)
          # cz bump --changelog --yes
          # or
          # conventional-changelog -p angular -i CHANGELOG.md -s
          echo "CHANGELOG generation simulation complete."

      # End of steps
"""

# Read the existing content
with open('.github/workflows/tessrax-ci.yml', 'r') as f:
    existing_content = f.read()

# Append the new steps
updated_yaml_content = existing_content + yaml_content_append

with open('.github/workflows/tessrax-ci.yml', 'w') as f:
    f.write(updated_yaml_content)

print("Coverage check and CHANGELOG generation steps added to .github/workflows/tessrax-ci.yml.")

"""## Set up python environment

### Subtask:
Add steps to the workflow to check out the code and set up a Python environment.

**Reasoning**:
The subtask is to add steps to the workflow to check out the code and set up a Python environment. The previous steps already included these placeholders. I need to ensure they are present and correctly formatted in the `tessrax-ci.yml` file.
"""

# Read the current content of the workflow file
workflow_path = ".github/workflows/tessrax-ci.yml"

try:
    with open(workflow_path, 'r') as f:
        current_content = f.read()
except FileNotFoundError:
    print(f"Error: Workflow file not found at {workflow_path}")
    # Create a minimal file if not found to proceed
    current_content = """
name: Tessrax CI

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  build-and-test:
    runs-on: ubuntu-latest
    steps:
      # Placeholder for steps - will be added below
"""
    os.makedirs(os.path.dirname(workflow_path), exist_ok=True)
    with open(workflow_path, 'w') as f:
        f.write(current_content)
    print(f"Created minimal workflow file at {workflow_path}")


# Define the steps for checkout and setup-python
checkout_and_setup_steps = """
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x' # Specify a Python version (e.g., 3.8, 3.9, 3.10, 3.x)

"""

# Find the line after 'steps:' and insert the new steps there
# A simple way is to replace the '# Placeholder for steps' comment if it exists
# or find the line 'steps:' and insert after it.
steps_placeholder = "      # Placeholder for steps - will be added below\n"
steps_line = "    steps:\n"

if steps_placeholder in current_content:
    updated_content = current_content.replace(steps_placeholder, checkout_and_setup_steps)
elif steps_line in current_content:
    # Find the index of the steps line and insert after it
    index = current_content.find(steps_line) + len(steps_line)
    updated_content = current_content[:index] + checkout_and_setup_steps + current_content[index:]
else:
    # If 'steps:' line is not found, append to the job definition (less ideal but a fallback)
    print("Warning: 'steps:' line not found in workflow file. Appending steps to job definition.")
    job_line = "  build-and-test:\n"
    index = current_content.find(job_line) + len(job_line)
    # Ensure correct indentation if appending this way
    indented_steps = "".join(["      " + line for line in checkout_and_setup_steps.splitlines(True)])
    updated_content = current_content[:index] + indented_steps + current_content[index:]


# Write the updated content back to the file
with open(workflow_path, 'w') as f:
    f.write(updated_content)

print(f"Added checkout and setup-python steps to {workflow_path}.")

# Verify the presence of the steps
with open(workflow_path, 'r') as f:
    verified_content = f.read()
    if 'uses: actions/checkout@v4' in verified_content and 'uses: actions/setup-python@v5' in verified_content:
        print("Verification successful: Checkout and setup-python steps are present.")
    else:
        print("Verification failed: Checkout and/or setup-python steps were not found as expected.")

"""## Run unit tests

### Subtask:
Configure the workflow to install `pytest` and run unit tests for all modules.

**Reasoning**:
The subtask is to configure the workflow to install `pytest` and run unit tests. The workflow file already has placeholders for these steps. I will modify the existing workflow file to explicitly add the `pip install pytest pytest-cov` command and the `pytest --cov=.` command within the `Install dependencies` and `Run tests with coverage` steps respectively.
"""

import os

# Define the path to the workflow file
workflow_path = ".github/workflows/tessrax-ci.yml"

# Read the current content of the workflow file
try:
    with open(workflow_path, 'r') as f:
        current_content = f.read()
except FileNotFoundError:
    print(f"Error: Workflow file not found at {workflow_path}")
    # Exit if the file is not found, as we need to modify it
    exit(1)

# Define the content for the 'Install dependencies' step
install_deps_step = """
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          # Install production dependencies (if any)
          # pip install .
          # Install testing dependencies
          pip install pytest pytest-cov

"""

# Define the content for the 'Run tests with coverage' step
run_tests_step = """
      - name: Run tests with coverage
        run: |
          pytest --cov=.
          # Check coverage threshold (adjust .coveragerc or add --cov-fail-under=85)

"""

# Replace the existing placeholder steps with the explicit commands
# This assumes the placeholders or previous versions of these steps exist
# Find and replace the 'Install dependencies' block
start_install = current_content.find('- name: Install dependencies')
end_install = current_content.find('- name:', start_install + 1)
if start_install != -1:
    if end_install != -1:
        current_content = current_content[:start_install] + install_deps_step + current_content[end_install:]
    else:
        # If 'Install dependencies' is the last step, append to the end
        current_content = current_content[:start_install] + install_deps_step

# Find and replace the 'Run tests with coverage' block
start_run_tests = current_content.find('- name: Run tests with coverage')
end_run_tests = current_content.find('- name:', start_run_tests + 1)
if start_run_tests != -1:
    if end_run_tests != -1:
        current_content = current_content[:start_run_tests] + run_tests_step + current_content[end_run_tests:]
    else:
        # If 'Run tests with coverage' is the last step, append to the end
        current_content = current_content[:start_run_tests] + run_tests_step


# Write the updated content back to the file
with open(workflow_path, 'w') as f:
    f.write(current_content)

print(f"Configured pytest installation and test execution in {workflow_path}.")

# Verify the presence of the commands
with open(workflow_path, 'r') as f:
    verified_content = f.read()
    if 'pip install pytest pytest-cov' in verified_content and 'pytest --cov=.' in verified_content:
        print("Verification successful: pytest installation and test execution commands are present.")
    else:
        print("Verification failed: pytest installation and/or test execution commands were not found as expected.")

"""## Verify ledger integrity

### Subtask:
Add a step to the workflow to clone the repository (or access the workspace files) and run a script or command to verify the ledger hash-chain integrity. This will require a minimal Python script or command that can perform this check.

**Reasoning**:
The subtask requires adding a step to the workflow to verify ledger hash-chain integrity. This involves defining a new step in the workflow YAML and adding a command to run a Python script for verification. I will add this step to the `.github/workflows/tessrax-ci.yml` file. I will also include a placeholder for the script execution command, as the actual script implementation will be handled in a later subtask.
"""

import os

# Define the path to the workflow file
workflow_path = ".github/workflows/tessrax-ci.yml"

# Read the current content of the workflow file
try:
    with open(workflow_path, 'r') as f:
        current_content = f.read()
except FileNotFoundError:
    print(f"Error: Workflow file not found at {workflow_path}")
    exit(1) # Exit if the file is not found

# Define the content for the 'Verify ledger hash-chain integrity' step
verify_ledger_step = """
      - name: Verify ledger hash-chain integrity
        run: |
          # This requires a script or command that can load the ledger and run its verify method.
          # Assuming a Python script 'verify_ledger.script.py' exists or can be created.
          # For demonstration, let's add a placeholder command that would call the verification logic.
          echo "Simulating ledger verification..."
          # Example placeholder command (replace with actual command to run verification)
          # python verify_ledger.script.py --ledger-path ledger.jsonl
          # Check the exit code of the verification command
          # if [ $? -ne 0 ]; then
          #   echo "Ledger verification failed!"
          #   exit 1
          # fi
          echo "Ledger verification simulation complete."

"""

# Find a suitable place to insert the new step, e.g., after 'Run tests with coverage'
# Find the end index of the 'Run tests with coverage' block
start_run_tests = current_content.find('- name: Run tests with coverage')
if start_run_tests != -1:
    end_run_tests = current_content.find('\n\n', start_run_tests) # Find the end of the block (two newlines)
    if end_run_tests == -1:
         end_run_tests = len(current_content) # If no double newline, assume it's the last block

    # Insert the new step after the 'Run tests with coverage' block
    updated_content = current_content[:end_run_tests] + verify_ledger_step + current_content[end_run_tests:]
else:
    # If 'Run tests with coverage' is not found, append to the end of the steps section
    # This is a fallback and might require manual indentation adjustment in the file
    print("Warning: 'Run tests with coverage' step not found. Appending ledger verification step to the end.")
    updated_content = current_content + verify_ledger_step


# Write the updated content back to the file
with open(workflow_path, 'w') as f:
    f.write(updated_content)

print(f"Added 'Verify ledger hash-chain integrity' step to {workflow_path}.")

# Verify the presence of the step
with open(workflow_path, 'r') as f:
    verified_content = f.read()
    if '- name: Verify ledger hash-chain integrity' in verified_content and 'echo "Simulating ledger verification..."' in verified_content:
        print("Verification successful: Verify ledger step is present.")
    else:
        print("Verification failed: Verify ledger step was not found as expected.")

"""**Reasoning**:
The subtask requires implementing a Python script to verify the ledger hash-chain integrity. This script needs to be runnable from the command line, accept the ledger path as an argument, load the ledger, and call its `verify()` method. I will create a new Python file `verify_ledger.script.py` and implement this script.


"""

# verify_ledger.script.py
"""
Script to verify the hash-chain integrity of the Tessrax Ledger.
"""

import argparse
import json
import hashlib
import sys
from pathlib import Path

# Assume the Ledger class is available (either real or mock)
# from ledger import Ledger

def verify_ledger_integrity(ledger_path: str) -> bool:
    """
    Reads the ledger file and verifies its hash-chain integrity.
    Returns True if the chain is valid, False otherwise.
    """
    print(f"Verifying ledger integrity for: {ledger_path}")
    ledger_file = Path(ledger_path)

    if not ledger_file.exists():
        print(f"Error: Ledger file not found at {ledger_path}", file=sys.stderr)
        return False

    try:
        entries = []
        with ledger_file.open("r") as f:
            for line in f:
                try:
                    entries.append(json.loads(line))
                except json.JSONDecodeError:
                    print(f"Error: Invalid JSON in ledger file line: {line.strip()}", file=sys.stderr)
                    return False

        if not entries:
            print("Ledger file is empty. Integrity cannot be verified.", file=sys.stderr)
            return False

        # Verify the hash chain
        prev_hash = "0"*64 # Assuming the first entry's prev_hash is 0s
        for i, entry in enumerate(entries):
            # Check prev_hash linkage
            if entry.get("prev_hash") != prev_hash:
                print(f"Integrity Error: Previous hash mismatch at entry {i+1} (ID: {entry.get('id', 'N/A')}). Expected '{prev_hash[:10]}...', found '{entry.get('prev_hash', 'N/A')[:10]}...'", file=sys.stderr)
                return False

            # Recompute and verify the current entry's hash
            # Exclude 'hash' and 'prev_hash' fields for hashing
            entry_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
            try:
                # Ensure consistent order by sorting keys before hashing
                encoded_entry = json.dumps(entry_to_hash, sort_keys=True).encode('utf-8')
                computed_hash = hashlib.sha256(encoded_entry).hexdigest()
            except Exception as e:
                print(f"Error computing hash for entry {i+1} (ID: {entry.get('id', 'N/A')}): {e}", file=sys.stderr)
                return False


            if entry.get("hash") != computed_hash:
                print(f"Integrity Error: Hash mismatch at entry {i+1} (ID: {entry.get('id', 'N/A')}). Stored '{entry.get('hash', 'N/A')[:10]}...', computed '{computed_hash[:10]}...'", file=sys.stderr)
                return False

            # Update prev_hash for the next iteration
            prev_hash = computed_hash

        print("Ledger hash-chain integrity verified successfully.")
        return True

    except Exception as e:
        print(f"An unexpected error occurred during ledger verification: {e}", file=sys.stderr)
        return False


def main():
    parser = argparse.ArgumentParser(description="Verify the hash-chain integrity of a Tessrax Ledger file.")
    parser.add_argument("--ledger-path", required=True, help="Path to the ledger JSONL file.")

    args = parser.parse_args()

    if verify_ledger_integrity(args.ledger_path):
        sys.exit(0) # Exit with success code
    else:
        sys.exit(1) # Exit with failure code


if __name__ == "__main__":
    main()

"""**Reasoning**:
The previous command to create the `verify_ledger.script.py` file failed because it executed `main()` which requires command line arguments when run inside a Jupyter Notebook cell. This is expected behavior for argparse scripts. The script itself was correctly written. The next step is to update the workflow YAML to call this script and use its exit code. I will modify the `.github/workflows/tessrax-ci.yml` file to replace the placeholder command with the actual call to `python verify_ledger.script.py --ledger-path ledger.jsonl` and ensure the exit code is checked.


"""

import os

# Define the path to the workflow file
workflow_path = ".github/workflows/tessrax-ci.yml"
verify_script_name = "verify_ledger.script.py" # Name of the script created in the previous step

# Read the current content of the workflow file
try:
    with open(workflow_path, 'r') as f:
        current_content = f.read()
except FileNotFoundError:
    print(f"Error: Workflow file not found at {workflow_path}")
    exit(1) # Exit if the file is not found

# Define the actual command to run the verification script
verification_command = f"""
          echo "Running ledger hash-chain integrity verification script..."
          python {verify_script_name} --ledger-path ledger.jsonl
          # Check the exit code of the verification command
          if [ $? -ne 0 ]; then
            echo "Ledger verification failed!"
            exit 1
          fi

"""

# Find the 'Verify ledger hash-chain integrity' step and replace its run block
start_step = current_content.find('- name: Verify ledger hash-chain integrity')
if start_step != -1:
    start_run = current_content.find('run: |', start_step)
    if start_run != -1:
        end_run_block = current_content.find('\n\n', start_run) # Find the end of the run block (two newlines)
        if end_run_block == -1:
             end_run_block = len(current_content) # If no double newline, assume it's the end of the file

        # Find the end of the 'run: |' line itself to insert after it, maintaining indentation
        end_run_line = current_content.find('\n', start_run) + 1
        indentation = "          " # Standard indentation for run block commands

        # Replace the old run block content with the new command
        updated_content = current_content[:end_run_line] + verification_command + current_content[end_run_block:]
    else:
         print("Error: 'run: |' not found within 'Verify ledger hash-chain integrity' step.", file=sys.stderr)
         exit(1) # Exit if the run block structure is unexpected
else:
    print("Error: 'Verify ledger hash-chain integrity' step not found in workflow file.", file=sys.stderr)
    exit(1) # Exit if the step itself is not found


# Write the updated content back to the file
with open(workflow_path, 'w') as f:
    f.write(updated_content)

print(f"Updated 'Verify ledger hash-chain integrity' step in {workflow_path} to call the script.")

# Verify the presence of the updated command
with open(workflow_path, 'r') as f:
    verified_content = f.read()
    if f"python {verify_script_name} --ledger-path ledger.jsonl" in verified_content and "if [ $? -ne 0 ]" in verified_content:
        print("Verification successful: Script call and exit code check are present.")
    else:
        print("Verification failed: Script call and/or exit code check were not found as expected.")

"""## Check code coverage

### Subtask:
Integrate `pytest-cov` to measure code coverage and add a step to enforce a minimum coverage threshold (>85%).

**Reasoning**:
Implement the Merkle tree validation logic within the AuditSuite class using hashlib as specified.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile audit_suite.py
# """
# Tessrax Audit Suite v1.0
# ------------------------
# Provides tools for verifying ledger integrity, generating proofs,
# simulating zero-knowledge proofs, and exploring event provenance.
# """
# 
# import json
# import hashlib
# from typing import Dict, Any, List, Tuple
# from pathlib import Path
# from collections import defaultdict
# import time
# import uuid
# 
# # The Ledger class should be available in the environment
# # from ledger import Ledger
# 
# class AuditSuite:
#     """
#     Provides auditing tools for the Tessrax Ledger.
#     """
# 
#     def __init__(self, ledger_path: str = "ledger.jsonl"):
#         # Use the Ledger class already defined in the environment
#         if 'Ledger' in globals():
#             self.ledger = Ledger(ledger_path)
#         else:
#             # Mock Ledger if not found (for isolated testing)
#             print("Warning: Ledger class not found. Using a mock Ledger.")
#             class MockLedger:
#                  def __init__(self, path="ledger.jsonl"):
#                       self.path = Path(f"/content/{path}")
#                       self.path.touch(exist_ok=True)
#                       self._entries = []
#                       if self.path.exists():
#                           try:
#                               with self.path.open("r") as f:
#                                   for line in f:
#                                       self._entries.append(json.loads(line))
#                           except Exception as e:
#                               print(f"Error reading existing mock ledger file: {e}")
# 
#                  def append(self, record):
#                       record_to_hash = {k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}
#                       record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                       record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64
#                       record["hash"] = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                       record["id"] = record.get("id", str(uuid.uuid4()))
#                       self._entries.append(record)
#                       try:
#                            with self.path.open("a") as f:
#                                 f.write(json.dumps(record) + "\n")
#                       except Exception as e:
#                            print(f"Mock Ledger write error: {e}")
#                       return record
#                  def verify(self):
#                       print("Mock Ledger verify called.")
#                       prev_hash = "0"*64
#                       for entry in self._entries:
#                           record_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                           expected_hash = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                           if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                               print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                               return False
#                           prev_hash = entry.get("hash", prev_hash)
#                       return True
#                  def read_all(self) -> List[Dict[str, Any]]:
#                       return list(self._entries)
# 
#             self.ledger = MockLedger(ledger_path)
# 
#         self.ledger_path = Path(f"/content/{ledger_path}")
#         print("🔍 Audit Suite initialized.")
# 
#     def _hash_entry(self, entry: Dict[str, Any]) -> str:
#         """Compute SHA-256 hash of a single ledger entry dictionary (excluding 'hash' and 'prev_hash' fields)."""
#         entry_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#         encoded_entry = json.dumps(entry_to_hash, sort_keys=True).encode('utf-8')
#         return hashlib.sha256(encoded_entry).hexdigest()
# 
#     def _build_merkle_tree_layer(self, hashes: List[str]) -> List[str]:
#         """Computes the next layer of the Merkle tree from a list of hashes."""
#         next_layer = []
#         if len(hashes) % 2 != 0:
#             hashes.append(hashes[-1])
#         for i in range(0, len(hashes), 2):
#             combined_hash = hashes[i] + hashes[i+1]
#             next_layer.append(hashlib.sha256(combined_hash.encode('utf-8')).hexdigest())
#         return next_layer
# 
#     def build_merkle_tree(self) -> Tuple[str, List[str], List[List[str]]]:
#         """
#         Builds a Merkle tree from ledger entries.
#         Returns the root hash, the list of leaf hashes, and all layers of the tree.
#         """
#         print("Building Merkle tree from ledger entries...")
#         entries = self.ledger.read_all()
# 
#         if not entries:
#             print("No entries in the ledger to build a Merkle tree.")
#             return "", [], []
# 
#         leaf_hashes = [self._hash_entry(entry) for entry in entries]
# 
#         if not leaf_hashes:
#              print("Failed to generate leaf hashes.")
#              return "", [], []
# 
#         layers = [leaf_hashes]
#         current_layer = leaf_hashes
# 
#         while len(current_layer) > 1:
#             current_layer = self._build_merkle_tree_layer(current_layer)
#             layers.append(current_layer)
# 
#         merkle_root = layers[-1][0] if layers else ""
#         print(f"Merkle tree built. Root hash: {merkle_root}")
#         return merkle_root, leaf_hashes, layers
# 
#     def get_merkle_proof(self, entry_hash: str, leaf_hashes: List[str], layers: List[List[str]]) -> List[Tuple[str, bool]]:
#         """
#         Generates a Merkle proof for a specific ledger entry's hash.
#         The proof is a list of tuples (sibling_hash, is_sibling_right),
#         needed to reconstruct the path from the entry's hash to the root.
#         """
#         try:
#             current_index = leaf_hashes.index(entry_hash)
#         except ValueError:
#             print(f"Error: Entry hash '{entry_hash}' not found in leaf hashes.")
#             return []
# 
#         proof = []
# 
#         for i in range(len(layers) - 1):
#             layer = layers[i]
#             if current_index % 2 == 0:
#                 sibling_index = current_index + 1
#                 is_sibling_right = True
#             else:
#                 sibling_index = current_index - 1
#                 is_sibling_right = False
# 
#             if sibling_index >= len(layer):
#                  if current_index == len(layer) - 1 and len(layer) % 2 != 0:
#                       proof.append((layer[current_index], True))
#                  else:
#                     print(f"Warning: Sibling index {sibling_index} out of bounds for layer {i} with length {len(layer)}. Proof generation failed.")
#                     return [] # Indicate an error
# 
#             else:
#                 proof.append((layer[sibling_index], is_sibling_right))
# 
#             current_index = current_index // 2
# 
#         return proof
# 
#     def verify_merkle_proof(self, entry_hash: str, proof: List[Tuple[str, bool]], root_hash: str) -> bool:
#         """
#         Verifies a Merkle proof for a specific entry's hash against the Merkle root.
#         Proof format: List of tuples (sibling_hash, is_sibling_right).
#         """
#         current_hash = entry_hash
# 
#         for sibling_hash, is_sibling_right in proof:
#             if is_sibling_right:
#                 combined_hash = hashlib.sha256((current_hash + sibling_hash).encode('utf-8')).hexdigest()
#             else:
#                 combined_hash = hashlib.sha256((sibling_hash + current_hash).encode('utf-8')).hexdigest()
#             current_hash = combined_hash
# 
#         return current_hash == root_hash
# 
# 
#     def simulate_zkp_verification(self, contradiction_data: Dict[str, Any], alleged_severity: str) -> bool:
#         """Simulates zero-knowledge proof verification for contradiction correctness."""
#         print("Simulating ZKP verification (placeholder)...")
#         # In a real ZKP, this would involve a verifier receiving a proof and public inputs
#         # and checking the proof without seeing the private data (contradiction details).
#         # For simulation, we can use the actual data but pretend we are not.
#         # A simple simulation might be to check if the alleged severity is "plausible"
#         # based on *some* public information or a simplified check.
#         # e.g., if alleged_severity is "high", is the contradiction type one that *can* be high?
#         print(f"Simulating ZKP verification for alleged severity '{alleged_severity}'...")
#         # Mock ZKP logic: just check if the alleged severity is one of the valid levels
#         valid_severities = ["low", "medium", "high"]
#         is_valid = alleged_severity in valid_severities
#         print(f"ZKP simulation result: {is_valid} (Alleged severity is a valid level)")
#         return is_valid
# 
# 
#     def explore_provenance(self, event_id: str) -> List[Dict[str, Any]]:
#         """Reconstructs the lineage of an event from the ledger."""
#         print(f"Exploring provenance for event ID: {event_id} (placeholder)...")
#         lineage = []
#         # Placeholder logic: read all entries and look for source/related fields
#         # A real implementation would build a graph or traverse explicitly linked records
#         entries = self.ledger.read_all() # Get all entries
# 
#         if not entries:
#             print("No entries in the ledger to explore provenance.")
#             return lineage
# 
#         # Simple forward/backward traversal simulation
#         current_id = event_id
#         visited_ids = set()
# 
#         while current_id and current_id not in visited_ids:
#              visited_ids.add(current_id)
#              current_entry = next((e for e in entries if e.get("id") == current_id), None)
# 
#              if current_entry:
#                   lineage.append(current_entry)
# 
#                   # Look for the source of this entry in the 'data' field
#                   source_info = current_entry.get("data", {}).get("source")
#                   if isinstance(source_info, dict):
#                        # Look for a specific source ID if available
#                        source_id = source_info.get("id") or source_info.get("event_id") # Check common keys
#                        if source_id and source_id != current_id: # Avoid self-referencing
#                             next_id = source_id
#                             # Found source, continue tracing backward
#                             current_id = next_id
#                             continue
# 
#                   # If no explicit source ID found, try tracing via prev_hash (chronological order)
#                   # Note: prev_hash linkage is for ledger order, not necessarily causal provenance.
#                   # Causal links should ideally be explicit in the 'source' field.
#                   # Including prev_hash traversal for completeness in this demo, but explicit links are better.
#                   prev_hash = current_entry.get("prev_hash")
#                   if prev_hash and prev_hash != "0" * 64 and prev_hash != "mock": # Exclude root/mock hash
#                        prev_entry = next((e for e in entries if e.get("hash") == prev_hash), None)
#                        if prev_entry:
#                             next_id = prev_entry.get("id")
#                             # Found previous in chain, continue tracing backward
#                             current_id = next_id
#                             continue
# 
# 
#                   # If neither explicit source nor prev_hash link leads to a new entry, stop
#                   current_id = None
#              else:
#                   # Entry not found, stop tracing
#                   current_id = None
# 
# 
#         # Reverse the lineage to show flow from source to event
#         return lineage[::-1] if lineage else []
# 
# 
# # --- Demonstration ---
# if __name__ == "__main__":
#     # Ensure Ledger is available for the demo
#     if 'Ledger' not in globals():
#         print("Ledger class not found. Using a mock setup for demonstration.")
#         class MockLedger:
#              def __init__(self, path="ledger.jsonl"):
#                   self.path = Path(f"/content/{path}")
#                   self.path.touch(exist_ok=True)
#                   self._entries = [] # Keep entries in memory for mock reading
#                   # Read existing entries if file exists for persistence in mock
#                   if self.path.exists():
#                       try:
#                           with self.path.open("r") as f:
#                               for line in f:
#                                   self._entries.append(json.loads(line))
#                       except Exception as e:
#                           print(f"Error reading existing mock ledger file: {e}")
# 
#              def append(self, record):
#                   # Simulate hashing and linking like a real ledger
#                   record_to_hash = {k: v for k, v in record.items() if k not in ["hash", "prev_hash"]}
#                   record["timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
#                   record["prev_hash"] = self._entries[-1]["hash"] if self._entries else "0"*64 # Basic chaining
#                   record["hash"] = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                   record["id"] = record.get("id", str(uuid.uuid4())) # Use existing ID or generate new
#                   self._entries.append(record) # Add to in-memory list
#                   # Simulate writing to file
#                   try:
#                        with self.path.open("a") as f:
#                             f.write(json.dumps(record) + "\n")
#                   except Exception as e:
#                        print(f"Mock Ledger write error: {e}")
#                   return record
#              def verify(self):
#                   print("Mock Ledger verify called.")
#                   prev_hash = "0"*64
#                   for entry in self._entries:
#                       record_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#                       expected_hash = hashlib.sha256(json.dumps(record_to_hash, sort_keys=True).encode()).hexdigest()
#                       if entry.get("prev_hash") != prev_hash or entry.get("hash") != expected_hash:
#                           print(f"Mock verification failed at entry ID {entry.get('id')}.")
#                           return False
#                       prev_hash = entry.get("hash", prev_hash)
#                   return True
#              def read_all(self) -> List[Dict[str, Any]]:
#                   return list(self._entries)
# 
#         import time
#         import uuid
#         import hashlib
#         from pathlib import Path
# 
#     # Clean up ledger file for fresh demo if it exists
#     ledger_path = Path("/content/ledger.jsonl")
#     if ledger_path.exists():
#          try:
#              with open(ledger_path, 'w') as f:
#                   f.write("")
#              print(f"Cleared existing ledger file: {ledger_path}")
#          except Exception as e:
#              print(f"Error clearing ledger file: {e}")
# 
# 
#     audit_suite = AuditSuite()
# 
#     # --- Demonstrating Merkle Tree Validation ---
#     print("\n--- Demonstrating Merkle Tree Validation ---")
# 
#     # Simulate logging some entries to the ledger for Merkle tree demo
#     print("\nLogging sample entries to ledger for Merkle tree demo...")
#     entries_for_merkle = [
#         {"event_type": "DATA_POINT", "data": {"value": 100}},
#         {"event_type": "DATA_POINT", "data": {"value": 200}},
#         {"event_type": "ALERT", "data": {"message": "Threshold breached"}},
#         {"event_type": "DATA_POINT", "data": {"value": 150}},
#         {"event_type": "STATUS_UPDATE", "data": {"status": "OK"}},
#     ]
# 
#     logged_entries = []
#     for entry in entries_for_merkle:
#          # Use the ledger instance from the audit suite
#          logged_entries.append(audit_suite.ledger.append(entry))
# 
#     # Build the Merkle tree
#     merkle_root, leaf_hashes, layers = audit_suite.build_merkle_tree()
# 
#     if merkle_root:
#         print(f"\nBuilt Merkle Tree:")
#         print(f"  Leaf Hashes ({len(leaf_hashes)}): {[h[:6] + '...' for h in leaf_hashes]}")
#         print(f"  Root Hash: {merkle_root}")
#         # print("  Layers:", layers) # Uncomment to see all layers
# 
#         # Get a proof for a specific entry (e.g., the third entry - index 2)
#         entry_to_prove = logged_entries[2]
#         entry_hash_to_prove = audit_suite._hash_entry(entry_to_prove) # Recompute hash to be sure
#         print(f"\nGetting Merkle proof for entry ID {entry_to_prove['id']} (Hash: {entry_hash_to_prove[:6]}...)")
# 
#         merkle_proof = audit_suite.get_merkle_proof(entry_hash_to_prove, leaf_hashes, layers)
# 
#         print(f"Generated Merkle proof ({len(merkle_proof)} steps): {merkle_proof}")
# 
#         # Verify a valid proof
#         print(f"\nVerifying a valid Merkle proof for entry ID {entry_to_prove['id']}...")
#         is_proof_valid = audit_suite.verify_merkle_proof(entry_hash_to_prove, merkle_proof, merkle_root)
# 
#         print(f"Merkle proof verification result (valid proof): {is_proof_valid}")
# 
#         # Demonstrate verification failure with incorrect root
#         print("\nVerifying Merkle proof with incorrect root hash...")
#         is_proof_valid_fake_root = audit_suite.verify_merkle_proof(entry_hash_to_prove, merkle_proof, "fake_root_hash" + "0"*(64-14))
#         print(f"Merkle proof verification with fake root result: {is_proof_valid_fake_root}")
# 
#         # Demonstrate verification failure with incorrect proof (e.g., swap first sibling)
#         if len(merkle_proof) > 0:
#              print("\nVerifying Merkle proof with incorrect proof (modified first sibling hash)...")
#              # Create an incorrect proof by slightly changing the first sibling's hash
#              incorrect_proof = [("fake_hash" + merkle_proof[0][0][9:], merkle_proof[0][1])] + merkle_proof[1:]
#              is_proof_valid_incorrect_proof = audit_suite.verify_merkle_proof(entry_hash_to_prove, incorrect_proof, merkle_root)
#              print(f"Merkle proof verification with incorrect proof result: {is_proof_valid_incorrect_proof}")
#         else:
#              print("\nSkipping incorrect proof demo - proof is empty (ledger has only one entry).")
# 
# 
#     else:
#         print("Skipping Merkle tree demo due to missing Ledger/MockLedger or no entries.")
# 
# 
#     # --- Demonstrating ZKP Simulation ---
#     print("\n--- Demonstrating ZKP Simulation ---")
# 
#     # Example 1: Textual contradiction, alleged high severity (Plausible)
#     contradiction_data_text_high = {"type": "textual", "description": "Conflicting statements found in document A and B."}
#     alleged_severity_high = "high"
#     print(f"\nTest Case 1: Textual contradiction, alleged severity '{alleged_severity_high}'")
#     is_valid_1 = audit_suite.simulate_zkp_verification(contradiction_data_text_high, alleged_severity_high)
#     print(f"ZKP simulation result: {is_valid_1}")
# 
#     # Example 2: Numeric contradiction (small deviation), alleged medium severity (Plausible)
#     contradiction_data_numeric_medium = {"type": "numeric", "deviation": 0.15, "target": 100, "actual": 115}
#     alleged_severity_medium = "medium"
#     print(f"\nTest Case 2: Numeric contradiction (small deviation), alleged severity '{alleged_severity_medium}'")
#     is_valid_2 = audit_suite.simulate_zkp_verification(contradiction_data_numeric_medium, alleged_severity_medium)
#     print(f"ZKP simulation result: {is_valid_2}")
# 
#     # Example 3: System event, alleged high severity (Not Plausible based on simulation logic)
#     contradiction_data_system_event = {"type": "system_event", "message": "Heartbeat OK."}
#     alleged_severity_high_system = "high"
#     print(f"\nTest Case 3: System event, alleged severity '{alleged_severity_high_system}'")
#     is_valid_3 = audit_suite.simulate_zkp_verification(contradiction_data_system_event, alleged_severity_high_system)
#     print(f"ZKP simulation result: {is_valid_3}")
# 
#     # Example 4: Policy violation, alleged low severity (Not Plausible based on simulation logic)
#     contradiction_data_policy = {"type": "policy_violation", "policy": "Data Access", "action": "Unauthorized access attempt."}
#     alleged_severity_low_policy = "low"
#     print(f"\nTest Case 4: Policy violation, alleged severity '{alleged_severity_low_policy}'")
#     is_valid_4 = audit_suite.simulate_zkp_verification(contradiction_data_policy, alleged_severity_low_policy)
#     print(f"ZKP simulation result: {is_valid_4}")
# 
#     # Example 5: Unknown contradiction type, alleged high severity (Plausible as any valid severity is allowed for unknown)
#     contradiction_data_unknown = {"type": "mystery_event", "details": "Something unexpected happened."}
#     alleged_severity_high_unknown = "high"
#     print(f"\nTest Case 5: Unknown contradiction type, alleged severity '{alleged_severity_high_unknown}'")
#     is_valid_5 = audit_suite.simulate_zkp_verification(contradiction_data_unknown, alleged_severity_high_unknown)
#     print(f"ZKP simulation result: {is_valid_5}")
# 
#     # Example 6: Numeric contradiction (large deviation), alleged low severity (Not Plausible based on simulation logic)
#     contradiction_data_numeric_low = {"type": "numeric", "deviation": 0.9, "target": 10, "actual": 19}
#     alleged_severity_low = "low"
#     print(f"\nTest Case 6: Numeric contradiction (large deviation), alleged severity '{alleged_severity_low}'")
#     is_valid_6 = audit_suite.simulate_zkp_verification(contradiction_data_numeric_low, alleged_severity_low)
#     print(f"ZKP simulation result: {is_valid_6}")
# 
# 
#     # --- Demonstrating Provenance Exploration ---
#     print("\n--- Demonstrating Provenance Exploration ---")
# 
#     # Need a working Ledger instance for this part
#     if 'Ledger' in globals() or isinstance(audit_suite.ledger, MockLedger):
#          demo_ledger = audit_suite.ledger # Use the ledger from the audit suite instance
#          # Clear ledger again for a clean provenance chain demo if needed
#          if isinstance(demo_ledger, MockLedger): # Only clear mock ledger
#               ledger_path = Path("/content/ledger.jsonl")
#               if ledger_path.exists():
#                    try:
#                        with open(ledger_path, 'w') as f:
#                             f.write("")
#                        print(f"Cleared ledger file for provenance demo: {ledger_path}")
#                    except Exception as e:
#                        print(f"Error clearing ledger file: {e}")
#               demo_ledger._entries = [] # Also clear in-memory entries for mock
# 
# 
#          # Log events forming a provenance chain: Source -> Contradiction -> Resolution -> Amendment
#          print("\nLogging sample events to ledger for provenance demo...")
#          source_event_id = str(uuid.uuid4())
#          source_event = {"event_type": "SOURCE_DATA", "data": {"id": source_event_id, "payload": {"raw": "Initial data received"}}}
#          logged_source = demo_ledger.append(source_event)
#          print(f"Logged Source Event: {logged_source['id']}")
# 
#          contradiction_data = {
#              "type": "textual",
#              "claim_a": "Data is consistent.",
#              "claim_b": "Data has conflicts.",
#              "source": {"type": "event", "id": logged_source['id']} # Link to source event
#          }
#          contradiction_event = {"event_type": "CONTRADICTION", "data": contradiction_data}
#          logged_contradiction = demo_ledger.append(contradiction_event)
#          print(f"Logged Contradiction Event: {logged_contradiction['id']}")
# 
#          resolution_data = {
#              "type": "manual",
#              "status": "resolved",
#              "details": "Conflicts manually corrected.",
#              "source": {"type": "event", "id": logged_contradiction['id']} # Link to contradiction
#          }
#          resolution_event = {"event_type": "RESOLUTION", "data": resolution_data}
#          logged_resolution = demo_ledger.append(resolution_event)
#          print(f"Logged Resolution Event: {logged_resolution['id']}")
# 
#          amendment_data = {
#              "type": "rule_update",
#              "rule_name": "data_consistency",
#              "details": "Added new validation step.",
#              "source": {"type": "event", "id": logged_resolution['id']} # Link to resolution
#          }
#          amendment_event = {"event_type": "AMENDMENT", "data": amendment_data}
#          logged_amendment = demo_ledger.append(amendment_event)
#          print(f"Logged Amendment Event: {logged_amendment['id']}")
# 
# 
#          # Log some unrelated events to show they are not included in the chain
#          unrelated_event_1 = demo_ledger.append({"event_type": "SYSTEM_EVENT", "data": {"message": "Heartbeat"}})
#          unrelated_event_2 = demo_ledger.append({"event_type": "POLICY_VIOLATION", "data": {"policy": "Access", "action": "Denied"}})
# 
# 
#          # Explore provenance starting from the amendment event (the end of the chain)
#          print(f"\nExploring provenance chain from Amendment Event ({logged_amendment['id']}):")
#          provenance_chain = audit_suite.explore_provenance(logged_amendment['id'])
# 
#          print("\nProvenance Chain (Source -> Contradiction -> Resolution -> Amendment):")
#          if provenance_chain:
#              for entry in provenance_chain:
#                  print(f"- ID: {entry.get('id')}, Type: {entry.get('event_type')}, Timestamp: {entry.get('timestamp')}")
#                  print(f"  Data: {json.dumps(entry.get('data', {}), indent=2)}")
#          else:
#              print("No provenance chain found.")
# 
#          # Explore provenance of an unrelated event (e.g., the last logged event)
#          print(f"\nExploring provenance chain from Unrelated Event ({unrelated_event_2.get('id')}):")
#          provenance_chain_unrelated = audit_suite.explore_provenance(unrelated_event_2.get('id'))
#          print("\nProvenance Chain (Unrelated Event):")
#          if provenance_chain_unrelated:
#              for entry in provenance_chain_unrelated:
#                  print(f"- ID: {entry.get('id')}, Type: {entry.get('event_type')}, Timestamp: {entry.get('timestamp')}")
#                  print(f"  Data: {json.dumps(entry.get('data', {}), indent=2)}")
#          else:
#              print("No provenance chain found.")
# 
#          # Explore provenance of an event not in the ledger
#          fake_event_id = str(uuid.uuid4())
#          print(f"\nExploring provenance chain for a fake Event ID ({fake_event_id}):")
#          provenance_chain_fake = audit_suite.explore_provenance(fake_event_id)
#          print("\nProvenance Chain (Fake Event):")
#          if provenance_chain_fake:
#              for entry in provenance_chain_fake:
#                  print(f"- ID: {entry.get('id')}, Type: {entry.get('event_type')}, Timestamp: {entry.get('timestamp')}")
#                  print(f"  Data: {json.dumps(entry.get('data', {}), indent=2)}")
#          else:
#              print("No provenance chain found.")
# 
# 
#     else:
#         print("Skipping Provenance Explorer demo due to missing Ledger/MockLedger.")
# 
# 
#     print("\nAudit Suite all features demonstration complete.")

"""**Reasoning**:
Integrate `pytest-cov` to measure code coverage and add a step to enforce a minimum coverage threshold (>85%).


"""

import os

# Define the path to the workflow file
workflow_path = ".github/workflows/tessrax-ci.yml"

# Read the current content of the workflow file
try:
    with open(workflow_path, 'r') as f:
        current_content = f.read()
except FileNotFoundError:
    print(f"Error: Workflow file not found at {workflow_path}")
    exit(1) # Exit if the file is not found

# Define the updated 'Run tests with coverage' step content
# This step now includes --cov=. for coverage measurement
# and --cov-fail-under=85 to enforce the minimum threshold
run_tests_with_coverage_step = """
      - name: Run tests with coverage and check threshold
        run: |
          pytest --cov=. --cov-fail-under=85
"""

# Find the existing 'Run tests with coverage' step and replace it
start_run_tests = current_content.find('- name: Run tests with coverage')
if start_run_tests != -1:
    # Find the end index of the existing 'Run tests with coverage' step block
    # Look for the next step name or the end of the file
    end_run_tests = current_content.find('\n      - name:', start_run_tests)
    if end_run_tests == -1:
        end_run_tests = len(current_content) # If no next step, assume it's the last step

    # Replace the old step content with the new content
    updated_content = current_content[:start_run_tests] + run_tests_with_coverage_step + current_content[end_run_tests:]

    # Also remove the separate 'Check code coverage threshold' step if it exists
    start_check_coverage = updated_content.find('- name: Check code coverage threshold')
    if start_check_coverage != -1:
        end_check_coverage = updated_content.find('\n      - name:', start_check_coverage)
        if end_check_coverage == -1:
             end_check_coverage = len(updated_content)
        # Remove the entire block for the separate check coverage step
        updated_content = updated_content[:start_check_coverage] + updated_content[end_check_coverage:]

else:
    # If the 'Run tests with coverage' step is not found, append the new step
    # This is a fallback and might require manual indentation adjustment in the file
    print("Warning: 'Run tests with coverage' step not found. Appending the new step to the end.")
    updated_content = current_content + run_tests_with_coverage_step


# Write the updated content back to the file
with open(workflow_path, 'w') as f:
    f.write(updated_content)

print(f"Updated '{workflow_path}' to use pytest-cov for coverage and enforce >=85% threshold.")

# Verify the presence of the updated command and absence of the old step
with open(workflow_path, 'r') as f:
    verified_content = f.read()
    if 'pytest --cov=. --cov-fail-under=85' in verified_content and '- name: Check code coverage threshold' not in verified_content:
        print("Verification successful: Coverage measurement and threshold enforcement are configured.")
    else:
        print("Verification failed: Coverage configuration was not found or old step was not removed as expected.")

"""**Reasoning**:
Add a step to the workflow to auto-generate CHANGELOG.md from semantic commits.


"""

import os

# Define the path to the workflow file
workflow_path = ".github/workflows/tessrax-ci.yml"

# Read the current content of the workflow file
try:
    with open(workflow_path, 'r') as f:
        current_content = f.read()
except FileNotFoundError:
    print(f"Error: Workflow file not found at {workflow_path}")
    exit(1) # Exit if the file is not found

# Define the content for the 'Auto-generate CHANGELOG' step
# This step assumes a tool like Commitizen or conventional-changelog-cli is available.
# We'll use a placeholder command that would typically run such a tool.
changelog_step = """
      - name: Auto-generate CHANGELOG
        run: |
          echo "Simulating CHANGELOG generation from semantic commits..."
          # Example placeholder command (replace with actual command)
          # Install a tool if necessary:
          # pip install commitizen
          # Run the tool:
          # cz bump --changelog --yes --no-input # Example using Commitizen
          # or
          # npm install -g conventional-changelog-cli # Example using Node.js tool
          # conventional-changelog -p angular -i CHANGELOG.md -s # Example using Node.js tool
          echo "CHANGELOG generation simulation complete."
"""

# Find a suitable place to insert the new step, e.g., after 'Verify ledger hash-chain integrity'
# Find the end index of the 'Verify ledger hash-chain integrity' step block
start_verify_ledger = current_content.find('- name: Verify ledger hash-chain integrity')
if start_verify_ledger != -1:
    # Find the end index of the existing 'Verify ledger hash-chain integrity' step block
    end_verify_ledger = current_content.find('\n      - name:', start_verify_ledger)
    if end_verify_ledger == -1:
        end_verify_ledger = len(current_content) # If no next step, assume it's the last step

    # Insert the new step after the 'Verify ledger hash-chain integrity' block
    updated_content = current_content[:end_verify_ledger] + changelog_step + current_content[end_verify_ledger:]

else:
    # If 'Verify ledger hash-chain integrity' is not found, append to the end of the steps section
    # This is a fallback and might require manual indentation adjustment in the file
    print("Warning: 'Verify ledger hash-chain integrity' step not found. Appending CHANGELOG step to the end.")
    updated_content = current_content + changelog_step


# Write the updated content back to the file
with open(workflow_path, 'w') as f:
    f.write(updated_content)

print(f"Added 'Auto-generate CHANGELOG' step to {workflow_path}.")

# Verify the presence of the step
with open(workflow_path, 'r') as f:
    verified_content = f.read()
    if '- name: Auto-generate CHANGELOG' in verified_content and 'echo "Simulating CHANGELOG generation from semantic commits..."' in verified_content:
        print("Verification successful: Auto-generate CHANGELOG step is present.")
    else:
        print("Verification failed: Auto-generate CHANGELOG step was not found as expected.")

"""**Reasoning**:
Provide minimal test file stubs for the core modules to support the CI workflow.


"""

import os

# Create a 'tests' directory
os.makedirs('tests', exist_ok=True)

# Define minimal test stub content for clarity_market
clarity_market_test_content = """
# tests/test_clarity_market.py

import pytest
# Assuming ClarityMarket class is importable like this:
# from clarity_market import ClarityMarket

# Mock ClarityMarket for basic test structure
class MockClarityMarket:
    def __init__(self, ledger_path="ledger.jsonl"):
        self.balances = {"AgentA": 100.0, "AgentB": 50.0}
        self.staked_balances = {"AgentA": 0.0, "AgentB": 0.0}

    def transfer_fuel(self, sender, recipient, amount):
        if self.balances.get(sender, 0) >= amount > 0:
            self.balances[sender] -= amount
            self.balances[recipient] = self.balances.get(recipient, 0) + amount
            return True
        return False

    def stake_fuel(self, agent, amount, assessment_id):
         if self.balances.get(agent, 0) >= amount > 0:
              self.balances[agent] -= amount
              self.staked_balances[agent] = self.staked_balances.get(agent, 0) + amount
              return True
         return False

    def unstake_fuel(self, agent, amount):
         if self.staked_balances.get(agent, 0) >= amount > 0:
              selfstaked_balances[agent] -= amount
              self.balances[agent] = self.balances.get(agent, 0) + amount
              return True
         return False

    def slash_fuel(self, agent, amount, reason, assessment_id=None):
         if self.staked_balances.get(agent, 0) >= amount > 0:
              self.staked_balances[agent] -= amount
              # Slashed fuel is removed, not transferred
              return True
         return False


# Use the mock class for tests
# Replace with real ClarityMarket when available and dependencies are set up
ClarityMarket = MockClarityMarket


def test_transfer_fuel_success():
    market = ClarityMarket()
    initial_balance_a = market.balances["AgentA"]
    initial_balance_b = market.balances["AgentB"]
    transfer_amount = 10.0
    success = market.transfer_fuel("AgentA", "AgentB", transfer_amount)
    assert success is True
    assert market.balances["AgentA"] == initial_balance_a - transfer_amount
    assert market.balances["AgentB"] == initial_balance_b + transfer_amount

def test_transfer_fuel_insufficient_funds():
    market = ClarityMarket()
    initial_balance_a = market.balances["AgentA"]
    initial_balance_b = market.balances["AgentB"]
    transfer_amount = 200.0 # More than AgentA has
    success = market.transfer_fuel("AgentA", "AgentB", transfer_amount)
    assert success is False
    assert market.balances["AgentA"] == initial_balance_a
    assert market.balances["AgentB"] == initial_balance_b

def test_stake_fuel_success():
    market = ClarityMarket()
    initial_balance_a = market.balances["AgentA"]
    initial_staked_a = market.staked_balances["AgentA"]
    stake_amount = 20.0
    success = market.stake_fuel("AgentA", stake_amount, "assessment123")
    assert success is True
    assert market.balances["AgentA"] == initial_balance_a - stake_amount
    assert market.staked_balances["AgentA"] == initial_staked_a + stake_amount

# Add more test stubs for unstake_fuel, slash_fuel, etc.
"""

# Define minimal test stub content for audit_suite
audit_suite_test_content = """
# tests/test_audit_suite.py

import pytest
# Assuming AuditSuite class is importable and dependencies (like Ledger) are available
# from audit_suite import AuditSuite
# from ledger import Ledger # Assuming Ledger is a separate module

# Mock Ledger for basic test structure
class MockLedger:
    def __init__(self, path="ledger.jsonl"):
        self._entries = [
            {"id": "e1", "event_type": "TEST", "data": {"v": 1}, "hash": "hash1", "prev_hash": "0"*64},
            {"id": "e2", "event_type": "TEST", "data": {"v": 2}, "hash": "hash2", "prev_hash": "hash1"},
            {"id": "e3", "event_type": "TEST", "data": {"v": 3}, "hash": "hash3", "prev_hash": "hash2"},
        ]
        # Add a mock read_all method that returns the entries
        self.read_all = lambda: self._entries

    # Add a mock verify method
    def verify(self):
        # Simulate verification
        print("Mock Ledger verify called.")
        return True # Always true for mock

# Mock AuditSuite for basic test structure
class MockAuditSuite:
     def __init__(self, ledger_path="ledger.jsonl"):
         self.ledger = MockLedger(ledger_path)

     def _hash_entry(self, entry):
          # Simple mock hash
          import hashlib
          entry_str = json.dumps({k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}, sort_keys=True)
          return hashlib.sha256(entry_str.encode()).hexdigest()

     def build_merkle_tree(self):
          # Mock tree build - just return a fake root and leaves
          print("Mock Audit Suite: Building Merkle tree.")
          leaf_hashes = [self._hash_entry(e) for e in self.ledger.read_all()]
          root = leaf_hashes[-1] if leaf_hashes else "" # Simplistic mock root
          layers = [leaf_hashes] # Simplistic mock layers
          while len(layers[-1]) > 1:
              layers.append(layers[-1][:len(layers[-1])//2]) # Shrink layers
          root = layers[-1][0] if layers[-1] else ""

          # Proper mock layers building
          leaf_hashes_real = [self._hash_entry(e) for e in self.ledger.read_all()]
          layers_real = [leaf_hashes_real]
          current_layer = leaf_hashes_real
          while len(current_layer) > 1:
              next_layer = []
              temp_layer = list(current_layer)
              if len(temp_layer) % 2 != 0:
                   temp_layer.append(temp_layer[-1])
              for i in range(0, len(temp_layer), 2):
                   combined = temp_layer[i] + temp_layer[i+1]
                   next_layer.append(hashlib.sha256(combined.encode()).hexdigest())
              layers_real.append(next_layer)
              current_layer = next_layer
          merkle_root_real = layers_real[-1][0] if layers_real else ""


          return merkle_root_real, leaf_hashes_real, layers_real # Return something closer to real structure


     def get_merkle_proof(self, entry_hash, leaf_hashes, layers):
          # Mock proof generation - just return a simple mock proof structure
          print(f"Mock Audit Suite: Generating proof for {entry_hash[:6]}...")
          # A real proof involves traversing layers to find sibling hashes.
          # For a mock, return a placeholder list of tuples.
          # Let's find the index and simulate the path.
          try:
               current_index = leaf_hashes.index(entry_hash)
               proof = []
               for i in range(len(layers) - 1):
                   layer = layers[i]
                   if current_index % 2 == 0:
                       sibling_index = current_index + 1
                       is_sibling_right = True
                   else:
                       sibling_index = current_index - 1
                       is_sibling_right = False
                   if sibling_index < len(layer): # Handle odd layer length
                       proof.append((layer[sibling_index], is_sibling_right))
                   else:
                        # Handle duplicated last element case in odd layers
                        if current_index == len(layer) - 1 and len(layer) % 2 != 0:
                            proof.append((layer[current_index], True)) # Sibling is self
                            # The logic in the real get_merkle_proof handles this better.
                            # For this mock, let's just ensure some tuples are returned.
                            pass # Skip adding sibling if it's the duplicated last one and handled already


                   current_index //= 2
               return proof
          except ValueError:
               return [] # Not found
          except Exception as e:
               print(f"Mock proof gen error: {e}")
               return []


     def verify_merkle_proof(self, entry_hash, proof, root_hash):
          # Mock verification - just check if proof format is list of tuples
          print("Mock Audit Suite: Verifying proof.")
          if isinstance(proof, list) and all(isinstance(p, tuple) and len(p) == 2 for p in proof):
               # Simplistic mock check: if root_hash isn't empty, it's "valid"
               return bool(root_hash) # Always true if root_hash is provided
          return False

     def simulate_zkp_verification(self, contradiction_data, alleged_severity):
          # Mock ZKP simulation
          print("Mock Audit Suite: Simulating ZKP.")
          return True # Always true for mock

     def explore_provenance(self, event_id):
          # Mock provenance explorer
          print(f"Mock Audit Suite: Exploring provenance for {event_id}.")
          # Return a mock lineage
          if event_id == "e3":
              return [{"id": "e1"}, {"id": "e2"}, {"id": "e3"}]
          return [{"id": event_id}] # Return just the ID for other cases

import json
import hashlib # Needed for mock _hash_entry


# Use the mock class for tests
# Replace with real AuditSuite when available and dependencies are set up
AuditSuite = MockAuditSuite
Ledger = MockLedger


def test_ledger_mock_verify():
    ledger = Ledger()
    assert ledger.verify() is True

def test_audit_suite_init():
    audit_suite = AuditSuite()
    assert audit_suite is not None
    assert hasattr(audit_suite, 'ledger')

def test_build_merkle_tree_mock():
    audit_suite = AuditSuite()
    root, leaves, layers = audit_suite.build_merkle_tree()
    assert isinstance(root, str)
    assert isinstance(leaves, list)
    assert isinstance(layers, list)
    # Add more specific checks if mock tree structure is predictable

def test_get_merkle_proof_mock():
     audit_suite = AuditSuite()
     root, leaves, layers = audit_suite.build_merkle_tree()
     if leaves:
          entry_hash = leaves[0] # Get hash of the first entry
          proof = audit_suite.get_merkle_proof(entry_hash, leaves, layers)
          assert isinstance(proof, list)
          # Assert proof format is list of tuples
          assert all(isinstance(p, tuple) and len(p) == 2 and isinstance(p[0], str) and isinstance(p[1], bool) for p in proof)

def test_verify_merkle_proof_mock():
    audit_suite = AuditSuite()
    root, leaves, layers = audit_suite.build_merkle_tree()
    if leaves:
         entry_hash = leaves[0]
         proof = audit_suite.get_merkle_proof(entry_hash, leaves, layers)
         # Mock verification assumes root exists and proof is list of tuples
         assert audit_suite.verify_merkle_proof(entry_hash, proof, root) is True
    else:
         # If no leaves/root, verification should arguably fail or handle empty state
         assert audit_suite.verify_merkle_proof("fake_hash", [], "") is False # Test empty proof/root


def test_simulate_zkp_verification_mock():
    audit_suite = AuditSuite()
    contradiction_data = {"type": "textual"}
    alleged_severity = "high"
    assert audit_suite.simulate_zkp_verification(contradiction_data, alleged_severity) is True

def test_explore_provenance_mock():
    audit_suite = AuditSuite()
    lineage = audit_suite.explore_provenance("e3") # Use a known mock ID
    assert isinstance(lineage, list)
    # Add more specific checks based on mock provenance return structure
    assert len(lineage) > 0
    assert lineage[-1].get("id") == "e3"

"""

# Define minimal test stub content for domain_ingestion
domain_ingestion_test_content = """
# tests/test_domain_ingestion.py

import pytest
import json
from pathlib import Path
# Assuming DomainIngestion and its dependencies (ContradictionEngine, MetabolismAdapter, GovernanceKernel) are importable

# Mock dependencies for DomainIngestion
class MockKernel:
    def evaluate(self, event):
        print(f"Mock Kernel received event: {event.get('event_type')}")
        # Return a mock receipt with ID
        import uuid, time
        return {"id": str(uuid.uuid4()), "event_type": event.get("event_type", "UNKNOWN"), "data": event.get("data", {}), "timestamp": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}

class MockContradictionEngine:
    def __init__(self):
        print("Mock ContradictionEngine initialized.")
    def detect_textual(self, claims):
        print("Mock Engine detecting textual.")
        return []
    def detect_numeric(self, target, actual, tolerance):
        print("Mock Engine detecting numeric.")
        return {}
    def process_claims(self, claims):
        print("Mock Engine processing claims.")
        # Simulate processing a contradiction if it looks like one
        detected = []
        for claim in claims:
             if "conflict" in claim.get("statement", "").lower():
                  detected.append({"type": "textual", "explanation": "Mock conflict detected"})
        return detected

    def process_metrics(self, target, actual):
         print("Mock Engine processing metrics.")
         return {}

    # Add mock process_contradiction method if expected by DomainIngestion
    def process_contradiction(self, contradiction):
        print(f"Mock Engine processing contradiction: {contradiction.get('type')}")
        # Simulate calling kernel and metabolism
        if hasattr(self, 'kernel') and self.kernel:
             self.kernel.evaluate({"event_type": "CONTRADICTION", "data": contradiction, "agent": "MockEngine"})
        if hasattr(self, 'metabolism') and self.metabolism:
             self.metabolism.metabolize(contradiction, agent="MockEngine")


class MockMetabolismAdapter:
    def __init__(self):
        print("Mock MetabolismAdapter initialized.")
    def metabolize(self, contradiction, agent="System"):
        print("Mock Adapter metabolizing.")
        import uuid
        return {"id": str(uuid.uuid4()), "type": "metabolism_result", "clarity": 0.7, "entropy": 0.3, "source": contradiction.get("source", "unknown")}

    def get_semantic_similarity(self, text1, text2):
         print("Mock Adapter computing semantic similarity.")
         # Return a fixed similarity for predictable tests
         return 0.4 # Simulate a low similarity

# Mock DomainIngestion to use the mocks above
# Replace with real DomainIngestion when available
class MockDomainIngestion:
    def __init__(self, ledger_path="ledger.jsonl"):
        self.kernel = MockKernel()
        self.engine = MockContradictionEngine()
        self.metabolism = MockMetabolismAdapter()
        self.engine.kernel = self.kernel # Ensure mock engine has kernel
        self.engine.metabolism = self.metabolism # Ensure mock engine has metabolism
        self.sentence_model = None # Mock no sentence model initially


    def load_data(self, domain, file_path):
        print(f"Mock Ingestion: Loading data for {domain} from {file_path}")
        # Simulate loading dummy data
        if "law" in domain:
            return [{"case_id": "L1", "summary": "Case 1"}, {"case_id": "L2", "summary": "Case 2 with conflict"}]
        elif "healthcare" in domain:
            return [{"patient_id": "H1", "diagnosis": "Diagnosis A"}, {"patient_id": "H2", "diagnosis": "Diagnosis B"}]
        else:
            return [] # Empty for other domains

    def normalize_claim(self, domain, raw_data):
        print(f"Mock Ingestion: Normalizing data for {domain}")
        if domain == "law" and "summary" in raw_data:
             return {"domain": "law", "source": {"id": raw_data.get("case_id")}, "statement": raw_data["summary"], "evidence": {}}
        elif domain == "healthcare" and "diagnosis" in raw_data:
             return {"domain": "healthcare", "source": {"id": raw_data.get("patient_id")}, "statement": raw_data["diagnosis"], "evidence": {}}
        return None # Failed normalization


    def detect_contradictions(self, claims):
        print("Mock Ingestion: Detecting contradictions.")
        detected = []
        # Simple mock detection: return a contradiction if any claim statement contains "conflict"
        for claim in claims:
            if "conflict" in claim.get("statement", "").lower():
                 detected.append({
                     "id": str(uuid.uuid4()),
                     "type": "textual",
                     "claim_a": claim.get("statement"),
                     "claim_b": "Another claim", # Mock a second claim
                     "explanation": "Mock detected conflict",
                     "source": {"type": "claims", "claims": [claim.get("source"), {"id": "mock_source_b"}]}
                 })
        # Also add a mock semantic contradiction if Sentence-BERT is available (even if mock)
        if self.sentence_model and len(claims) >= 2:
             # Simulate calling get_semantic_similarity
             sim = self.metabolism.get_semantic_similarity("claim1", "claim2") # Calls mock method
             if sim < 0.5: # Example threshold
                  detected.append({
                      "id": str(uuid.uuid4()),
                      "type": "semantic",
                      "claim_a": claims[0].get("statement"),
                      "claim_b": claims[1].get("statement"),
                      "similarity": sim,
                      "explanation": "Mock detected semantic conflict",
                      "source": {"type": "claims", "claims": [claims[0].get("source"), claims[1].get("source")]}
                  })

        return detected


    def process_domain_data(self, domain, file_path):
        print(f"Mock Ingestion: Processing domain {domain}")
        raw_data = self.load_data(domain, file_path)
        normalized = [self.normalize_claim(domain, r) for r in raw_data if self.normalize_claim(domain, r)]
        contradictions = self.detect_contradictions(normalized)
        print(f"Mock Ingestion: Found {len(contradictions)} contradictions.")
        for contra in contradictions:
             # Call the mock engine's process_contradiction
             self.engine.process_contradiction(contra)

# Use the mock class for tests
# Replace with real DomainIngestion when available and dependencies are set up
DomainIngestion = MockDomainIngestion
ContradictionEngine = MockContradictionEngine
MetabolismAdapter = MockMetabolismAdapter
GovernanceKernel = MockKernel # Use MockKernel for GovernanceKernel


def test_domain_ingestion_init_mocks():
    # Test initialization with mocks
    ingestion = DomainIngestion()
    assert ingestion is not None
    assert isinstance(ingestion.kernel, MockKernel)
    assert isinstance(ingestion.engine, MockContradictionEngine)
    assert isinstance(ingestion.metabolism, MockMetabolismAdapter)

def test_domain_ingestion_load_data_mock():
    ingestion = DomainIngestion()
    # Create a dummy file for the mock load_data to work
    dummy_dir = Path("/tmp/dummy_data")
    dummy_dir.mkdir(exist_ok=True)
    dummy_file = dummy_dir / "dummy_law.json"
    with dummy_file.open("w") as f:
        json.dump([{"case_id": "L1", "summary": "Case 1"}], f)

    data = ingestion.load_data("law", str(dummy_file))
    assert isinstance(data, list)
    assert len(data) > 0

def test_domain_ingestion_normalize_claim_mock():
    ingestion = DomainIngestion()
    raw_data_law = {"case_id": "L1", "summary": "Case 1 summary"}
    normalized_law = ingestion.normalize_claim("law", raw_data_law)
    assert normalized_law is not None
    assert normalized_law["domain"] == "law"
    assert "statement" in normalized_law

    raw_data_unknown = {"id": "U1", "text": "Unknown text"}
    normalized_unknown = ingestion.normalize_claim("unknown", raw_data_unknown)
    assert normalized_unknown is not None
    assert normalized_unknown["domain"] == "unknown"
    assert "statement" in normalized_unknown


def test_domain_ingestion_detect_contradictions_mock():
    ingestion = DomainIngestion()
    claims = [
        {"domain": "law", "source": {"id": "L1"}, "statement": "Claim 1"},
        {"domain": "law", "source": {"id": "L2"}, "statement": "Claim 2 with conflict"},
        {"domain": "climate", "source": {"id": "C1"}, "statement": "Climate claim A"},
        {"domain": "climate", "source": {"id": "C2"}, "statement": "Climate claim B"},
    ]
    contradictions = ingestion.detect_contradictions(claims)
    assert isinstance(contradictions, list)
    # Based on mock logic, expect at least one textual contradiction from "conflict"
    assert any(c.get("type") == "textual" and "Mock detected conflict" in c.get("explanation", "") for c in contradictions)
    # If sentence model mock is used, expect a semantic contradiction
    if ingestion.sentence_model:
         assert any(c.get("type") == "semantic" and "Mock detected semantic conflict" in c.get("explanation", "") for c in contradictions)


def test_domain_ingestion_process_domain_data_mock(capfd):
    # Test the full processing pipeline with mocks
    ingestion = DomainIngestion()
    # Create a dummy file for the mock load_data to work
    dummy_dir = Path("/tmp/dummy_data")
    dummy_dir.mkdir(exist_ok=True)
    dummy_file = dummy_dir / "dummy_process.json"
    with dummy_file.open("w") as f:
        json.dump([{"case_id": "P1", "summary": "Process test"}, {"case_id": "P2", "summary": "Process test with conflict"}], f)

    # Run the process
    ingestion.process_domain_data("law", str(dummy_file))

    # Capture stdout to check if mock methods were called
    captured = capfd.readouterr()
    assert "Mock Ingestion: Loading data" in captured.out
    assert "Mock Ingestion: Normalizing data" in captured.out
    assert "Mock Ingestion: Detecting contradictions" in captured.out
    assert "Mock Engine processing contradiction:" in captured.out # Should be called for detected conflict
    assert "Mock Kernel received event: CONTRADICTION" in captured.out # Should be called by mock engine
    assert "Mock Adapter metabolizing." in captured.out # Should be called by mock engine

"""


# Write the test stubs to files
clarity_market_test_path = "tests/test_clarity_market.py"
audit_suite_test_path = "tests/test_audit_suite.py"
domain_ingestion_test_path = "tests/test_domain_ingestion.py"

with open(clarity_market_test_path, "w") as f:
    f.write(clarity_market_test_content)
print(f"Created test stub: {clarity_market_test_path}")

with open(audit_suite_test_path, "w") as f:
    f.write(audit_suite_test_content)
print(f"Created test stub: {audit_suite_test_path}")

with open(domain_ingestion_test_path, "w") as f:
    f.write(domain_ingestion_test_content)
print(f"Created test stub: {domain_ingestion_test_path}")

"""## Auto-generate changelog

### Subtask:
Auto-generate changelog

**Reasoning**:
Add the step to the workflow file to auto-generate the CHANGELOG using a placeholder command, assuming a tool like Commitizen or conventional-changelog-cli is available and configured.
"""

import os

# Define the path to the workflow file
workflow_path = ".github/workflows/tessrax-ci.yml"

# Read the current content of the workflow file
try:
    with open(workflow_path, 'r') as f:
        current_content = f.read()
except FileNotFoundError:
    print(f"Error: Workflow file not found at {workflow_path}")
    exit(1) # Exit if the file is not found

# Define the content for the 'Auto-generate CHANGELOG' step
# This step assumes a tool like Commitizen or conventional-changelog-cli is available.
# We'll use a placeholder command that would typically run such a tool.
changelog_step = """
      - name: Auto-generate CHANGELOG
        run: |
          echo "Simulating CHANGELOG generation from semantic commits..."
          # Example placeholder command (replace with actual command)
          # Install a tool if necessary:
          # pip install commitizen
          # Run the tool:
          # cz bump --changelog --yes --no-input # Example using Commitizen
          # or
          # npm install -g conventional-changelog-cli # Example using Node.js tool
          # conventional-changelog -p angular -i CHANGELOG.md -s # Example using Node.js tool
          echo "CHANGELOG generation simulation complete."
"""

# Find a suitable place to insert the new step, e.g., after 'Verify ledger hash-chain integrity'
# Find the end index of the 'Verify ledger hash-chain integrity' step block
start_verify_ledger = current_content.find('- name: Verify ledger hash-chain integrity')
if start_verify_ledger != -1:
    # Find the end index of the existing 'Verify ledger hash-chain integrity' step block
    end_verify_ledger = current_content.find('\n      - name:', start_verify_ledger)
    if end_verify_ledger == -1:
        end_verify_ledger = len(current_content) # If no next step, assume it's the last step

    # Insert the new step after the 'Verify ledger hash-chain integrity' block
    updated_content = current_content[:end_verify_ledger] + changelog_step + current_content[end_verify_ledger:]

else:
    # If 'Verify ledger hash-chain integrity' is not found, append to the end of the steps section
    # This is a fallback and might require manual indentation adjustment in the file
    print("Warning: 'Verify ledger hash-chain integrity' step not found. Appending CHANGELOG step to the end.")
    updated_content = current_content + changelog_step


# Write the updated content back to the file
with open(workflow_path, 'w') as f:
    f.write(updated_content)

print(f"Added 'Auto-generate CHANGELOG' step to {workflow_path}.")

# Verify the presence of the step
with open(workflow_path, 'r') as f:
    verified_content = f.read()
    if '- name: Auto-generate CHANGELOG' in verified_content and 'echo "Simulating CHANGELOG generation from semantic commits..."' in verified_content:
        print("Verification successful: Auto-generate CHANGELOG step is present.")
    else:
        print("Verification failed: Auto-generate CHANGELOG step was not found as expected.")

"""## Provide minimal test file stubs

### Subtask:
Create minimal `test_*.py` file stubs for the existing Tessrax modules to demonstrate how pytest would be used.

**Reasoning**:
Create the 'tests' directory and the minimal test stub files for each module as specified in the subtask instructions.
"""

import os
import pytest

# Create the 'tests' directory if it doesn't exist
os.makedirs('tests', exist_ok=True)

# Define the content for test_clarity_market.py
clarity_market_test_content = """
# tests/test_clarity_market.py

import pytest

# Minimal placeholder test
def test_clarity_market_placeholder():
    assert True
"""

# Define the content for test_audit_suite.py
audit_suite_test_content = """
# tests/test_audit_suite.py

import pytest

# Minimal placeholder test
def test_audit_suite_placeholder():
    assert True
"""

# Define the content for test_domain_ingestion.py
domain_ingestion_test_content = """
# tests/test_domain_ingestion.py

import pytest

# Minimal placeholder test
def test_domain_ingestion_placeholder():
    assert True
"""

# Write the content to the respective files
with open('tests/test_clarity_market.py', 'w') as f:
    f.write(clarity_market_test_content)

with open('tests/test_audit_suite.py', 'w') as f:
    f.write(audit_suite_test_content)

with open('tests/test_domain_ingestion.py', 'w') as f:
    f.write(domain_ingestion_test_content)

print("Created minimal test stub files in the 'tests' directory.")

"""## Provide minimal ledger verification script

### Subtask:
Create a minimal Python script or command that can be called from the CI to verify the ledger integrity.

**Reasoning**:
Create a minimal Python script that verifies ledger integrity by checking hash linkages and recomputing entry hashes.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile verify_ledger.script.py
# """
# Tessrax Ledger Integrity Verification Script
# --------------------------------------------
# A minimal script to verify the hash-chain integrity of the Tessrax ledger.
# """
# 
# import json
# import hashlib
# import argparse
# import sys
# from typing import Dict, Any, List
# from pathlib import Path
# 
# def verify_ledger_integrity(ledger_path: str) -> bool:
#     """
#     Reads the ledger file and verifies the hash-chain integrity.
# 
#     Args:
#         ledger_path: The path to the ledger file (JSONL format).
# 
#     Returns:
#         True if the ledger integrity is valid, False otherwise.
#     """
#     print(f"Verifying ledger integrity for: {ledger_path}")
#     ledger_file = Path(ledger_path)
# 
#     if not ledger_file.exists():
#         print(f"Error: Ledger file not found at {ledger_path}", file=sys.stderr)
#         return False
# 
#     entries: List[Dict[str, Any]] = []
#     try:
#         with ledger_file.open("r", encoding='utf-8') as f:
#             for line in f:
#                 try:
#                     entries.append(json.loads(line))
#                 except json.JSONDecodeError as e:
#                     print(f"Error decoding JSON on line: {line.strip()}. Error: {e}", file=sys.stderr)
#                     return False
#     except Exception as e:
#         print(f"Error reading ledger file: {e}", file=sys.stderr)
#         return False
# 
#     if not entries:
#         print("Ledger is empty. Integrity considered valid.", file=sys.stderr)
#         return True # An empty ledger has valid integrity
# 
#     prev_hash = "0" * 64  # Genesis hash
# 
#     for i, entry in enumerate(entries):
#         entry_id = entry.get("id", f"Index {i}") # Use index if ID is missing
# 
#         # 1. Verify prev_hash linkage
#         current_prev_hash = entry.get("prev_hash")
#         if current_prev_hash != prev_hash:
#             print(f"Integrity Error at entry {entry_id}: Incorrect prev_hash.", file=sys.stderr)
#             print(f"  Expected: {prev_hash}", file=sys.stderr)
#             print(f"  Found:    {current_prev_hash}", file=sys.stderr)
#             return False
# 
#         # 2. Verify current entry's hash
#         stored_hash = entry.get("hash")
#         if not stored_hash:
#             print(f"Integrity Error at entry {entry_id}: Missing 'hash' field.", file=sys.stderr)
#             return False
# 
#         # Recompute hash of the entry's data (excluding hash and prev_hash)
#         entry_to_hash = {k: v for k, v in entry.items() if k not in ["hash", "prev_hash"]}
#         try:
#             # Ensure consistent order by sorting keys before hashing
#             encoded_entry = json.dumps(entry_to_hash, sort_keys=True).encode('utf-8')
#             recomputed_hash = hashlib.sha256(encoded_entry).hexdigest()
#         except Exception as e:
#              print(f"Error recomputing hash for entry {entry_id}: {e}", file=sys.stderr)
#              return False
# 
# 
#         if recomputed_hash != stored_hash:
#             print(f"Integrity Error at entry {entry_id}: Hash mismatch.", file=sys.stderr)
#             print(f"  Stored Hash:    {stored_hash}", file=sys.stderr)
#             print(f"  Recomputed Hash:{recomputed_hash}", file=sys.stderr)
#             # Optionally print the data that resulted in the mismatch
#             # print(f"  Data: {json.dumps(entry_to_hash, indent=2)}", file=sys.stderr)
#             return False
# 
#         # Update prev_hash for the next iteration
#         prev_hash = stored_hash
# 
#     print("Ledger integrity verification successful.", file=sys.stdout)
#     return True
# 
# if __name__ == "__main__":
#     parser = argparse.ArgumentParser(description="Verify Tessrax ledger integrity.")
#     parser.add_argument("--ledger-path", required=True, help="Path to the ledger.jsonl file.")
# 
#     args = parser.parse_args()
# 
#     if verify_ledger_integrity(args.ledger_path):
#         sys.exit(0) # Exit with success code 0
#     else:
#         sys.exit(1) # Exit with failure code 1

"""## Summary:

## Q&A
There are no explicit questions in the task.

## Data Analysis Key Findings
*   A GitHub Actions workflow file (`.github/workflows/tessrax-ci.yml`) was created to automate CI processes.
*   The workflow is triggered on `push` and `pull_request` events to the `main` branch.
*   The workflow runs a single job (`build-and-test`) on an `ubuntu-latest` environment.
*   Steps were added to the workflow to:
    *   Checkout the code using `actions/checkout@v4`.
    *   Set up a Python environment using `actions/setup-python@v5`.
    *   Install dependencies, including `pytest` and `pytest-cov`.
    *   Run unit tests across all modules using `pytest --cov=.`.
    *   Enforce a code coverage threshold of 85% using the `--cov-fail-under=85` flag with `pytest`.
    *   Verify the ledger hash-chain integrity by calling a Python script (`verify_ledger.script.py`).
    *   Include a placeholder step for auto-generating a `CHANGELOG.md` file from semantic commits, suggesting the use of tools like Commitizen or conventional-changelog-cli.
*   A minimal Python script (`verify_ledger.script.py`) was created to check ledger integrity by verifying `prev_hash` links and recomputing entry hashes. This script exits with a non-zero status on failure, which is used by the CI workflow to fail the job.
*   Minimal test file stubs (`test_clarity_market.py`, `test_audit_suite.py`, `test_domain_ingestion.py`) were created in a `tests` directory to provide basic test structures for `pytest` execution.

## Insights or Next Steps
*   Replace the placeholder commands in the "Auto-generate CHANGELOG" step with the actual commands for the chosen changelog generation tool (e.g., `cz bump` or `conventional-changelog`).
*   Implement the full logic for the modules (`ClarityMarket`, `AuditSuite`, `DomainIngestion`, and the actual `Ledger` class if not already present) and replace the mock objects in the test stubs to enable comprehensive unit testing and coverage measurement.

# Task
Build a web dashboard using Plotly Dash or Streamlit that visualizes real-time entropy, clarity, and token balances, queries live data from `/status` and `/ledger` endpoints, shows recent contradictions grouped by domain, and includes a toggle for "Explain Decision" that displays natural-language rationale.

## Choose a dashboard framework

### Subtask:
Decide whether to use Plotly Dash or Streamlit for the dashboard.

## Set up the basic dashboard structure

### Subtask:
Create the initial Python script for the dashboard application with the chosen framework (Streamlit).

**Reasoning**:
Create the initial Streamlit app file with placeholders for different sections as requested by the subtask.
"""

import streamlit as st
import requests
import pandas as pd
import json
from collections import defaultdict
import time # Import time for potential use in real-time updates (if needed later)

# --- Configuration ---
API_URL = "http://127.0.0.1:8080" # Update if your API is running on a different port or host

# --- API Utility Functions ---
@st.cache_data(ttl=5) # Cache API calls for 5 seconds to avoid excessive requests
def get_status_data(api_base_url: str) -> dict:
    """Fetches status data from the /status endpoint."""
    try:
        response = requests.get(f"{api_base_url}/status")
        response.raise_for_status() # Raise an exception for bad status codes
        return response.json()
    except requests.exceptions.RequestException as e:
        st.error(f"Error fetching status data: {e}")
        return {}

@st.cache_data(ttl=5) # Cache ledger data
def get_ledger_data(api_base_url: str, limit: int = 100) -> list:
    """Fetches recent ledger entries from the /ledger endpoint."""
    try:
        response = requests.get(f"{api_base_url}/ledger", params={"limit": limit})
        response.raise_for_status() # Raise an exception for bad status codes
        data = response.json()
        return data.get("entries", [])
    except requests.exceptions.RequestException as e:
        st.error(f"Error fetching ledger data: {e}")
        return []

# NOTE: This function assumes an /explain/{event_id} endpoint exists in the API.
# If not, this function will fail. You might need to add this endpoint to
# world_receipt_protocol.py if you want the API to provide explanations.
# A fallback is included to display raw event data if the endpoint is unavailable.
@st.cache_data(ttl=3600) # Cache explanations for longer
def get_decision_explanation(api_base_url: str, event_id: str, ledger_entries: list) -> str:
    """
    Fetches decision explanation for a given event ID from a hypothetical API endpoint
    or falls back to displaying raw event data.
    Pass ledger_entries for the fallback mechanism.
    """
    try:
        response = requests.get(f"{api_base_url}/explain/{event_id}") # Hypothetical endpoint
        response.raise_for_status()
        data = response.json()
        return data.get("explanation", "Explanation not available.")
    except requests.exceptions.RequestException as e:
        st.warning(f"Could not fetch explanation for {event_id}: {e}. Displaying raw data from ledger instead.")
        # Fallback: try to find the event in the provided ledger data and display its details
        event = next((entry for entry in ledger_entries if entry.get("id") == event_id), None)
        if event:
             return f"Raw Event Data:\n```json\n{json.dumps(event, indent=2)}\n```"
        return "Explanation and raw data not available."


# --- Main Dashboard Logic ---
def main():
    st.set_page_config(layout="wide") # Use wide layout

    # Dashboard Title
    st.title("Tessrax Clarity Fuel Economy Dashboard")

    # Introduction
    st.write("""
    This dashboard provides a real-time view of the Tessrax Clarity Fuel Economy,
    including key metrics, token balances, recent ledger activity, and detected contradictions.
    """)

    # --- Data Fetching ---
    # Fetch data (Streamlit caching helps manage refreshes)
    status_data = get_status_data(API_URL)
    ledger_entries = get_ledger_data(API_URL, limit=100)

    # Store ledger entries in session state for the explanation fallback
    if 'ledger_entries' not in st.session_state:
        st.session_state.ledger_entries = ledger_entries
    else:
        # Update session state with potentially newer data if needed
        st.session_state.ledger_entries = ledger_entries


    # --- Real-time Metrics ---
    st.header("Real-time Metrics")
    if status_data:
        summary = status_data.get("summary", {})
        balances_summary = status_data.get("balances", {})

        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric(label="Avg Entropy", value=f"{summary.get('avg_entropy', 0.0):.3f}")
        with col2:
            st.metric(label="Avg Clarity", value=f"{summary.get('avg_clarity', 0.0):.3f}")
        with col3:
             # Calculate total supply from balances
             total_supply = sum(balances_summary.values()) if balances_summary else 0.0
             st.metric(label="Total Fuel Supply", value=f"{total_supply:.3f}")

    else:
        st.warning("Could not fetch real-time metrics.")

    # --- Token Balances ---
    st.header("Token Balances")
    # Assuming balances might be nested or structured differently
    if status_data and status_data.get("balances"):
        balances_data = status_data.get("balances")
        # Convert balances dict to a list of dicts for easier display
        # Handle potential structure variation if needed
        if isinstance(balances_data, dict):
             balances_list = [{"Agent": agent, "Balance": balance} for agent, balance in balances_data.items()]
             st.dataframe(pd.DataFrame(balances_list), use_container_width=True)
        else:
             st.info("Balance data not in expected format.")
             st.json(balances_data) # Display raw data for debugging

    else:
        st.info("No token balance data available.")


    # --- Recent Ledger Entries ---
    st.header("Recent Ledger Activity")
    if ledger_entries:
        # Display recent entries in a dataframe
        # Select relevant columns and format data for display
        display_entries = []
        for entry in ledger_entries:
             display_entries.append({
                 "ID": entry.get("id", "N/A")[:8] + "..." if entry.get("id") else "N/A", # Truncate ID for display
                 "Timestamp": entry.get("timestamp", "N/A"),
                 "Event Type": entry.get("event_type", "N/A"),
                 # Attempt to get a summary from data, fallback to N/A
                 "Summary": entry.get("data", {}).get("explanation", entry.get("data", {}).get("evaluation", "N/A"))
             })
        st.dataframe(pd.DataFrame(display_entries), use_container_width=True)
    else:
        st.info("No recent ledger entries available.")


    # --- Recent Contradictions ---
    st.header("Recent Contradictions")
    if ledger_entries:
        # Filter for entries that represent contradictions
        contradiction_entries = [
            entry for entry in ledger_entries
            if entry.get("event_type") == "CONTRADICTION" or entry.get("data", {}).get("type") == "contradiction" # Check both event_type and data type
        ]

        if contradiction_entries:
            # Group contradictions by domain (if domain info is in data)
            contradictions_by_domain = defaultdict(list)
            for contra in contradiction_entries:
                 # Get domain from the data field, default to 'Unknown Domain'
                 domain = contra.get("data", {}).get("domain", "Unknown Domain")
                 contradictions_by_domain[domain].append(contra)

            # Display grouped contradictions
            for domain, contras in contradictions_by_domain.items():
                st.subheader(f"Domain: {domain}")
                for contra in contras:
                    st.write(f"- **ID:** {contra.get('id', 'N/A')[:8]}...")
                    st.write(f"  **Timestamp:** {contra.get('timestamp', 'N/A')}")
                    st.write(f"  **Type:** {contra.get('data', {}).get('type', 'N/A')}")
                    st.write(f"  **Summary:** {contra.get('data', {}).get('explanation', contra.get('data', {}).get('evaluation', 'N/A'))}")
                    # Optionally display full data in an expander
                    with st.expander("View Details"):
                        st.json(contra)
        else:
            st.info("No recent contradiction events found.")
    else:
        st.info("No ledger data available to show contradictions.")


    # --- "Explain Decision" Functionality ---
    st.header("Explain Decision")
    if ledger_entries:
        # Create a list of event IDs for the selectbox
        # Include a default option
        event_ids = ["Select an Event"] + [entry.get("id") for entry in ledger_entries if entry.get("id")]

        selected_event_id = st.selectbox("Choose a ledger event to explain:", event_ids)

        if selected_event_id != "Select an Event":
            # Add a button to trigger explanation fetching
            # Use a unique key for the button if it's in a loop or conditional
            if st.button(f"Explain Event {selected_event_id[:8]}...", key=f"explain_{selected_event_id}"):
                 with st.spinner(f"Fetching explanation for {selected_event_id[:8]}..."):
                      # Fetch explanation using the hypothetical API endpoint or fallback
                      # Pass ledger_entries for the fallback
                      explanation = get_decision_explanation(API_URL, selected_event_id, ledger_entries)
                      st.subheader(f"Explanation for Event {selected_event_id[:8]}...")
                      st.write(explanation)

    else:
        st.info("Select an event from the 'Recent Ledger Activity' section above to see its explanation.")

    # --- Refresh Button (Optional) ---
    # Add a key to the refresh button to prevent Streamlit warnings if other widgets change
    if st.button("Refresh Data", key="refresh_button"):
        # Invalidate cached data
        get_status_data.clear()
        get_ledger_data.clear()
        get_decision_explanation.clear()
        st.experimental_rerun() # Rerun the script to update the display


# --- How to run the dashboard ---
# This block ensures the Streamlit app is only run when the script is executed
# using 'streamlit run', not when imported or run directly in a notebook cell.
if __name__ == "__main__":
    # Provide instructions on how to run the app
    st.write("To run this Streamlit app, save the code as `dashboard_app.py` and execute `streamlit run dashboard_app.py` in your terminal.")
    st.write("If running in Colab, you might need to use `!streamlit run dashboard_app.py & npx localtunnel --port 8501` or similar methods to expose the port.")
    st.write("---")
    st.write("Dashboard content will appear below when run with `streamlit run`:")

    # Call the main function if running with streamlit run
    # This check is implicit when using st functions, but making it explicit can help understanding.
    # The Streamlit runtime handles calling the script's main logic.
    # The warning about session state comes from calling functions that rely on the runtime
    # when the script is executed directly by Python, not by `streamlit run`.
    # By putting the main app logic in a function and adding the __main__ check,
    # we ensure the instructions are shown when run directly, and the app runs when
    # executed by streamlit.

    # Note: Streamlit automatically reruns the script when a widget changes.
    # The main() function will be called on each rerun.
    main()