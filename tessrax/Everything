```python
# tessrax/core/conflict_graph.py
# Tessrax Conflict Graph Prototype v1.0
# Author: Joshua Vetos / Claude (Anthropic)
# License: CC BY 4.0

"""
Builds a weighted contradiction graph from natural-language statements.
Each node = a statement; each edge = degree of contradiction.
Output bundles are hashed and anchored via the Tessrax ContradictionLedger.
"""

import re
import json
import time
import hashlib
import datetime
import numpy as np
import networkx as nx
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from sentence_transformers import SentenceTransformer, util

from tessrax.core.contradiction_ledger import ContradictionLedger


# ----------------------------------------------------------
# Utility: lightweight confidence estimator
# ----------------------------------------------------------
def estimate_confidence(text: str) -> float:
    """Heuristic confidence: longer factual statements score higher."""
    length_score = min(len(text) / 200, 1.0)
    numeric_bonus = 0.1 if re.search(r"\d", text) else 0.0
    modal_penalty = 0.1 if re.search(r"\b(may|might|could|possibly)\b", text.lower()) else 0.0
    return max(0.1, min(1.0, length_score + numeric_bonus - modal_penalty))


# ----------------------------------------------------------
# Core contradiction metrics
# ----------------------------------------------------------
def logical_contradiction(a: str, b: str) -> float:
    """Simple negation detection."""
    a_low, b_low = a.lower(), b.lower()

    # Check for explicit negation patterns
    a_negated = "not" in a_low or "no " in a_low or "never" in a_low
    b_negated = "not" in b_low or "no " in b_low or "never" in b_low

    # Extract core content
    a_clean = re.sub(r"\b(not|no|never)\b", "", a_low).strip()
    b_clean = re.sub(r"\b(not|no|never)\b", "", b_low).strip()

    # High score if one is negated and core content matches
    if a_negated != b_negated and a_clean == b_clean:
        return 1.0

    # Partial match for similar but differently negated statements
    if a_negated != b_negated:
        from difflib import SequenceMatcher
        similarity = SequenceMatcher(None, a_clean, b_clean).ratio()
        if similarity > 0.7:
            return similarity

    return 0.0


def numeric_inconsistency(a: str, b: str) -> float:
    """Detect quantitative disagreement (percentage/number)."""
    # Extract numbers with optional % sign
    nums_a = [float(x.strip("%")) for x in re.findall(r"\d+\.?\d*%?", a)]
    nums_b = [float(x.strip("%")) for x in re.findall(r"\d+\.?\d*%?", b)]

    if not nums_a or not nums_b:
        return 0.0

    # Compare corresponding numbers (take first for simplicity)
    diff = abs(nums_a[0] - nums_b[0])
    ref = max(1e-5, max(nums_a[0], nums_b[0]))
    score = diff / ref

    # Cap at 1.0 and scale: small differences get lower scores
    return min(score, 1.0) if score > 0.1 else 0.0


def temporal_contradiction(a: str, b: str) -> float:
    """Flag reversed or inconsistent temporal claims."""
    # Extract years
    years_a = [int(y) for y in re.findall(r"\b(19|20)\d{2}\b", a)]
    years_b = [int(y) for y in re.findall(r"\b(19|20)\d{2}\b", b)]

    if not years_a or not years_b:
        return 0.0

    # Check for temporal ordering issues
    # e.g., "by 2030" vs "in 2024 only 5% done" implies likely contradiction
    all_years = years_a + years_b

    # Look for "by" vs actual achievement patterns
    by_pattern_a = bool(re.search(r"\bby\s+\d{4}", a))
    by_pattern_b = bool(re.search(r"\bby\s+\d{4}", b))

    # If one states a future goal and another states past underperformance
    if by_pattern_a and not by_pattern_b:
        if max(years_a) > max(years_b):  # Goal year is later than report year
            return 0.5  # Potential contradiction (underperformance)

    return 0.0


def categorical_mismatch(a: str, b: str) -> float:
    """Simple categorical clash detection using keyword domains."""
    # Define domain keywords
    domains = {
        "environmental": ["emission", "carbon", "co2", "climate", "renewable", "pollution"],
        "financial": ["profit", "revenue", "loss", "earnings", "dividend"],
        "social": ["diversity", "equality", "workforce", "community"],
        "operational": ["production", "capacity", "efficiency", "output"]
    }

    # Detect which domains each statement belongs to
    def get_domains(text):
        text_lower = text.lower()
        return {name for name, keywords in domains.items()
                if any(k in text_lower for k in keywords)}

    domains_a = get_domains(a)
    domains_b = get_domains(b)

    # No mismatch if statements share domains
    if domains_a & domains_b:
        return 0.0

    # Potential contradiction if discussing completely different categories
    # but appearing to contradict (this is context-dependent)
    return 0.3 if domains_a and domains_b else 0.0


# ----------------------------------------------------------
# Contradiction Graph Builder
# ----------------------------------------------------------
class ConflictGraph:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2", threshold: float = 0.5):
        """
        Initialize the conflict graph.

        Args:
            model_name: Sentence transformer model for embeddings
            threshold: Minimum contradiction score to create an edge (0.5 more sensitive than 0.7)
        """
        self.model = SentenceTransformer(model_name)
        self.threshold = threshold
        self.graph = nx.Graph()
        self.ledger = ContradictionLedger(Path("data/conflict_graph_ledger.db"))
        self.metadata = {"created": datetime.datetime.utcnow().isoformat()}

    # ------------------------------------------------------
    def add_statements(self, statements: List[Dict[str, Any]]):
        """
        Add statements to the graph.
        Each statement dict should contain: {"text": "...", "source": "...", "timestamp": "..."}.
        Confidence is auto-estimated if not provided.
        """
        for s in statements:
            if "text" not in s:
                raise ValueError("Each statement must have a 'text' field")

            # Auto-fill missing fields
            s.setdefault("confidence", estimate_confidence(s["text"]))
            s.setdefault("source", "unknown")
            s.setdefault("timestamp", datetime.datetime.utcnow().isoformat())

            self.graph.add_node(s["text"], **s)

    # ------------------------------------------------------
    def compute_edges(self, verbose: bool = False):
        """
        Compute contradiction edges between all statement pairs.
        Uses semantic similarity to filter, then calculates contradiction metrics.
        """
        texts = list(self.graph.nodes())
        n = len(texts)

        if n < 2:
            if verbose:
                print("Need at least 2 statements to compute contradictions")
            return

        if verbose:
            print(f"Computing embeddings for {n} statements...")

        embeddings = self.model.encode(texts, convert_to_tensor=True, show_progress_bar=verbose)
        cos_sim = util.cos_sim(embeddings, embeddings).cpu().numpy()

        edges_added = 0
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                sim = float(cos_sim[i][j])

                # Skip pairs that are too dissimilar (unrelated topics)
                if sim < 0.3:
                    continue

                # Compute individual contradiction metrics
                L = logical_contradiction(texts[i], texts[j])
                N = numeric_inconsistency(texts[i], texts[j])
                T = temporal_contradiction(texts[i], texts[j])
                C = categorical_mismatch(texts[i], texts[j])

                # Get confidence scores
                conf_i = self.graph.nodes[texts[i]]["confidence"]
                conf_j = self.graph.nodes[texts[j]]["confidence"]
                avg_confidence = (conf_i + conf_j) / 2

                # Weighted contradiction score
                # Higher weights for logical and numeric contradictions
                contradiction_score = (
                    0.4 * L +  # Logical negation
                    0.3 * N +  # Numerical mismatch
                    0.2 * T +  # Temporal inconsistency
                    0.1 * C    # Categorical clash
                ) * sim * avg_confidence

                if contradiction_score >= self.threshold:
                    self.graph.add_edge(
                        texts[i],
                        texts[j],
                        weight=round(contradiction_score, 3),
                        sim=round(sim, 3),
                        metrics={"L": round(L, 2), "N": round(N, 2),
                                "T": round(T, 2), "C": round(C, 2)}
                    )
                    edges_added += 1

        if verbose:
            print(f"Added {edges_added} contradiction edges (threshold={self.threshold})")

    # ------------------------------------------------------
    def detect_clusters(self) -> List[List[str]]:
        """
        Find densely connected contradiction clusters.
        Returns list of clusters, each containing contradicting statements.
        """
        # Find connected components (groups of mutually contradicting statements)
        comps = nx.connected_components(self.graph)
        clusters = [list(c) for c in comps if len(c) > 1]
        return clusters

    # ------------------------------------------------------
    def get_worst_offenders(self, top_k: int = 5) -> List[Tuple[str, int]]:
        """Find statements with the most contradictions."""
        degrees = dict(self.graph.degree())
        sorted_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
        return sorted_nodes[:top_k]

    # ------------------------------------------------------
    def export_bundle(self) -> Dict[str, Any]:
        """Export the current graph as a contradiction bundle for the ledger."""
        edges = [
            {
                "s_i": u,
                "s_j": v,
                "score": d["weight"],
                "similarity": d["sim"],
                "breakdown": d["metrics"]
            }
            for u, v, d in self.graph.edges(data=True)
        ]

        clusters = self.detect_clusters()
        worst_offenders = self.get_worst_offenders()

        bundle = {
            "edges": edges,
            "clusters": clusters,
            "worst_offenders": [{"text": text[:100], "contradiction_count": count}
                               for text, count in worst_offenders],
            "stats": {
                "total_statements": self.graph.number_of_nodes(),
                "total_contradictions": self.graph.number_of_edges(),
                "cluster_count": len(clusters)
            },
            "timestamp": datetime.datetime.utcnow().isoformat(),
        }

        bundle_json = json.dumps(bundle, sort_keys=True)
        bundle_hash = hashlib.sha256(bundle_json.encode()).hexdigest()
        bundle["hash"] = "sha256:" + bundle_hash

        return {"contradiction_bundle": bundle}

    # ------------------------------------------------------
    def anchor_to_ledger(self) -> Dict[str, Any]:
        """Anchor the contradiction bundle in the Tessrax ContradictionLedger."""
        bundle = self.export_bundle()["contradiction_bundle"]

        receipt = self.ledger.record_contradiction(
            claim_a={"text": "Graph bundle", "id": f"bundle_{bundle['hash'][:8]}"},
            claim_b={"text": f"{bundle['stats']['total_contradictions']} conflicts", "id": "edges"},
            resolution=f"Graph with {bundle['stats']['cluster_count']} contradiction clusters",
            metadata=bundle,
        )

        return receipt

    # ------------------------------------------------------
    def summary(self, verbose: bool = True):
        """Print summary stats."""
        if not verbose:
            return

        print(f"\n{'='*60}")
        print(f"CONFLICT GRAPH SUMMARY")
        print(f"{'='*60}")
        print(f"Nodes (statements): {self.graph.number_of_nodes()}")
        print(f"Edges (contradictions): {self.graph.number_of_edges()}")
        print(f"Contradiction threshold: {self.threshold}")

        if self.graph.number_of_edges() == 0:
            print("\n✓ No contradictions detected above threshold")
            return

        print(f"\n--- Detected Contradictions ---")
        for u, v, d in self.graph.edges(data=True):
            print(f"\n  Score: {d['weight']:.3f} | Similarity: {d['sim']:.3f}")
            print(f"  A: {u[:80]}...")
            print(f"  B: {v[:80]}...")
            print(f"  Metrics: {d['metrics']}")

        clusters = self.detect_clusters()
        if clusters:
            print(f"\n--- Contradiction Clusters ({len(clusters)}) ---")
            for i, cluster in enumerate(clusters, 1):
                print(f"\n  Cluster {i}: {len(cluster)} statements")
                for stmt in cluster:
                    print(f"    • {stmt[:70]}...")

        worst = self.get_worst_offenders(3)
        if worst:
            print(f"\n--- Most Contradictory Statements ---")
            for stmt, count in worst:
                print(f"  • {count} contradictions: {stmt[:70]}...")

    # ------------------------------------------------------
    def visualize(self, output_path: Optional[Path] = None):
        """
        Generate a visual representation of the contradiction graph.
        Requires matplotlib. Saves to file if output_path provided.
        """
        try:
            import matplotlib.pyplot as plt

            pos = nx.spring_layout(self.graph, k=2, iterations=50)

            plt.figure(figsize=(14, 10))

            # Draw nodes
            nx.draw_networkx_nodes(self.graph, pos, node_size=700,
                                  node_color='lightblue', alpha=0.9)

            # Draw edges with width proportional to contradiction score
            edges = self.graph.edges()
            weights = [self.graph[u][v]['weight'] * 3 for u, v in edges]
            nx.draw_networkx_edges(self.graph, pos, width=weights,
                                  alpha=0.6, edge_color='red')

            # Draw labels (truncated)
            labels = {node: node[:30] + "..." if len(node) > 30 else node
                     for node in self.graph.nodes()}
            nx.draw_networkx_labels(self.graph, pos, labels, font_size=8)

            plt.title("Tessrax Contradiction Graph", fontsize=16, fontweight='bold')
            plt.axis('off')
            plt.tight_layout()

            if output_path:
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                print(f"\n✓ Graph visualization saved to {output_path}")
            else:
                plt.show()

        except ImportError:
            print("⚠ matplotlib not installed. Skipping visualization.")


# ----------------------------------------------------------
# Demo Harness
# ----------------------------------------------------------
if __name__ == "__main__":
    print("Tessrax Conflict Graph - Demo Harness")
    print("="*60)

    sample_texts = [
        {"text": "In 2020, Acme Corp pledged to cut CO2 emissions 50% by 2030.",
         "source": "Press Release", "timestamp": "2020-01-15"},
        {"text": "In 2024, Acme Corp reported emissions down only 5%.",
         "source": "Annual Report", "timestamp": "2024-03-01"},
        {"text": "Acme Corp achieved record revenue growth of 40% in 2024.",
         "source": "Investor Call", "timestamp": "2024-04-10"},
        {"text": "Acme Corp will eliminate all carbon emissions by 2035.",
         "source": "CEO Interview", "timestamp": "2024-06-01"},
        {"text": "Acme Corp has not reduced carbon emissions significantly.",
         "source": "Third-party Audit", "timestamp": "2024-07-15"},
    ]

    cg = ConflictGraph(threshold=0.5)
    cg.add_statements(sample_texts)
    cg.compute_edges(verbose=True)
    cg.summary()

    print("\n" + "="*60)
    print("ANCHORING TO LEDGER")
    print("="*60)

    receipt = cg.anchor_to_ledger()
    print("\n--- Ledger Receipt ---")
    print(json.dumps(receipt, indent=2))

    # Visualize if matplotlib available
    try:
        cg.visualize(output_path=Path("data/conflict_graph_demo.png"))
    except:
        pass

    cg.ledger.close()
    print("\n✓ Demo complete")
```

# Task
Analyze the provided code for potential improvements and expansions, focusing on areas like sentiment analysis, named entity recognition, contradiction metrics, code structure, temporal/numerical processing, graph analysis, visualization, and error handling. Implement the suggested changes and provide the modified code.
## Implement sentiment analysis and ner

### Subtask:
Add functions to perform sentiment analysis and named entity recognition on the statements and integrate the results into the node attributes.

**Reasoning**:
The first step is to add the necessary imports for sentiment analysis and named entity recognition. I will use the `transformers` library for both tasks.


from transformers import pipeline
**Reasoning**:
Now I will define the `perform_sentiment_analysis` and `perform_ner` functions using the imported `pipeline` and integrate these functions into the `add_statements` method of the `ConflictGraph` class to add sentiment and NER results as node attributes.


def perform_sentiment_analysis(text: str) -> Dict[str, Any]:
    """Performs sentiment analysis on the input text."""
    sentiment_analyzer = pipeline("sentiment-analysis")
    result = sentiment_analyzer(text)[0]
    return {"label": result['label'], "score": result['score']}

def perform_ner(text: str) -> List[Dict[str, Any]]:
    """Performs named entity recognition on the input text."""
    ner_analyzer = pipeline("ner", grouped_entities=True)
    entities = ner_analyzer(text)
    return entities

# Modify the add_statements method
def add_statements(self, statements: List[Dict[str, Any]]):
    """
    Add statements to the graph.
    Each statement dict should contain: {"text": "...", "source": "...", "timestamp": "..."}.
    Confidence is auto-estimated if not provided.
    Sentiment and NER are added.
    """
    for s in statements:
        if "text" not in s:
            raise ValueError("Each statement must have a 'text' field")

        # Auto-fill missing fields
        s.setdefault("confidence", estimate_confidence(s["text"]))
        s.setdefault("source", "unknown")
        s.setdefault("timestamp", datetime.datetime.utcnow().isoformat())

        # Add sentiment and NER
        s["sentiment"] = perform_sentiment_analysis(s["text"])
        s["named_entities"] = perform_ner(s["text"])

        self.graph.add_node(s["text"], **s)

# Replace the original add_statements method in the class
ConflictGraph.add_statements = add_statements
**Reasoning**:
The previous code block failed because `Dict` and `Any` were not imported. I need to add the import for `Dict` and `Any` from the `typing` module.


from typing import Dict, Any, List

def perform_sentiment_analysis(text: str) -> Dict[str, Any]:
    """Performs sentiment analysis on the input text."""
    sentiment_analyzer = pipeline("sentiment-analysis")
    result = sentiment_analyzer(text)[0]
    return {"label": result['label'], "score": result['score']}

def perform_ner(text: str) -> List[Dict[str, Any]]:
    """Performs named entity recognition on the input text."""
    ner_analyzer = pipeline("ner", grouped_entities=True)
    entities = ner_analyzer(text)
    return entities

# Modify the add_statements method
def add_statements(self, statements: List[Dict[str, Any]]):
    """
    Add statements to the graph.
    Each statement dict should contain: {"text": "...", "source": "...", "timestamp": "..."}.
    Confidence is auto-estimated if not provided.
    Sentiment and NER are added.
    """
    for s in statements:
        if "text" not in s:
            raise ValueError("Each statement must have a 'text' field")

        # Auto-fill missing fields
        s.setdefault("confidence", estimate_confidence(s["text"]))
        s.setdefault("source", "unknown")
        s.setdefault("timestamp", datetime.datetime.utcnow().isoformat())

        # Add sentiment and NER
        s["sentiment"] = perform_sentiment_analysis(s["text"])
        s["named_entities"] = perform_ner(s["text"])

        self.graph.add_node(s["text"], **s)

# Replace the original add_statements method in the class
ConflictGraph.add_statements = add_statements
**Reasoning**:
The previous code block failed because the `ConflictGraph` class was not defined in the current cell. I need to include the definition of the `ConflictGraph` class along with the new functions and the modified `add_statements` method in a single code block.


import re
import json
import time
import hashlib
import datetime
import numpy as np
import networkx as nx
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline

from tessrax.core.contradiction_ledger import ContradictionLedger


# ----------------------------------------------------------
# Utility: lightweight confidence estimator
# ----------------------------------------------------------
def estimate_confidence(text: str) -> float:
    """Heuristic confidence: longer factual statements score higher."""
    length_score = min(len(text) / 200, 1.0)
    numeric_bonus = 0.1 if re.search(r"\d", text) else 0.0
    modal_penalty = 0.1 if re.search(r"\b(may|might|could|possibly)\b", text.lower()) else 0.0
    return max(0.1, min(1.0, length_score + numeric_bonus - modal_penalty))


# ----------------------------------------------------------
# Core contradiction metrics
# ----------------------------------------------------------
def logical_contradiction(a: str, b: str) -> float:
    """Simple negation detection."""
    a_low, b_low = a.lower(), b.lower()

    # Check for explicit negation patterns
    a_negated = "not" in a_low or "no " in a_low or "never" in a_low
    b_negated = "not" in b_low or "no " in b_low or "never" in b_low

    # Extract core content
    a_clean = re.sub(r"\b(not|no|never)\b", "", a_low).strip()
    b_clean = re.sub(r"\b(not|no|never)\b", "", b_low).strip()

    # High score if one is negated and core content matches
    if a_negated != b_negated and a_clean == b_clean:
        return 1.0

    # Partial match for similar but differently negated statements
    if a_negated != b_negated:
        from difflib import SequenceMatcher
        similarity = SequenceMatcher(None, a_clean, b_clean).ratio()
        if similarity > 0.7:
            return similarity

    return 0.0


def numeric_inconsistency(a: str, b: str) -> float:
    """Detect quantitative disagreement (percentage/number)."""
    # Extract numbers with optional % sign
    nums_a = [float(x.strip("%")) for x in re.findall(r"\d+\.?\d*%?", a)]
    nums_b = [float(x.strip("%")) for x in re.findall(r"\d+\.?\d*%?", b)]

    if not nums_a or not nums_b:
        return 0.0

    # Compare corresponding numbers (take first for simplicity)
    diff = abs(nums_a[0] - nums_b[0])
    ref = max(1e-5, max(nums_a[0], nums_b[0]))
    score = diff / ref

    # Cap at 1.0 and scale: small differences get lower scores
    return min(score, 1.0) if score > 0.1 else 0.0


def temporal_contradiction(a: str, b: str) -> float:
    """Flag reversed or inconsistent temporal claims."""
    # Extract years
    years_a = [int(y) for y in re.findall(r"\b(19|20)\d{2}\b", a)]
    years_b = [int(y) for y in re.findall(r"\b(19|20)\d{2}\b", b)]

    if not years_a or not years_b:
        return 0.0

    # Check for temporal ordering issues
    # e.g., "by 2030" vs "in 2024 only 5% done" implies likely contradiction
    all_years = years_a + years_b

    # Look for "by" vs actual achievement patterns
    by_pattern_a = bool(re.search(r"\bby\s+\d{4}", a))
    by_pattern_b = bool(re.search(r"\bby\s+\d{4}", b))

    # If one states a future goal and another states past underperformance
    if by_pattern_a and not by_pattern_b:
        if max(years_a) > max(years_b):  # Goal year is later than report year
            return 0.5  # Potential contradiction (underperformance)

    return 0.0


def categorical_mismatch(a: str, b: str) -> float:
    """Simple categorical clash detection using keyword domains."""
    # Define domain keywords
    domains = {
        "environmental": ["emission", "carbon", "co2", "climate", "renewable", "pollution"],
        "financial": ["profit", "revenue", "loss", "earnings", "dividend"],
        "social": ["diversity", "equality", "workforce", "community"],
        "operational": ["production", "capacity", "efficiency", "output"]
    }

    # Detect which domains each statement belongs to
    def get_domains(text):
        text_lower = text.lower()
        return {name for name, keywords in domains.items()
                if any(k in text_lower for k in keywords)}

    domains_a = get_domains(a)
    domains_b = get_domains(b)

    # No mismatch if statements share domains
    if domains_a & domains_b:
        return 0.0

    # Potential contradiction if discussing completely different categories
    # but appearing to contradict (this is context-dependent)
    return 0.3 if domains_a and domains_b else 0.0

# ----------------------------------------------------------
# Sentiment Analysis and NER Functions
# ----------------------------------------------------------
def perform_sentiment_analysis(text: str) -> Dict[str, Any]:
    """Performs sentiment analysis on the input text."""
    sentiment_analyzer = pipeline("sentiment-analysis")
    result = sentiment_analyzer(text)[0]
    return {"label": result['label'], "score": result['score']}

def perform_ner(text: str) -> List[Dict[str, Any]]:
    """Performs named entity recognition on the input text."""
    ner_analyzer = pipeline("ner", grouped_entities=True)
    entities = ner_analyzer(text)
    return entities


# ----------------------------------------------------------
# Contradiction Graph Builder
# ----------------------------------------------------------
class ConflictGraph:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2", threshold: float = 0.5):
        """
        Initialize the conflict graph.

        Args:
            model_name: Sentence transformer model for embeddings
            threshold: Minimum contradiction score to create an edge (0.5 more sensitive than 0.7)
        """
        self.model = SentenceTransformer(model_name)
        self.threshold = threshold
        self.graph = nx.Graph()
        self.ledger = ContradictionLedger(Path("data/conflict_graph_ledger.db"))
        self.metadata = {"created": datetime.datetime.utcnow().isoformat()}

    # ------------------------------------------------------
    def add_statements(self, statements: List[Dict[str, Any]]):
        """
        Add statements to the graph.
        Each statement dict should contain: {"text": "...", "source": "...", "timestamp": "..."}.
        Confidence is auto-estimated if not provided.
        Sentiment and NER are added.
        """
        for s in statements:
            if "text" not in s:
                raise ValueError("Each statement must have a 'text' field")

            # Auto-fill missing fields
            s.setdefault("confidence", estimate_confidence(s["text"]))
            s.setdefault("source", "unknown")
            s.setdefault("timestamp", datetime.datetime.utcnow().isoformat())

            # Add sentiment and NER
            s["sentiment"] = perform_sentiment_analysis(s["text"])
            s["named_entities"] = perform_ner(s["text"])


            self.graph.add_node(s["text"], **s)

    # ------------------------------------------------------
    def compute_edges(self, verbose: bool = False):
        """
        Compute contradiction edges between all statement pairs.
        Uses semantic similarity to filter, then calculates contradiction metrics.
        """
        texts = list(self.graph.nodes())
        n = len(texts)

        if n < 2:
            if verbose:
                print("Need at least 2 statements to compute contradictions")
            return

        if verbose:
            print(f"Computing embeddings for {n} statements...")

        embeddings = self.model.encode(texts, convert_to_tensor=True, show_progress_bar=verbose)
        cos_sim = util.cos_sim(embeddings, embeddings).cpu().numpy()

        edges_added = 0
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                sim = float(cos_sim[i][j])

                # Skip pairs that are too dissimilar (unrelated topics)
                if sim < 0.3:
                    continue

                # Compute individual contradiction metrics
                L = logical_contradiction(texts[i], texts[j])
                N = numeric_inconsistency(texts[i], texts[j])
                T = temporal_contradiction(texts[i], texts[j])
                C = categorical_mismatch(texts[i], texts[j])

                # Get confidence scores
                conf_i = self.graph.nodes[texts[i]]["confidence"]
                conf_j = self.graph.nodes[texts[j]]["confidence"]
                avg_confidence = (conf_i + conf_j) / 2

                # Weighted contradiction score
                # Higher weights for logical and numeric contradictions
                contradiction_score = (
                    0.4 * L +  # Logical negation
                    0.3 * N +  # Numerical mismatch
                    0.2 * T +  # Temporal inconsistency
                    0.1 * C    # Categorical clash
                ) * sim * avg_confidence

                if contradiction_score >= self.threshold:
                    self.graph.add_edge(
                        texts[i],
                        texts[j],
                        weight=round(contradiction_score, 3),
                        sim=round(sim, 3),
                        metrics={"L": round(L, 2), "N": round(N, 2),
                                "T": round(T, 2), "C": round(C, 2)}
                    )
                    edges_added += 1

        if verbose:
            print(f"Added {edges_added} contradiction edges (threshold={self.threshold})")

    # ------------------------------------------------------
    def detect_clusters(self) -> List[List[str]]:
        """
        Find densely connected contradiction clusters.
        Returns list of clusters, each containing contradicting statements.
        """
        # Find connected components (groups of mutually contradicting statements)
        comps = nx.connected_components(self.graph)
        clusters = [list(c) for c in comps if len(c) > 1]
        return clusters

    # ------------------------------------------------------
    def get_worst_offenders(self, top_k: int = 5) -> List[Tuple[str, int]]:
        """Find statements with the most contradictions."""
        degrees = dict(self.graph.degree())
        sorted_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
        return sorted_nodes[:top_k]

    # ------------------------------------------------------
    def export_bundle(self) -> Dict[str, Any]:
        """Export the current graph as a contradiction bundle for the ledger."""
        edges = [
            {
                "s_i": u,
                "s_j": v,
                "score": d["weight"],
                "similarity": d["sim"],
                "breakdown": d["metrics"]
            }
            for u, v, d in self.graph.edges(data=True)
        ]

        clusters = self.detect_clusters()
        worst_offenders = self.get_worst_offenders()

        bundle = {
            "edges": edges,
            "clusters": clusters,
            "worst_offenders": [{"text": text[:100], "contradiction_count": count}
                               for text, count in worst_offenders],
            "stats": {
                "total_statements": self.graph.number_of_nodes(),
                "total_contradictions": self.graph.number_of_edges(),
                "cluster_count": len(clusters)
            },
            "timestamp": datetime.datetime.utcnow().isoformat(),
        }

        bundle_json = json.dumps(bundle, sort_keys=True)
        bundle_hash = hashlib.sha256(bundle_json.encode()).hexdigest()
        bundle["hash"] = "sha256:" + bundle_hash

        return {"contradiction_bundle": bundle}

    # ------------------------------------------------------
    def anchor_to_ledger(self) -> Dict[str, Any]:
        """Anchor the contradiction bundle in the Tessrax ContradictionLedger."""
        bundle = self.export_bundle()["contradiction_bundle"]

        receipt = self.ledger.record_contradiction(
            claim_a={"text": "Graph bundle", "id": f"bundle_{bundle['hash'][:8]}"},
            claim_b={"text": f"{bundle['stats']['total_contradictions']} conflicts", "id": "edges"},
            resolution=f"Graph with {bundle['stats']['cluster_count']} contradiction clusters",
            metadata=bundle,
        )

        return receipt

    # ------------------------------------------------------
    def summary(self, verbose: bool = True):
        """Print summary stats."""
        if not verbose:
            return

        print(f"\n{'='*60}")
        print(f"CONFLICT GRAPH SUMMARY")
        print(f"{'='*60}")
        print(f"Nodes (statements): {self.graph.number_of_nodes()}")
        print(f"Edges (contradictions): {self.graph.number_of_edges()}")
        print(f"Contradiction threshold: {self.threshold}")

        if self.graph.number_of_edges() == 0:
            print("\n✓ No contradictions detected above threshold")
            return

        print(f"\n--- Detected Contradictions ---")
        for u, v, d in self.graph.edges(data=True):
            print(f"\n  Score: {d['weight']:.3f} | Similarity: {d['sim']:.3f}")
            print(f"  A: {u[:80]}...")
            print(f"  B: {v[:80]}...")
            print(f"  Metrics: {d['metrics']}")

        clusters = self.detect_clusters()
        if clusters:
            print(f"\n--- Contradiction Clusters ({len(clusters)}) ---")
            for i, cluster in enumerate(clusters, 1):
                print(f"\n  Cluster {i}: {len(cluster)} statements")
                for stmt in cluster:
                    print(f"    • {stmt[:70]}...")

        worst = self.get_worst_offenders(3)
        if worst:
            print(f"\n--- Most Contradictory Statements ---")
            for stmt, count in worst:
                print(f"  • {count} contradictions: {stmt[:70]}...")

    # ------------------------------------------------------
    def visualize(self, output_path: Optional[Path] = None):
        """
        Generate a visual representation of the contradiction graph.
        Requires matplotlib. Saves to file if output_path provided.
        """
        try:
            import matplotlib.pyplot as plt

            pos = nx.spring_layout(self.graph, k=2, iterations=50)

            plt.figure(figsize=(14, 10))

            # Draw nodes
            nx.draw_networkx_nodes(self.graph, pos, node_size=700,
                                  node_color='lightblue', alpha=0.9)

            # Draw edges with width proportional to contradiction score
            edges = self.graph.edges()
            weights = [self.graph[u][v]['weight'] * 3 for u, v in edges]
            nx.draw_networkx_edges(self.graph, pos, width=weights,
                                  alpha=0.6, edge_color='red')

            # Draw labels (truncated)
            labels = {node: node[:30] + "..." if len(node) > 30 else node
                     for node in self.graph.nodes()}
            nx.draw_networkx_labels(self.graph, pos, labels, font_size=8)

            plt.title("Tessrax Contradiction Graph", fontsize=16, fontweight='bold')
            plt.axis('off')
            plt.tight_layout()

            if output_path:
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                print(f"\n✓ Graph visualization saved to {output_path}")
            else:
                plt.show()

        except ImportError:
            print("⚠ matplotlib not installed. Skipping visualization.")

**Reasoning**:
The previous command failed because the `tessrax` module was not found. I need to create a dummy `ContradictionLedger` class to resolve this `ModuleNotFoundError` since the focus is on integrating sentiment analysis and NER into the graph, not on the ledger functionality itself.


import re
import json
import time
import hashlib
import datetime
import numpy as np
import networkx as nx
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline

# Create a dummy ContradictionLedger class
class ContradictionLedger:
    def __init__(self, db_path: Path):
        print(f"Dummy ContradictionLedger initialized with path: {db_path}")
        self.db_path = db_path
        self.records = []

    def record_contradiction(self, claim_a: Dict, claim_b: Dict, resolution: str, metadata: Dict) -> Dict:
        record = {
            "timestamp": datetime.datetime.utcnow().isoformat(),
            "claim_a": claim_a,
            "claim_b": claim_b,
            "resolution": resolution,
            "metadata": metadata
        }
        self.records.append(record)
        print("Dummy Contradiction recorded.")
        return {"status": "success", "record": record}

    def close(self):
        print("Dummy ContradictionLedger closed.")
        pass


# ----------------------------------------------------------
# Utility: lightweight confidence estimator
# ----------------------------------------------------------
def estimate_confidence(text: str) -> float:
    """Heuristic confidence: longer factual statements score higher."""
    length_score = min(len(text) / 200, 1.0)
    numeric_bonus = 0.1 if re.search(r"\d", text) else 0.0
    modal_penalty = 0.1 if re.search(r"\b(may|might|could|possibly)\b", text.lower()) else 0.0
    return max(0.1, min(1.0, length_score + numeric_bonus - modal_penalty))


# ----------------------------------------------------------
# Core contradiction metrics
# ----------------------------------------------------------
def logical_contradiction(a: str, b: str) -> float:
    """Simple negation detection."""
    a_low, b_low = a.lower(), b.lower()

    # Check for explicit negation patterns
    a_negated = "not" in a_low or "no " in a_low or "never" in a_low
    b_negated = "not" in b_low or "no " in b_low or "never" in b_low

    # Extract core content
    a_clean = re.sub(r"\b(not|no|never)\b", "", a_low).strip()
    b_clean = re.sub(r"\b(not|no|never)\b", "", b_low).strip()

    # High score if one is negated and core content matches
    if a_negated != b_negated and a_clean == b_clean:
        return 1.0

    # Partial match for similar but differently negated statements
    if a_negated != b_negated:
        from difflib import SequenceMatcher
        similarity = SequenceMatcher(None, a_clean, b_clean).ratio()
        if similarity > 0.7:
            return similarity

    return 0.0


def numeric_inconsistency(a: str, b: str) -> float:
    """Detect quantitative disagreement (percentage/number)."""
    # Extract numbers with optional % sign
    nums_a = [float(x.strip("%")) for x in re.findall(r"\d+\.?\d*%?", a)]
    nums_b = [float(x.strip("%")) for x in re.findall(r"\d+\.?\d*%?", b)]

    if not nums_a or not nums_b:
        return 0.0

    # Compare corresponding numbers (take first for simplicity)
    diff = abs(nums_a[0] - nums_b[0])
    ref = max(1e-5, max(nums_a[0], nums_b[0]))
    score = diff / ref

    # Cap at 1.0 and scale: small differences get lower scores
    return min(score, 1.0) if score > 0.1 else 0.0


def temporal_contradiction(a: str, b: str) -> float:
    """Flag reversed or inconsistent temporal claims."""
    # Extract years
    years_a = [int(y) for y in re.findall(r"\b(19|20)\d{2}\b", a)]
    years_b = [int(y) for y in re.findall(r"\b(19|20)\d{2}\b", b)]

    if not years_a or not years_b:
        return 0.0

    # Check for temporal ordering issues
    # e.g., "by 2030" vs "in 2024 only 5% done" implies likely contradiction
    all_years = years_a + years_b

    # Look for "by" vs actual achievement patterns
    by_pattern_a = bool(re.search(r"\bby\s+\d{4}", a))
    by_pattern_b = bool(re.search(r"\bby\s+\d{4}", b))

    # If one states a future goal and another states past underperformance
    if by_pattern_a and not by_pattern_b:
        if max(years_a) > max(years_b):  # Goal year is later than report year
            return 0.5  # Potential contradiction (underperformance)

    return 0.0


def categorical_mismatch(a: str, b: str) -> float:
    """Simple categorical clash detection using keyword domains."""
    # Define domain keywords
    domains = {
        "environmental": ["emission", "carbon", "co2", "climate", "renewable", "pollution"],
        "financial": ["profit", "revenue", "loss", "earnings", "dividend"],
        "social": ["diversity", "equality", "workforce", "community"],
        "operational": ["production", "capacity", "efficiency", "output"]
    }

    # Detect which domains each statement belongs to
    def get_domains(text):
        text_lower = text.lower()
        return {name for name, keywords in domains.items()
                if any(k in text_lower for k in keywords)}

    domains_a = get_domains(a)
    domains_b = get_domains(b)

    # No mismatch if statements share domains
    if domains_a & domains_b:
        return 0.0

    # Potential contradiction if discussing completely different categories
    # but appearing to contradict (this is context-dependent)
    return 0.3 if domains_a and domains_b else 0.0

# ----------------------------------------------------------
# Sentiment Analysis and NER Functions
# ----------------------------------------------------------
def perform_sentiment_analysis(text: str) -> Dict[str, Any]:
    """Performs sentiment analysis on the input text."""
    sentiment_analyzer = pipeline("sentiment-analysis")
    result = sentiment_analyzer(text)[0]
    return {"label": result['label'], "score": result['score']}

def perform_ner(text: str) -> List[Dict[str, Any]]:
    """Performs named entity recognition on the input text."""
    ner_analyzer = pipeline("ner", grouped_entities=True)
    entities = ner_analyzer(text)
    return entities


# ----------------------------------------------------------
# Contradiction Graph Builder
# ----------------------------------------------------------
class ConflictGraph:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2", threshold: float = 0.5):
        """
        Initialize the conflict graph.

        Args:
            model_name: Sentence transformer model for embeddings
            threshold: Minimum contradiction score to create an edge (0.5 more sensitive than 0.7)
        """
        self.model = SentenceTransformer(model_name)
        self.threshold = threshold
        self.graph = nx.Graph()
        self.ledger = ContradictionLedger(Path("data/conflict_graph_ledger.db"))
        self.metadata = {"created": datetime.datetime.utcnow().isoformat()}

    # ------------------------------------------------------
    def add_statements(self, statements: List[Dict[str, Any]]):
        """
        Add statements to the graph.
        Each statement dict should contain: {"text": "...", "source": "...", "timestamp": "..."}.
        Confidence is auto-estimated if not provided.
        Sentiment and NER are added.
        """
        for s in statements:
            if "text" not in s:
                raise ValueError("Each statement must have a 'text' field")

            # Auto-fill missing fields
            s.setdefault("confidence", estimate_confidence(s["text"]))
            s.setdefault("source", "unknown")
            s.setdefault("timestamp", datetime.datetime.utcnow().isoformat())

            # Add sentiment and NER
            s["sentiment"] = perform_sentiment_analysis(s["text"])
            s["named_entities"] = perform_ner(s["text"])


            self.graph.add_node(s["text"], **s)

    # ------------------------------------------------------
    def compute_edges(self, verbose: bool = False):
        """
        Compute contradiction edges between all statement pairs.
        Uses semantic similarity to filter, then calculates contradiction metrics.
        """
        texts = list(self.graph.nodes())
        n = len(texts)

        if n < 2:
            if verbose:
                print("Need at least 2 statements to compute contradictions")
            return

        if verbose:
            print(f"Computing embeddings for {n} statements...")

        embeddings = self.model.encode(texts, convert_to_tensor=True, show_progress_bar=verbose)
        cos_sim = util.cos_sim(embeddings, embeddings).cpu().numpy()

        edges_added = 0
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                sim = float(cos_sim[i][j])

                # Skip pairs that are too dissimilar (unrelated topics)
                if sim < 0.3:
                    continue

                # Compute individual contradiction metrics
                L = logical_contradiction(texts[i], texts[j])
                N = numeric_inconsistency(texts[i], texts[j])
                T = temporal_contradiction(texts[i], texts[j])
                C = categorical_mismatch(texts[i], texts[j])

                # Get confidence scores
                conf_i = self.graph.nodes[texts[i]]["confidence"]
                conf_j = self.graph.nodes[texts[j]]["confidence"]
                avg_confidence = (conf_i + conf_j) / 2

                # Weighted contradiction score
                # Higher weights for logical and numeric contradictions
                contradiction_score = (
                    0.4 * L +  # Logical negation
                    0.3 * N +  # Numerical mismatch
                    0.2 * T +  # Temporal inconsistency
                    0.1 * C    # Categorical clash
                ) * sim * avg_confidence

                if contradiction_score >= self.threshold:
                    self.graph.add_edge(
                        texts[i],
                        texts[j],
                        weight=round(contradiction_score, 3),
                        sim=round(sim, 3),
                        metrics={"L": round(L, 2), "N": round(N, 2),
                                "T": round(T, 2), "C": round(C, 2)}
                    )
                    edges_added += 1

        if verbose:
            print(f"Added {edges_added} contradiction edges (threshold={self.threshold})")

    # ------------------------------------------------------
    def detect_clusters(self) -> List[List[str]]:
        """
        Find densely connected contradiction clusters.
        Returns list of clusters, each containing contradicting statements.
        """
        # Find connected components (groups of mutually contradicting statements)
        comps = nx.connected_components(self.graph)
        clusters = [list(c) for c in comps if len(c) > 1]
        return clusters

    # ------------------------------------------------------
    def get_worst_offenders(self, top_k: int = 5) -> List[Tuple[str, int]]:
        """Find statements with the most contradictions."""
        degrees = dict(self.graph.degree())
        sorted_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
        return sorted_nodes[:top_k]

    # ------------------------------------------------------
    def export_bundle(self) -> Dict[str, Any]:
        """Export the current graph as a contradiction bundle for the ledger."""
        edges = [
            {
                "s_i": u,
                "s_j": v,
                "score": d["weight"],
                "similarity": d["sim"],
                "breakdown": d["metrics"]
            }
            for u, v, d in self.graph.edges(data=True)
        ]

        clusters = self.detect_clusters()
        worst_offenders = self.get_worst_offenders()

        bundle = {
            "edges": edges,
            "clusters": clusters,
            "worst_offenders": [{"text": text[:100], "contradiction_count": count}
                               for text, count in worst_offenders],
            "stats": {
                "total_statements": self.graph.number_of_nodes(),
                "total_contradictions": self.graph.number_of_edges(),
                "cluster_count": len(clusters)
            },
            "timestamp": datetime.datetime.utcnow().isoformat(),
        }

        bundle_json = json.dumps(bundle, sort_keys=True)
        bundle_hash = hashlib.sha256(bundle_json.encode()).hexdigest()
        bundle["hash"] = "sha256:" + bundle_hash

        return {"contradiction_bundle": bundle}

    # ------------------------------------------------------
    def anchor_to_ledger(self) -> Dict[str, Any]:
        """Anchor the contradiction bundle in the Tessrax ContradictionLedger."""
        bundle = self.export_bundle()["contradiction_bundle"]

        receipt = self.ledger.record_contradiction(
            claim_a={"text": "Graph bundle", "id": f"bundle_{bundle['hash'][:8]}"},
            claim_b={"text": f"{bundle['stats']['total_contradictions']} conflicts", "id": "edges"},
            resolution=f"Graph with {bundle['stats']['cluster_count']} contradiction clusters",
            metadata=bundle,
        )

        return receipt

    # ------------------------------------------------------
    def summary(self, verbose: bool = True):
        """Print summary stats."""
        if not verbose:
            return

        print(f"\n{'='*60}")
        print(f"CONFLICT GRAPH SUMMARY")
        print(f"{'='*60}")
        print(f"Nodes (statements): {self.graph.number_of_nodes()}")
        print(f"Edges (contradictions): {self.graph.number_of_edges()}")
        print(f"Contradiction threshold: {self.threshold}")

        if self.graph.number_of_edges() == 0:
            print("\n✓ No contradictions detected above threshold")
            return

        print(f"\n--- Detected Contradictions ---")
        for u, v, d in self.graph.edges(data=True):
            print(f"\n  Score: {d['weight']:.3f} | Similarity: {d['sim']:.3f}")
            print(f"  A: {u[:80]}...")
            print(f"  B: {v[:80]}...")
            print(f"  Metrics: {d['metrics']}")

        clusters = self.detect_clusters()
        if clusters:
            print(f"\n--- Contradiction Clusters ({len(clusters)}) ---")
            for i, cluster in enumerate(clusters, 1):
                print(f"\n  Cluster {i}: {len(cluster)} statements")
                for stmt in cluster:
                    print(f"    • {stmt[:70]}...")

        worst = self.get_worst_offenders(3)
        if worst:
            print(f"\n--- Most Contradictory Statements ---")
            for stmt, count in worst:
                print(f"  • {count} contradictions: {stmt[:70]}...")

    # ------------------------------------------------------
    def visualize(self, output_path: Optional[Path] = None):
        """
        Generate a visual representation of the contradiction graph.
        Requires matplotlib. Saves to file if output_path provided.
        """
        try:
            import matplotlib.pyplot as plt

            pos = nx.spring_layout(self.graph, k=2, iterations=50)

            plt.figure(figsize=(14, 10))

            # Draw nodes
            nx.draw_networkx_nodes(self.graph, pos, node_size=700,
                                  node_color='lightblue', alpha=0.9)

            # Draw edges with width proportional to contradiction score
            edges = self.graph.edges()
            weights = [self.graph[u][v]['weight'] * 3 for u, v in edges]
            nx.draw_networkx_edges(self.graph, pos, width=weights,
                                  alpha=0.6, edge_color='red')

            # Draw labels (truncated)
            labels = {node: node[:30] + "..." if len(node) > 30 else node
                     for node in self.graph.nodes()}
            nx.draw_networkx_labels(self.graph, pos, labels, font_size=8)

            plt.title("Tessrax Contradiction Graph", fontsize=16, fontweight='bold')
            plt.axis('off')
            plt.tight_layout()

            if output_path:
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                print(f"\n✓ Graph visualization saved to {output_path}")
            else:
                plt.show()

        except ImportError:
            print("⚠ matplotlib not installed. Skipping visualization.")
## Update contradiction metrics

### Subtask:
Modify the contradiction metric functions to leverage the sentiment and NER information for more accurate scoring.

**Reasoning**:
Modify the contradiction metric functions (`logical_contradiction`, `numeric_inconsistency`, `temporal_contradiction`, `categorical_mismatch`) to accept sentiment and NER data for statements a and b as arguments, and incorporate this data into their scoring logic as per the instructions. Update the `compute_edges` method to pass these attributes to the metric functions.


# Import necessary modules (already done in previous cells, just listing for clarity)
# import re
# import json
# import time
# import hashlib
# import datetime
# import numpy as np
# import networkx as nx
# from pathlib import Path
# from typing import List, Dict, Any, Tuple, Optional
# from sentence_transformers import SentenceTransformer, util
# from transformers import pipeline

# Dummy ContradictionLedger (already defined)
# class ContradictionLedger: ...

# Utility: lightweight confidence estimator (already defined)
# def estimate_confidence(text: str) -> float: ...

# ----------------------------------------------------------
# Core contradiction metrics (Modified to accept sentiment/NER)
# ----------------------------------------------------------
def logical_contradiction(a: str, b: str, sentiment_a: Dict[str, Any], sentiment_b: Dict[str, Any]) -> float:
    """Simple negation detection, enhanced with sentiment."""
    a_low, b_low = a.lower(), b.lower()

    # Check for explicit negation patterns
    a_negated = "not" in a_low or "no " in a_low or "never" in a_low
    b_negated = "not" in b_low or "no " in b_low or "never" in b_low

    # Extract core content
    a_clean = re.sub(r"\b(not|no|never)\b", "", a_low).strip()
    b_clean = re.sub(r"\b(not|no|never)\b", "", b_low).strip()

    score = 0.0

    # High score if one is negated and core content matches
    if a_negated != b_negated and a_clean == b_clean:
        score = 1.0
    # Partial match for similar but differently negated statements
    elif a_negated != b_negated:
        from difflib import SequenceMatcher
        similarity = SequenceMatcher(None, a_clean, b_clean).ratio()
        if similarity > 0.7:
            score = similarity

    # --- Sentiment Enhancement ---
    # If sentiment is opposite and confidence is high, slightly increase score
    if sentiment_a and sentiment_b:
        if sentiment_a['label'] != sentiment_b['label'] and \
           sentiment_a['score'] > 0.8 and sentiment_b['score'] > 0.8: # Use a threshold for sentiment confidence
            score = min(1.0, score + 0.1) # Add a small bonus, cap at 1.0

    return score


def numeric_inconsistency(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float:
    """Detect quantitative disagreement (percentage/number), enhanced with NER."""
    # Extract numbers with optional % sign
    nums_a = [float(x.strip("%")) for x in re.findall(r"\d+\.?\d*%?", a)]
    nums_b = [float(x.strip("%")) for x in re.findall(r"\d+\.?\d*%?", b)]

    if not nums_a or not nums_b:
        return 0.0

    # Compare corresponding numbers (take first for simplicity)
    diff = abs(nums_a[0] - nums_b[0])
    ref = max(1e-5, max(nums_a[0], nums_b[0]))
    score = diff / ref

    # Cap at 1.0 and scale: small differences get lower scores
    score = min(score, 1.0) if score > 0.1 else 0.0

    # --- NER Enhancement ---
    # Check if the numbers are associated with the same entity type
    shared_entity_type = False
    if ner_a and ner_b:
        entity_types_a = {entity['entity_group'] for entity in ner_a if 'entity_group' in entity}
        entity_types_b = {entity['entity_group'] for entity in ner_b if 'entity_group' in entity}
        if entity_types_a & entity_types_b: # Check for common entity types
             # Further check if the numbers appear close to shared entities in text
            for entity_type in entity_types_a & entity_types_b:
                entities_in_a = [e for e in ner_a if e['entity_group'] == entity_type]
                entities_in_b = [e for e in ner_b if e['entity_group'] == entity_type]
                if entities_in_a and entities_in_b:
                     # Simple check: see if the first number is within a reasonable distance of *any* shared entity
                     # This is a heuristic, more complex analysis would involve matching numbers to specific entities
                     num_a_idx = a.find(str(nums_a[0])) if nums_a[0] else -1
                     num_b_idx = b.find(str(nums_b[0])) if nums_b[0] else -1
                     if num_a_idx != -1 and num_b_idx != -1:
                         for ent_a in entities_in_a:
                             if abs(num_a_idx - ent_a['start']) < 20: # Check proximity
                                 for ent_b in entities_in_b:
                                     if abs(num_b_idx - ent_b['start']) < 20: # Check proximity
                                         shared_entity_type = True
                                         break
                                 if shared_entity_type: break
                             if shared_entity_type: break
                         if shared_entity_type: break


    if shared_entity_type:
        score = min(1.0, score * 1.2) # Increase weight if numbers relate to same entity type

    return score


def temporal_contradiction(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float:
    """Flag reversed or inconsistent temporal claims, enhanced with NER."""
    # Extract years
    years_a = [int(y) for y in re.findall(r"\b(19|20)\d{2}\b", a)]
    years_b = [int(y) for y in re.findall(r"\b(19|20)\d{2}\b", b)]

    if not years_a or not years_b:
        return 0.0

    score = 0.0

    # Check for temporal ordering issues (original logic)
    all_years = years_a + years_b
    by_pattern_a = bool(re.search(r"\bby\s+\d{4}", a))
    by_pattern_b = bool(re.search(r"\bby\s+\d{4}", b))

    if by_pattern_a and not by_pattern_b:
        if max(years_a) > max(years_b):
            score = 0.5

    # --- NER Enhancement ---
    # If temporal expressions are linked to the same entity or event, increase score
    shared_event_or_entity = False
    if ner_a and ner_b:
         temporal_entities_a = [ent for ent in ner_a if ent['entity_group'] in ['DATE', 'TIME']]
         temporal_entities_b = [ent for ent in ner_b if ent['entity_group'] in ['DATE', 'TIME']]

         if temporal_entities_a and temporal_entities_b:
             # Simple check: look for shared non-temporal entities near the temporal ones
             non_temporal_entities_a = {ent['word'] for ent in ner_a if ent['entity_group'] not in ['DATE', 'TIME']}
             non_temporal_entities_b = {ent['word'] for ent in ner_b if ent['entity_group'] not in ['DATE', 'TIME']}

             if non_temporal_entities_a & non_temporal_entities_b:
                 shared_event_or_entity = True # Indicate potential link

    if shared_event_or_entity and score > 0: # Only boost if there's an existing temporal score
        score = min(1.0, score + 0.2) # Add a bonus

    return score


def categorical_mismatch(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float:
    """Simple categorical clash detection using keyword domains, enhanced with NER."""
    # Define domain keywords (already defined)
    domains = {
        "environmental": ["emission", "carbon", "co2", "climate", "renewable", "pollution"],
        "financial": ["profit", "revenue", "loss", "earnings", "dividend"],
        "social": ["diversity", "equality", "workforce", "community"],
        "operational": ["production", "capacity", "efficiency", "output"]
    }

    # Detect which domains each statement belongs to
    def get_domains(text):
        text_lower = text.lower()
        return {name for name, keywords in domains.items()
                if any(k in text_lower for k in keywords)}

    domains_a = get_domains(a)
    domains_b = get_domains(b)

    score = 0.0

    # Potential contradiction if discussing completely different categories but appearing to contradict
    if domains_a and domains_b and not (domains_a & domains_b):
        score = 0.3

    # --- NER Enhancement ---
    # If statements discuss different categories but share the same named entity, slightly increase score
    shared_entity_in_different_categories = False
    if ner_a and ner_b and score > 0: # Only check if there's a categorical mismatch score
        entities_a = {ent['word'] for ent in ner_a}
        entities_b = {ent['word'] for ent in ner_b}
        if entities_a & entities_b: # Check for common entities
            shared_entity_in_different_categories = True

    if shared_entity_in_different_categories:
         score = min(1.0, score + 0.1) # Add a small bonus

    return score

# Sentiment Analysis and NER Functions (already defined)
# def perform_sentiment_analysis(text: str) -> Dict[str, Any]: ...
# def perform_ner(text: str) -> List[Dict[str, Any]]: ...


# ----------------------------------------------------------
# Contradiction Graph Builder (Modified compute_edges)
# ----------------------------------------------------------
class ConflictGraph:
    def __init__(self, model_name: str = "all-MiniLM-L6-v2", threshold: float = 0.5):
        """
        Initialize the conflict graph.

        Args:
            model_name: Sentence transformer model for embeddings
            threshold: Minimum contradiction score to create an edge (0.5 more sensitive than 0.7)
        """
        self.model = SentenceTransformer(model_name)
        self.threshold = threshold
        self.graph = nx.Graph()
        self.ledger = ContradictionLedger(Path("data/conflict_graph_ledger.db"))
        self.metadata = {"created": datetime.datetime.utcnow().isoformat()}

    # ------------------------------------------------------
    def add_statements(self, statements: List[Dict[str, Any]]):
        """
        Add statements to the graph.
        Each statement dict should contain: {"text": "...", "source": "...", "timestamp": "..."}.
        Confidence is auto-estimated if not provided.
        Sentiment and NER are added.
        """
        # Define pipelines here to avoid re-initializing every time add_statements is called
        self.sentiment_analyzer = pipeline("sentiment-analysis")
        self.ner_analyzer = pipeline("ner", grouped_entities=True)


        for s in statements:
            if "text" not in s:
                raise ValueError("Each statement must have a 'text' field")

            # Auto-fill missing fields
            s.setdefault("confidence", estimate_confidence(s["text"]))
            s.setdefault("source", "unknown")
            s.setdefault("timestamp", datetime.datetime.utcnow().isoformat())

            # Add sentiment and NER using initialized pipelines
            s["sentiment"] = self.sentiment_analyzer(s["text"])[0]
            s["named_entities"] = self.ner_analyzer(s["text"])

            self.graph.add_node(s["text"], **s)

    # ------------------------------------------------------
    def compute_edges(self, verbose: bool = False):
        """
        Compute contradiction edges between all statement pairs.
        Uses semantic similarity to filter, then calculates contradiction metrics.
        Passes sentiment and NER data to metric functions.
        """
        texts = list(self.graph.nodes())
        n = len(texts)

        if n < 2:
            if verbose:
                print("Need at least 2 statements to compute contradictions")
            return

        if verbose:
            print(f"Computing embeddings for {n} statements...")

        embeddings = self.model.encode(texts, convert_to_tensor=True, show_progress_bar=verbose)
        cos_sim = util.cos_sim(embeddings, embeddings).cpu().numpy()

        edges_added = 0
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                text_i = texts[i]
                text_j = texts[j]

                # Get node attributes for sentiment and NER
                sentiment_i = self.graph.nodes[text_i].get("sentiment")
                ner_i = self.graph.nodes[text_i].get("named_entities")
                sentiment_j = self.graph.nodes[text_j].get("sentiment")
                ner_j = self.graph.nodes[text_j].get("named_entities")


                sim = float(cos_sim[i][j])

                # Skip pairs that are too dissimilar (unrelated topics)
                if sim < 0.3:
                    continue

                # Compute individual contradiction metrics, passing sentiment and NER
                L = logical_contradiction(text_i, text_j, sentiment_i, sentiment_j)
                N = numeric_inconsistency(text_i, text_j, ner_i, ner_j)
                T = temporal_contradiction(text_i, text_j, ner_i, ner_j)
                C = categorical_mismatch(text_i, text_j, ner_i, ner_j)

                # Get confidence scores
                conf_i = self.graph.nodes[text_i]["confidence"]
                conf_j = self.graph.nodes[text_j]["confidence"]
                avg_confidence = (conf_i + conf_j) / 2

                # Weighted contradiction score
                # Higher weights for logical and numeric contradictions
                contradiction_score = (
                    0.4 * L +  # Logical negation
                    0.3 * N +  # Numerical mismatch
                    0.2 * T +  # Temporal inconsistency
                    0.1 * C    # Categorical clash
                ) * sim * avg_confidence

                if contradiction_score >= self.threshold:
                    self.graph.add_edge(
                        text_i,
                        text_j,
                        weight=round(contradiction_score, 3),
                        sim=round(sim, 3),
                        metrics={"L": round(L, 2), "N": round(N, 2),
                                "T": round(T, 2), "C": round(C, 2)}
                    )
                    edges_added += 1

        if verbose:
            print(f"Added {edges_added} contradiction edges (threshold={self.threshold})")

    # Rest of the class methods (detect_clusters, get_worst_offenders, export_bundle, anchor_to_ledger, summary, visualize) are unchanged

    # ------------------------------------------------------
    def detect_clusters(self) -> List[List[str]]:
        """
        Find densely connected contradiction clusters.
        Returns list of clusters, each containing contradicting statements.
        """
        # Find connected components (groups of mutually contradicting statements)
        comps = nx.connected_components(self.graph)
        clusters = [list(c) for c in comps if len(c) > 1]
        return clusters

    # ------------------------------------------------------
    def get_worst_offenders(self, top_k: int = 5) -> List[Tuple[str, int]]:
        """Find statements with the most contradictions."""
        degrees = dict(self.graph.degree())
        sorted_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
        return sorted_nodes[:top_k]

    # ------------------------------------------------------
    def export_bundle(self) -> Dict[str, Any]:
        """Export the current graph as a contradiction bundle for the ledger."""
        edges = [
            {
                "s_i": u,
                "s_j": v,
                "score": d["weight"],
                "similarity": d["sim"],
                "breakdown": d["metrics"]
            }
            for u, v, d in self.graph.edges(data=True)
        ]

        clusters = self.detect_clusters()
        worst_offenders = self.get_worst_offenders()

        bundle = {
            "edges": edges,
            "clusters": clusters,
            "worst_offenders": [{"text": text[:100], "contradiction_count": count}
                               for text, count in worst_offenders],
            "stats": {
                "total_statements": self.graph.number_of_nodes(),
                "total_contradictions": self.graph.number_of_edges(),
                "cluster_count": len(clusters)
            },
            "timestamp": datetime.datetime.utcnow().isoformat(),
        }

        bundle_json = json.dumps(bundle, sort_keys=True)
        bundle_hash = hashlib.sha256(bundle_json.encode()).hexdigest()
        bundle["hash"] = "sha256:" + bundle_hash

        return {"contradiction_bundle": bundle}

    # ------------------------------------------------------
    def anchor_to_ledger(self) -> Dict[str, Any]:
        """Anchor the contradiction bundle in the Tessrax ContradictionLedger."""
        bundle = self.export_bundle()["contradiction_bundle"]

        receipt = self.ledger.record_contradiction(
            claim_a={"text": "Graph bundle", "id": f"bundle_{bundle['hash'][:8]}"},
            claim_b={"text": f"{bundle['stats']['total_contradictions']} conflicts", "id": "edges"},
            resolution=f"Graph with {bundle['stats']['cluster_count']} contradiction clusters",
            metadata=bundle,
        )

        return receipt

    # ------------------------------------------------------
    def summary(self, verbose: bool = True):
        """Print summary stats."""
        if not verbose:
            return

        print(f"\n{'='*60}")
        print(f"CONFLICT GRAPH SUMMARY")
        print(f"{'='*60}")
        print(f"Nodes (statements): {self.graph.number_of_nodes()}")
        print(f"Edges (contradictions): {self.graph.number_of_edges()}")
        print(f"Contradiction threshold: {self.threshold}")

        if self.graph.number_of_edges() == 0:
            print("\n✓ No contradictions detected above threshold")
            return

        print(f"\n--- Detected Contradictions ---")
        for u, v, d in self.graph.edges(data=True):
            print(f"\n  Score: {d['weight']:.3f} | Similarity: {d['sim']:.3f}")
            print(f"  A: {u[:80]}...")
            print(f"  B: {v[:80]}...")
            print(f"  Metrics: {d['metrics']}")

        clusters = self.detect_clusters()
        if clusters:
            print(f"\n--- Contradiction Clusters ({len(clusters)}) ---")
            for i, cluster in enumerate(clusters, 1):
                print(f"\n  Cluster {i}: {len(cluster)} statements")
                for stmt in cluster:
                    print(f"    • {stmt[:70]}...")

        worst = self.get_worst_offenders(3)
        if worst:
            print(f"\n--- Most Contradictory Statements ---")
            for stmt, count in worst:
                print(f"  • {count} contradictions: {stmt[:70]}...")

    # ------------------------------------------------------
    def visualize(self, output_path: Optional[Path] = None):
        """
        Generate a visual representation of the contradiction graph.
        Requires matplotlib. Saves to file if output_path provided.
        """
        try:
            import matplotlib.pyplot as plt

            pos = nx.spring_layout(self.graph, k=2, iterations=50)

            plt.figure(figsize=(14, 10))

            # Draw nodes
            nx.draw_networkx_nodes(self.graph, pos, node_size=700,
                                  node_color='lightblue', alpha=0.9)

            # Draw edges with width proportional to contradiction score
            edges = self.graph.edges()
            weights = [self.graph[u][v]['weight'] * 3 for u, v in edges]
            nx.draw_networkx_edges(self.graph, pos, width=weights,
                                  alpha=0.6, edge_color='red')

            # Draw labels (truncated)
            labels = {node: node[:30] + "..." if len(node) > 30 else node
                     for node in self.graph.nodes()}
            nx.draw_networkx_labels(self.graph, pos, labels, font_size=8)

            plt.title("Tessrax Contradiction Graph", fontsize=16, fontweight='bold')
            plt.axis('off')
            plt.tight_layout()

            if output_path:
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                print(f"\n✓ Graph visualization saved to {output_path}")
            else:
                plt.show()

        except ImportError:
            print("⚠ matplotlib not installed. Skipping visualization.")
## Refactor code structure

### Subtask:
Organize parameters into a configuration file and add logging.

**Reasoning**:
Define a configuration dictionary and update the `ConflictGraph` class to use these parameters. Also, set up basic logging and add log messages to key methods.


import logging
import sys

# Configuration dictionary
CONFIG = {
    "model_name": "all-MiniLM-L6-v2",
    "threshold": 0.5,
    "ledger_db_path": "data/conflict_graph_ledger.db",
    "log_level": "INFO",
    "log_file": "conflict_graph.log"
}

# Set up logging
logging.basicConfig(
    level=getattr(logging, CONFIG["log_level"].upper(), logging.INFO),
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout), # Log to console
        logging.FileHandler(CONFIG["log_file"]) # Log to file
    ]
)

logger = logging.getLogger(__name__)

# Dummy ContradictionLedger (already defined)
# class ContradictionLedger: ...

# Utility: lightweight confidence estimator (already defined)
# def estimate_confidence(text: str) -> float: ...

# Core contradiction metrics (already defined)
# def logical_contradiction(a: str, b: str, sentiment_a: Dict[str, Any], sentiment_b: Dict[str, Any]) -> float: ...
# def numeric_inconsistency(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float: ...
# def temporal_contradiction(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float: ...
# def categorical_mismatch(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float: ...

# Sentiment Analysis and NER Functions (already defined)
# def perform_sentiment_analysis(text: str) -> Dict[str, Any]: ...
# def perform_ner(text: str) -> List[Dict[str, Any]]: ...


# ----------------------------------------------------------
# Contradiction Graph Builder (Modified to use config and logging)
# ----------------------------------------------------------
class ConflictGraph:
    def __init__(self):
        """
        Initialize the conflict graph using parameters from CONFIG.
        """
        logger.info("Initializing ConflictGraph...")
        self.model = SentenceTransformer(CONFIG["model_name"])
        self.threshold = CONFIG["threshold"]
        self.graph = nx.Graph()
        self.ledger = ContradictionLedger(Path(CONFIG["ledger_db_path"]))
        self.metadata = {"created": datetime.datetime.utcnow().isoformat()}
        logger.info(f"ConflictGraph initialized with model: {CONFIG['model_name']} and threshold: {self.threshold}")


    # ------------------------------------------------------
    def add_statements(self, statements: List[Dict[str, Any]]):
        """
        Add statements to the graph.
        Each statement dict should contain: {"text": "...", "source": "...", "timestamp": "..."}.
        Confidence is auto-estimated if not provided.
        Sentiment and NER are added.
        """
        logger.info(f"Adding {len(statements)} statements to the graph.")
        # Define pipelines here to avoid re-initializing every time add_statements is called
        self.sentiment_analyzer = pipeline("sentiment-analysis")
        self.ner_analyzer = pipeline("ner", grouped_entities=True)


        for s in statements:
            if "text" not in s:
                logger.warning("Skipping statement due to missing 'text' field.")
                continue

            # Auto-fill missing fields
            s.setdefault("confidence", estimate_confidence(s["text"]))
            s.setdefault("source", "unknown")
            s.setdefault("timestamp", datetime.datetime.utcnow().isoformat())

            # Add sentiment and NER using initialized pipelines
            try:
                s["sentiment"] = self.sentiment_analyzer(s["text"])[0]
                s["named_entities"] = self.ner_analyzer(s["text"])
                logger.debug(f"Processed sentiment and NER for statement: {s['text'][:50]}...")
            except Exception as e:
                logger.error(f"Error processing sentiment/NER for statement: {s['text'][:50]}... - {e}")
                s["sentiment"] = None
                s["named_entities"] = []


            self.graph.add_node(s["text"], **s)
            logger.debug(f"Added node for statement: {s['text'][:50]}...")

        logger.info(f"Finished adding statements. Total nodes: {self.graph.number_of_nodes()}")


    # ------------------------------------------------------
    def compute_edges(self, verbose: bool = False):
        """
        Compute contradiction edges between all statement pairs.
        Uses semantic similarity to filter, then calculates contradiction metrics.
        Passes sentiment and NER data to metric functions.
        """
        texts = list(self.graph.nodes())
        n = len(texts)
        logger.info(f"Computing edges for {n} statements.")

        if n < 2:
            logger.info("Need at least 2 statements to compute contradictions. Skipping edge computation.")
            if verbose:
                print("Need at least 2 statements to compute contradictions")
            return

        if verbose:
            print(f"Computing embeddings for {n} statements...")
        logger.info("Computing sentence embeddings.")

        embeddings = self.model.encode(texts, convert_to_tensor=True, show_progress_bar=verbose)
        cos_sim = util.cos_sim(embeddings, embeddings).cpu().numpy()

        edges_added = 0
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                text_i = texts[i]
                text_j = texts[j]

                # Get node attributes for sentiment and NER
                sentiment_i = self.graph.nodes[text_i].get("sentiment")
                ner_i = self.graph.nodes[text_i].get("named_entities")
                sentiment_j = self.graph.nodes[text_j].get("sentiment")
                ner_j = self.graph.nodes[text_j].get("named_entities")


                sim = float(cos_sim[i][j])

                # Skip pairs that are too dissimilar (unrelated topics)
                if sim < 0.3:
                    logger.debug(f"Skipping pair (sim={sim:.3f}): '{text_i[:30]}...' vs '{text_j[:30]}...'")
                    continue

                # Compute individual contradiction metrics, passing sentiment and NER
                L = logical_contradiction(text_i, text_j, sentiment_i, sentiment_j)
                N = numeric_inconsistency(text_i, text_j, ner_i, ner_j)
                T = temporal_contradiction(text_i, text_j, ner_i, ner_j)
                C = categorical_mismatch(text_i, text_j, ner_i, ner_j)

                # Get confidence scores
                conf_i = self.graph.nodes[text_i]["confidence"]
                conf_j = self.graph.nodes[text_j]["confidence"]
                avg_confidence = (conf_i + conf_j) / 2

                # Weighted contradiction score
                # Higher weights for logical and numeric contradictions
                contradiction_score = (
                    0.4 * L +  # Logical negation
                    0.3 * N +  # Numerical mismatch
                    0.2 * T +  # Temporal inconsistency
                    0.1 * C    # Categorical clash
                ) * sim * avg_confidence

                if contradiction_score >= self.threshold:
                    self.graph.add_edge(
                        text_i,
                        text_j,
                        weight=round(contradiction_score, 3),
                        sim=round(sim, 3),
                        metrics={"L": round(L, 2), "N": round(N, 2),
                                "T": round(T, 2), "C": round(C, 2)}
                    )
                    edges_added += 1
                    logger.info(f"Added edge with score {contradiction_score:.3f}: '{text_i[:50]}...' vs '{text_j[:50]}...'")


        if verbose:
            print(f"Added {edges_added} contradiction edges (threshold={self.threshold})")
        logger.info(f"Finished computing edges. Added {edges_added} edges.")

    # Rest of the class methods (detect_clusters, get_worst_offenders, export_bundle, anchor_to_ledger, summary, visualize) are unchanged

    # ------------------------------------------------------
    def detect_clusters(self) -> List[List[str]]:
        """
        Find densely connected contradiction clusters.
        Returns list of clusters, each containing contradicting statements.
        """
        logger.info("Detecting contradiction clusters.")
        # Find connected components (groups of mutually contradicting statements)
        comps = nx.connected_components(self.graph)
        clusters = [list(c) for c in comps if len(c) > 1]
        logger.info(f"Detected {len(clusters)} clusters.")
        return clusters

    # ------------------------------------------------------
    def get_worst_offenders(self, top_k: int = 5) -> List[Tuple[str, int]]:
        """Find statements with the most contradictions."""
        logger.info(f"Finding top {top_k} worst offenders.")
        degrees = dict(self.graph.degree())
        sorted_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
        worst = sorted_nodes[:top_k]
        logger.info(f"Found {len(worst)} worst offenders.")
        return worst

    # ------------------------------------------------------
    def export_bundle(self) -> Dict[str, Any]:
        """Export the current graph as a contradiction bundle for the ledger."""
        logger.info("Exporting graph bundle.")
        edges = [
            {
                "s_i": u,
                "s_j": v,
                "score": d["weight"],
                "similarity": d["sim"],
                "breakdown": d["metrics"]
            }
            for u, v, d in self.graph.edges(data=True)
        ]

        clusters = self.detect_clusters()
        worst_offenders = self.get_worst_offenders()

        bundle = {
            "edges": edges,
            "clusters": clusters,
            "worst_offenders": [{"text": text[:100], "contradiction_count": count}
                               for text, count in worst_offenders],
            "stats": {
                "total_statements": self.graph.number_of_nodes(),
                "total_contradictions": self.graph.number_of_edges(),
                "cluster_count": len(clusters)
            },
            "timestamp": datetime.datetime.utcnow().isoformat(),
        }

        bundle_json = json.dumps(bundle, sort_keys=True)
        bundle_hash = hashlib.sha256(bundle_json.encode()).hexdigest()
        bundle["hash"] = "sha256:" + bundle_hash

        logger.info(f"Graph bundle exported with hash: {bundle['hash'][:8]}")
        return {"contradiction_bundle": bundle}

    # ------------------------------------------------------
    def anchor_to_ledger(self) -> Dict[str, Any]:
        """Anchor the contradiction bundle in the Tessrax ContradictionLedger."""
        logger.info("Anchoring bundle to ledger.")
        bundle = self.export_bundle()["contradiction_bundle"]

        receipt = self.ledger.record_contradiction(
            claim_a={"text": "Graph bundle", "id": f"bundle_{bundle['hash'][:8]}"},
            claim_b={"text": f"{bundle['stats']['total_contradictions']} conflicts", "id": "edges"},
            resolution=f"Graph with {bundle['stats']['cluster_count']} contradiction clusters",
            metadata=bundle,
        )

        logger.info(f"Bundle anchored. Receipt: {receipt.get('status', 'N/A')}")
        return receipt

    # ------------------------------------------------------
    def summary(self, verbose: bool = True):
        """Print summary stats."""
        if not verbose:
            return

        logger.info("Generating summary.")
        print(f"\n{'='*60}")
        print(f"CONFLICT GRAPH SUMMARY")
        print(f"{'='*60}")
        print(f"Nodes (statements): {self.graph.number_of_nodes()}")
        print(f"Edges (contradictions): {self.graph.number_of_edges()}")
        print(f"Contradiction threshold: {self.threshold}")

        if self.graph.number_of_edges() == 0:
            print("\n✓ No contradictions detected above threshold")
            logger.info("No contradictions detected above threshold.")
            return

        print(f"\n--- Detected Contradictions ---")
        for u, v, d in self.graph.edges(data=True):
            print(f"\n  Score: {d['weight']:.3f} | Similarity: {d['sim']:.3f}")
            print(f"  A: {u[:80]}...")
            print(f"  B: {v[:80]}...")
            print(f"  Metrics: {d['metrics']}")
            logger.debug(f"Contradiction: Score={d['weight']:.3f}, A='{u[:50]}...', B='{v[:50]}...'")


        clusters = self.detect_clusters()
        if clusters:
            print(f"\n--- Contradiction Clusters ({len(clusters)}) ---")
            for i, cluster in enumerate(clusters, 1):
                print(f"\n  Cluster {i}: {len(cluster)} statements")
                for stmt in cluster:
                    print(f"    • {stmt[:70]}...")
                logger.debug(f"Cluster {i}: {len(cluster)} statements")

        worst = self.get_worst_offenders(3)
        if worst:
            print(f"\n--- Most Contradictory Statements ---")
            for stmt, count in worst:
                print(f"  • {count} contradictions: {stmt[:70]}...")
            logger.debug(f"Top 3 worst offenders: {worst}")

    # ------------------------------------------------------
    def visualize(self, output_path: Optional[Path] = None):
        """
        Generate a visual representation of the contradiction graph.
        Requires matplotlib. Saves to file if output_path provided.
        """
        logger.info("Generating graph visualization.")
        try:
            import matplotlib.pyplot as plt

            pos = nx.spring_layout(self.graph, k=2, iterations=50)

            plt.figure(figsize=(14, 10))

            # Draw nodes
            nx.draw_networkx_nodes(self.graph, pos, node_size=700,
                                  node_color='lightblue', alpha=0.9)

            # Draw edges with width proportional to contradiction score
            edges = self.graph.edges()
            weights = [self.graph[u][v]['weight'] * 3 for u, v in edges]
            nx.draw_networkx_edges(self.graph, pos, width=weights,
                                  alpha=0.6, edge_color='red')

            # Draw labels (truncated)
            labels = {node: node[:30] + "..." if len(node) > 30 else node
                     for node in self.graph.nodes()}
            nx.draw_networkx_labels(self.graph, pos, labels, font_size=8)

            plt.title("Tessrax Contradiction Graph", fontsize=16, fontweight='bold')
            plt.axis('off')
            plt.tight_layout()

            if output_path:
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                logger.info(f"Graph visualization saved to {output_path}")
                print(f"\n✓ Graph visualization saved to {output_path}")
            else:
                plt.show()
                logger.info("Graph visualization displayed.")

        except ImportError:
            print("⚠ matplotlib not installed. Skipping visualization.")
            logger.warning("Matplotlib not installed. Skipping visualization.")
        except Exception as e:
            logger.error(f"Error during visualization: {e}")
            print(f"⚠ Error during visualization: {e}")

# ----------------------------------------------------------
# Demo Harness (Modified to use config and logging)
# ----------------------------------------------------------
if __name__ == "__main__":
    logger.info("Starting demo harness.")
    print("Tessrax Conflict Graph - Demo Harness")
    print("="*60)

    sample_texts = [
        {"text": "In 2020, Acme Corp pledged to cut CO2 emissions 50% by 2030.",
         "source": "Press Release", "timestamp": "2020-01-15"},
        {"text": "In 2024, Acme Corp reported emissions down only 5%.",
         "source": "Annual Report", "timestamp": "2024-03-01"},
        {"text": "Acme Corp achieved record revenue growth of 40% in 2024.",
         "source": "Investor Call", "timestamp": "2024-04-10"},
        {"text": "Acme Corp will eliminate all carbon emissions by 2035.",
         "source": "CEO Interview", "timestamp": "2024-06-01"},
        {"text": "Acme Corp has not reduced carbon emissions significantly.",
         "source": "Third-party Audit", "timestamp": "2024-07-15"},
    ]

    # Initialize ConflictGraph without arguments, it uses CONFIG
    cg = ConflictGraph()
    cg.add_statements(sample_texts)
    cg.compute_edges(verbose=True)
    cg.summary()

    print("\n" + "="*60)
    print("ANCHORING TO LEDGER")
    print("="*60)

    receipt = cg.anchor_to_ledger()
    print("\n--- Ledger Receipt ---")
    print(json.dumps(receipt, indent=2))

    # Visualize if matplotlib available
    try:
        cg.visualize(output_path=Path("data/conflict_graph_demo.png"))
    except:
        pass

    cg.ledger.close()
    logger.info("Demo harness finished.")
    print("\n✓ Demo complete")
## Enhance temporal and numerical processing

### Subtask:
Integrate more robust libraries for parsing temporal expressions and handling numerical units.

**Reasoning**:
Install the required libraries `parsedatetime` and `pint`.


%pip install parsedatetime pint
**Reasoning**:
Import the newly installed libraries and integrate them into the `temporal_contradiction` and `numeric_inconsistency` functions, then include the updated `ConflictGraph` class to use these modified functions.


import parsedatetime as pdt
from pint import UnitRegistry

# Initialize parsedatetime calendar
cal = pdt.Calendar()

# Initialize Pint Unit Registry
ureg = UnitRegistry()
Q_ = ureg.Quantity

# Dummy ContradictionLedger (already defined)
# class ContradictionLedger: ...

# Utility: lightweight confidence estimator (already defined)
# def estimate_confidence(text: str) -> float: ...

# Sentiment Analysis and NER Functions (already defined)
# def perform_sentiment_analysis(text: str) -> Dict[str, Any]: ...
# def perform_ner(text: str) -> List[Dict[str, Any]]: ...


# ----------------------------------------------------------
# Core contradiction metrics (Modified to use parsedatetime and pint)
# ----------------------------------------------------------
def logical_contradiction(a: str, b: str, sentiment_a: Dict[str, Any], sentiment_b: Dict[str, Any]) -> float:
    """Simple negation detection, enhanced with sentiment."""
    a_low, b_low = a.lower(), b.lower()

    # Check for explicit negation patterns
    a_negated = "not" in a_low or "no " in a_low or "never" in a_low
    b_negated = "not" in b_low or "no " in b_low or "never" in b_low

    # Extract core content
    a_clean = re.sub(r"\b(not|no|never)\b", "", a_low).strip()
    b_clean = re.sub(r"\b(not|no|never)\b", "", b_low).strip()

    score = 0.0

    # High score if one is negated and core content matches
    if a_negated != b_negated and a_clean == b_clean:
        score = 1.0
    # Partial match for similar but differently negated statements
    elif a_negated != b_negated:
        from difflib import SequenceMatcher
        similarity = SequenceMatcher(None, a_clean, b_clean).ratio()
        if similarity > 0.7:
            score = similarity

    # --- Sentiment Enhancement ---
    # If sentiment is opposite and confidence is high, slightly increase score
    if sentiment_a and sentiment_b:
        if sentiment_a['label'] != sentiment_b['label'] and \
           sentiment_a['score'] > 0.8 and sentiment_b['score'] > 0.8: # Use a threshold for sentiment confidence
            score = min(1.0, score + 0.1) # Add a small bonus, cap at 1.0

    return score


def numeric_inconsistency(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float:
    """Detect quantitative disagreement (percentage/number), enhanced with NER and Pint."""

    # Use NER to find potential number entities (including units)
    numbers_a = [ent for ent in ner_a if ent['entity_group'] in ['CARDINAL', 'PERCENT']]
    numbers_b = [ent for ent in ner_b if ent['entity_group'] in ['CARDINAL', 'PERCENT']]

    quantities_a = []
    for num_ent in numbers_a:
        try:
            # Attempt to parse with Pint, handling units
            # Simple approach: use the word itself. More robust would look at surrounding text.
            q = ureg(num_ent['word'].replace('%', ' percent')) # Pint understands 'percent'
            quantities_a.append(q)
        except Exception as e:
            logger.debug(f"Could not parse '{num_ent['word']}' as a quantity in statement A: {e}")
            # Fallback to simple float parsing if Pint fails
            try:
                 quantities_a.append(float(num_ent['word'].replace('%', '')) * ureg.dimensionless)
            except:
                 pass


    quantities_b = []
    for num_ent in numbers_b:
        try:
            q = ureg(num_ent['word'].replace('%', ' percent'))
            quantities_b.append(q)
        except Exception as e:
            logger.debug(f"Could not parse '{num_ent['word']}' as a quantity in statement B: {e}")
            try:
                 quantities_b.append(float(num_ent['word'].replace('%', '')) * ureg.dimensionless)
            except:
                 pass


    if not quantities_a or not quantities_b:
        # Fallback to old method if NER/Pint didn't find quantities
        nums_a_fallback = [float(x.strip("%")) for x in re.findall(r"\d+\.?\d*%?", a)]
        nums_b_fallback = [float(x.strip("%")) for x in re.findall(r"\d+\.?\d*%?", b)]
        if not nums_a_fallback or not nums_b_fallback:
             return 0.0
        diff = abs(nums_a_fallback[0] - nums_b_fallback[0])
        ref = max(1e-5, max(nums_a_fallback[0], nums_b_fallback[0]))
        score = diff / ref
        return min(score, 1.0) if score > 0.1 else 0.0


    score = 0.0
    # Simple comparison: compare the first quantity found in each statement after converting to a common unit if possible
    q_a = quantities_a[0]
    q_b = quantities_b[0]

    try:
        # Attempt to convert q_b to the units of q_a
        q_b_converted = q_b.to(q_a.units)
        diff = abs(q_a.magnitude - q_b_converted.magnitude)
        ref = max(1e-5, max(abs(q_a.magnitude), abs(q_b_converted.magnitude)))
        score = diff / ref
    except Exception as e:
        # Units are incompatible, cannot directly compare with Pint
        logger.debug(f"Incompatible units for comparison: {q_a.units} vs {q_b.units} - {e}")
        # Fallback to comparing magnitudes if possible, might be less accurate
        try:
             diff = abs(q_a.magnitude - q_b.magnitude)
             ref = max(1e-5, max(abs(q_a.magnitude), abs(q_b.magnitude)))
             score = diff / ref
        except:
             score = 0.0 # Cannot compare magnitudes


    # Cap at 1.0 and scale: small differences get lower scores
    score = min(score, 1.0) if score > 0.05 else 0.0 # Lower threshold for Pint comparison

    # --- NER Enhancement (still relevant for context) ---
    # Check if the numbers are associated with the same entity type
    shared_entity_type = False
    if ner_a and ner_b:
        entity_types_a = {entity['entity_group'] for entity in ner_a if 'entity_group' in entity and entity['entity_group'] not in ['CARDINAL', 'PERCENT', 'DATE', 'TIME']}
        entity_types_b = {entity['entity_group'] for entity in ner_b if 'entity_group' in entity and entity['entity_group'] not in ['CARDINAL', 'PERCENT', 'DATE', 'TIME']}
        if entity_types_a & entity_types_b: # Check for common non-numeric/temporal entity types
             shared_entity_type = True

    if shared_entity_type and score > 0:
        score = min(1.0, score * 1.2) # Increase weight if numbers relate to same entity type

    return score


def temporal_contradiction(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float:
    """Flag reversed or inconsistent temporal claims, enhanced with parsedatetime and NER."""

    # Use parsedatetime to extract temporal information
    time_tuple_a, parse_status_a = cal.parse(a)
    time_tuple_b, parse_status_b = cal.parse(b)

    # parsedatetime returns a time.struct_time and a status flag (0=failure, 1=date, 2=time, 3=datetime)
    # We care if it successfully parsed *any* temporal info (status > 0)
    date_a = datetime.datetime(*time_tuple_a[:6]) if parse_status_a > 0 else None
    date_b = datetime.datetime(*time_tuple_b[:6]) if parse_status_b > 0 else None

    score = 0.0

    if date_a and date_b:
        # Basic temporal ordering check: is date B significantly before date A if text implies progression?
        # This is a simple heuristic; more complex logic needed for full temporal reasoning.
        time_difference = abs((date_a - date_b).total_seconds())
        # If dates are different but relatively close (e.g., within a few years), could be inconsistency
        if time_difference > 0 and time_difference < (5 * 365 * 24 * 3600): # Within 5 years
             score = 0.4 # Potential inconsistency

        # Check for explicit temporal phrases that imply contradiction (e.g., "completed by 2020" vs "will be completed in 2025")
        # This is harder to capture with just parsedatetime dates; still need some pattern matching or semantic analysis.
        by_pattern_a = bool(re.search(r"\bby\s+\d{4}", a.lower()))
        by_pattern_b = bool(re.search(r"\bby\s+\d{4}", b.lower()))
        in_pattern_a = bool(re.search(r"\bin\s+\d{4}", a.lower()))
        in_pattern_b = bool(re.search(r"\bin\s+\d{4}", b.lower()))

        # Simple case: "by Y1" vs "in Y2" where Y2 > Y1
        if by_pattern_a and in_pattern_b:
             years_a = [int(y) for y in re.findall(r"\bby\s+(\d{4})\b", a.lower())]
             years_b = [int(y) for y in re.findall(r"\bin\s+(\d{4})\b", b.lower())]
             if years_a and years_b and years_b[0] > years_a[0]:
                 score = max(score, 0.6) # Higher score for explicit contradiction

        if by_pattern_b and in_pattern_a:
             years_a = [int(y) for y in re.findall(r"\bin\s+(\d{4})\b", a.lower())]
             years_b = [int(y) for y in re.findall(r"\bby\s+(\d{4})\b", b.lower())]
             if years_a and years_b and years_a[0] > years_b[0]:
                 score = max(score, 0.6)


    # --- NER Enhancement ---
    # If temporal expressions are linked to the same entity or event, increase score
    shared_event_or_entity = False
    if ner_a and ner_b:
         # Check for shared entities (excluding dates/times themselves)
         non_temporal_entities_a = {ent['word'] for ent in ner_a if ent['entity_group'] not in ['DATE', 'TIME']}
         non_temporal_entities_b = {ent['word'] for ent in ner_b if ent['entity_group'] not in ['DATE', 'TIME']}

         if non_temporal_entities_a & non_temporal_entities_b:
             shared_event_or_entity = True # Indicate potential link

    if shared_event_or_entity and score > 0: # Only boost if there's an existing temporal score
        score = min(1.0, score + 0.2) # Add a bonus

    return score


def categorical_mismatch(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float:
    """Simple categorical clash detection using keyword domains, enhanced with NER."""
    # Define domain keywords (already defined)
    domains = {
        "environmental": ["emission", "carbon", "co2", "climate", "renewable", "pollution"],
        "financial": ["profit", "revenue", "loss", "earnings", "dividend"],
        "social": ["diversity", "equality", "workforce", "community"],
        "operational": ["production", "capacity", "efficiency", "output"]
    }

    # Detect which domains each statement belongs to
    def get_domains(text):
        text_lower = text.lower()
        return {name for name, keywords in domains.items()
                if any(k in text_lower for k in keywords)}

    domains_a = get_domains(a)
    domains_b = get_domains(b)

    score = 0.0

    # Potential contradiction if discussing completely different categories but appearing to contradict
    if domains_a and domains_b and not (domains_a & domains_b):
        score = 0.3

    # --- NER Enhancement ---
    # If statements discuss different categories but share the same named entity, slightly increase score
    shared_entity_in_different_categories = False
    if ner_a and ner_b and score > 0: # Only check if there's a categorical mismatch score
        entities_a = {ent['word'] for ent in ner_a}
        entities_b = {ent['word'] for ent in ner_b}
        if entities_a & entities_b: # Check for common entities
            shared_entity_in_different_categories = True

    if shared_entity_in_different_categories:
         score = min(1.0, score + 0.1) # Add a small bonus

    return score

# ----------------------------------------------------------
# Contradiction Graph Builder (Modified compute_edges)
# ----------------------------------------------------------
class ConflictGraph:
    def __init__(self):
        """
        Initialize the conflict graph using parameters from CONFIG.
        """
        logger.info("Initializing ConflictGraph...")
        self.model = SentenceTransformer(CONFIG["model_name"])
        self.threshold = CONFIG["threshold"]
        self.graph = nx.Graph()
        self.ledger = ContradictionLedger(Path(CONFIG["ledger_db_path"]))
        self.metadata = {"created": datetime.datetime.utcnow().isoformat()}
        logger.info(f"ConflictGraph initialized with model: {CONFIG['model_name']} and threshold: {self.threshold}")
        # Initialize pipelines here
        self.sentiment_analyzer = pipeline("sentiment-analysis")
        self.ner_analyzer = pipeline("ner", grouped_entities=True)


    # ------------------------------------------------------
    def add_statements(self, statements: List[Dict[str, Any]]):
        """
        Add statements to the graph.
        Each statement dict should contain: {"text": "...", "source": "...", "timestamp": "..."}.
        Confidence is auto-estimated if not provided.
        Sentiment and NER are added.
        """
        logger.info(f"Adding {len(statements)} statements to the graph.")

        for s in statements:
            if "text" not in s:
                logger.warning("Skipping statement due to missing 'text' field.")
                continue

            # Auto-fill missing fields
            s.setdefault("confidence", estimate_confidence(s["text"]))
            s.setdefault("source", "unknown")
            s.setdefault("timestamp", datetime.datetime.utcnow().isoformat())

            # Add sentiment and NER using initialized pipelines
            try:
                s["sentiment"] = self.sentiment_analyzer(s["text"])[0]
                s["named_entities"] = self.ner_analyzer(s["text"])
                logger.debug(f"Processed sentiment and NER for statement: {s['text'][:50]}...")
            except Exception as e:
                logger.error(f"Error processing sentiment/NER for statement: {s['text'][:50]}... - {e}")
                s["sentiment"] = None
                s["named_entities"] = []


            self.graph.add_node(s["text"], **s)
            logger.debug(f"Added node for statement: {s['text'][:50]}...")

        logger.info(f"Finished adding statements. Total nodes: {self.graph.number_of_nodes()}")


    # ------------------------------------------------------
    def compute_edges(self, verbose: bool = False):
        """
        Compute contradiction edges between all statement pairs.
        Uses semantic similarity to filter, then calculates contradiction metrics.
        Passes sentiment and NER data to metric functions.
        """
        texts = list(self.graph.nodes())
        n = len(texts)
        logger.info(f"Computing edges for {n} statements.")

        if n < 2:
            logger.info("Need at least 2 statements to compute contradictions. Skipping edge computation.")
            if verbose:
                print("Need at least 2 statements to compute contradictions")
            return

        if verbose:
            print(f"Computing embeddings for {n} statements...")
        logger.info("Computing sentence embeddings.")

        embeddings = self.model.encode(texts, convert_to_tensor=True, show_progress_bar=verbose)
        cos_sim = util.cos_sim(embeddings, embeddings).cpu().numpy()

        edges_added = 0
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                text_i = texts[i]
                text_j = texts[j]

                # Get node attributes for sentiment and NER
                sentiment_i = self.graph.nodes[text_i].get("sentiment")
                ner_i = self.graph.nodes[text_i].get("named_entities")
                sentiment_j = self.graph.nodes[text_j].get("sentiment")
                ner_j = self.graph.nodes[text_j].get("named_entities")


                sim = float(cos_sim[i][j])

                # Skip pairs that are too dissimilar (unrelated topics)
                if sim < 0.3:
                    logger.debug(f"Skipping pair (sim={sim:.3f}): '{text_i[:30]}...' vs '{text_j[:30]}...'")
                    continue

                # Compute individual contradiction metrics, passing sentiment and NER
                L = logical_contradiction(text_i, text_j, sentiment_i, sentiment_j)
                N = numeric_inconsistency(text_i, text_j, ner_i, ner_j)
                T = temporal_contradiction(text_i, text_j, ner_i, ner_j)
                C = categorical_mismatch(text_i, text_j, ner_i, ner_j)

                # Get confidence scores
                conf_i = self.graph.nodes[text_i]["confidence"]
                conf_j = self.graph.nodes[text_j]["confidence"]
                avg_confidence = (conf_i + conf_j) / 2

                # Weighted contradiction score
                # Higher weights for logical and numeric contradictions
                contradiction_score = (
                    0.4 * L +  # Logical negation
                    0.3 * N +  # Numerical mismatch
                    0.2 * T +  # Temporal inconsistency
                    0.1 * C    # Categorical clash
                ) * sim * avg_confidence

                if contradiction_score >= self.threshold:
                    self.graph.add_edge(
                        text_i,
                        text_j,
                        weight=round(contradiction_score, 3),
                        sim=round(sim, 3),
                        metrics={"L": round(L, 2), "N": round(N, 2),
                                "T": round(T, 2), "C": round(C, 2)}
                    )
                    edges_added += 1
                    logger.info(f"Added edge with score {contradiction_score:.3f}: '{text_i[:50]}...' vs '{text_j[:50]}...'")


        if verbose:
            print(f"Added {edges_added} contradiction edges (threshold={self.threshold})")
        logger.info(f"Finished computing edges. Added {edges_added} edges.")

    # Rest of the class methods (detect_clusters, get_worst_offenders, export_bundle, anchor_to_ledger, summary, visualize) are unchanged

    # ------------------------------------------------------
    def detect_clusters(self) -> List[List[str]]:
        """
        Find densely connected contradiction clusters.
        Returns list of clusters, each containing contradicting statements.
        """
        logger.info("Detecting contradiction clusters.")
        # Find connected components (groups of mutually contradicting statements)
        comps = nx.connected_components(self.graph)
        clusters = [list(c) for c in comps if len(c) > 1]
        logger.info(f"Detected {len(clusters)} clusters.")
        return clusters

    # ------------------------------------------------------
    def get_worst_offenders(self, top_k: int = 5) -> List[Tuple[str, int]]:
        """Find statements with the most contradictions."""
        logger.info(f"Finding top {top_k} worst offenders.")
        degrees = dict(self.graph.degree())
        sorted_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
        worst = sorted_nodes[:top_k]
        logger.info(f"Found {len(worst)} worst offenders.")
        return worst

    # ------------------------------------------------------
    def export_bundle(self) -> Dict[str, Any]:
        """Export the current graph as a contradiction bundle for the ledger."""
        logger.info("Exporting graph bundle.")
        edges = [
            {
                "s_i": u,
                "s_j": v,
                "score": d["weight"],
                "similarity": d["sim"],
                "breakdown": d["metrics"]
            }
            for u, v, d in self.graph.edges(data=True)
        ]

        clusters = self.detect_clusters()
        worst_offenders = self.get_worst_offenders()

        bundle = {
            "edges": edges,
            "clusters": clusters,
            "worst_offenders": [{"text": text[:100], "contradiction_count": count}
                               for text, count in worst_offenders],
            "stats": {
                "total_statements": self.graph.number_of_nodes(),
                "total_contradictions": self.graph.number_of_edges(),
                "cluster_count": len(clusters)
            },
            "timestamp": datetime.datetime.utcnow().isoformat(),
        }

        bundle_json = json.dumps(bundle, sort_keys=True)
        bundle_hash = hashlib.sha256(bundle_json.encode()).hexdigest()
        bundle["hash"] = "sha256:" + bundle_hash

        logger.info(f"Graph bundle exported with hash: {bundle['hash'][:8]}")
        return {"contradiction_bundle": bundle}

    # ------------------------------------------------------
    def anchor_to_ledger(self) -> Dict[str, Any]:
        """Anchor the contradiction bundle in the Tessrax ContradictionLedger."""
        logger.info("Anchoring bundle to ledger.")
        bundle = self.export_bundle()["contradiction_bundle"]

        receipt = self.ledger.record_contradiction(
            claim_a={"text": "Graph bundle", "id": f"bundle_{bundle['hash'][:8]}"},
            claim_b={"text": f"{bundle['stats']['total_contradictions']} conflicts", "id": "edges"},
            resolution=f"Graph with {bundle['stats']['cluster_count']} contradiction clusters",
            metadata=bundle,
        )

        logger.info(f"Bundle anchored. Receipt: {receipt.get('status', 'N/A')}")
        return receipt

    # ------------------------------------------------------
    def summary(self, verbose: bool = True):
        """Print summary stats."""
        if not verbose:
            return

        logger.info("Generating summary.")
        print(f"\n{'='*60}")
        print(f"CONFLICT GRAPH SUMMARY")
        print(f"{'='*60}")
        print(f"Nodes (statements): {self.graph.number_of_nodes()}")
        print(f"Edges (contradictions): {self.graph.number_of_edges()}")
        print(f"Contradiction threshold: {self.threshold}")

        if self.graph.number_of_edges() == 0:
            print("\n✓ No contradictions detected above threshold")
            logger.info("No contradictions detected above threshold.")
            return

        print(f"\n--- Detected Contradictions ---")
        for u, v, d in self.graph.edges(data=True):
            print(f"\n  Score: {d['weight']:.3f} | Similarity: {d['sim']:.3f}")
            print(f"  A: {u[:80]}...")
            print(f"  B: {v[:80]}...")
            print(f"  Metrics: {d['metrics']}")
            logger.debug(f"Contradiction: Score={d['weight']:.3f}, A='{u[:50]}...', B='{v[:50]}...'")


        clusters = self.detect_clusters()
        if clusters:
            print(f"\n--- Contradiction Clusters ({len(clusters)}) ---")
            for i, cluster in enumerate(clusters, 1):
                print(f"\n  Cluster {i}: {len(cluster)} statements")
                for stmt in cluster:
                    print(f"    • {stmt[:70]}...")
                logger.debug(f"Cluster {i}: {len(cluster)} statements")

        worst = self.get_worst_offenders(3)
        if worst:
            print(f"\n--- Most Contradictory Statements ---")
            for stmt, count in worst:
                print(f"  • {count} contradictions: {stmt[:70]}...")
            logger.debug(f"Top 3 worst offenders: {worst}")


    # ------------------------------------------------------
    def visualize(self, output_path: Optional[Path] = None):
        """
        Generate a visual representation of the contradiction graph.
        Requires matplotlib. Saves to file if output_path provided.
        """
        logger.info("Gener ating graph visualization.")
        try:
            import matplotlib.pyplot as plt

            pos = nx.spring_layout(self.graph, k=2, iterations=50)

            plt.figure(figsize=(14, 10))

            # Draw nodes
            nx.draw_networkx_nodes(self.graph, pos, node_size=700,
                                  node_color='lightblue', alpha=0.9)

            # Draw edges with width proportional to contradiction score
            edges = self.graph.edges()
            weights = [self.graph[u][v]['weight'] * 3 for u, v in edges]
            nx.draw_networkx_edges(self.graph, pos, width=weights,
                                  alpha=0.6, edge_color='red')

            # Draw labels (truncated)
            labels = {node: node[:30] + "..." if len(node) > 30 else node
                     for node in self.graph.nodes()}
            nx.draw_networkx_labels(self.graph, pos, labels, font_size=8)

            plt.title("Tessrax Contradiction Graph", fontsize=16, fontweight='bold')
            plt.axis('off')
            plt.tight_layout()

            if output_path:
                # Ensure the directory exists
                output_path.parent.mkdir(parents=True, exist_ok=True)
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                logger.info(f"Graph visualization saved to {output_path}")
                print(f"\n✓ Graph visualization saved to {output_path}")
            else:
                plt.show()
                logger.info("Graph visualization displayed.")

        except ImportError:
            print("⚠ matplotlib not installed. Skipping visualization.")
            logger.warning("Matplotlib not installed. Skipping visualization.")
        except Exception as e:
            logger.error(f"Error during visualization: {e}")
            print(f"⚠ Error during visualization: {e}")

# ----------------------------------------------------------
# Demo Harness (Modified to use config and logging)
# ----------------------------------------------------------
if __name__ == "__main__":
    logger.info("Starting demo harness.")
    print("Tessrax Conflict Graph - Demo Harness")
    print("="*60)

    sample_texts = [
        {"text": "In 2020, Acme Corp pledged to cut CO2 emissions 50% by 2030.",
         "source": "Press Release", "timestamp": "2020-01-15"},
        {"text": "In 2024, Acme Corp reported emissions down only 5%.",
         "source": "Annual Report", "timestamp": "2024-03-01"},
        {"text": "Acme Corp achieved record revenue growth of 40% in 2024.",
         "source": "Investor Call", "timestamp": "2024-04-10"},
        {"text": "Acme Corp will eliminate all carbon emissions by 2035.",
         "source": "CEO Interview", "timestamp": "2024-06-01"},
        {"text": "Acme Corp has not reduced carbon emissions significantly.",
         "source": "Third-party Audit", "timestamp": "2024-07-15"},
        {"text": "We completed 90% of the project by December 2022.", # New statement for temporal test
         "source": "Project Report", "timestamp": "2023-01-01"},
        {"text": "The project is only 10% complete as of January 2023.", # New statement for temporal and numeric test
         "source": "Project Update", "timestamp": "2023-01-05"},
         {"text": "Our profit was 10 million dollars in Q1.", # New statement for numeric test
         "source": "Financial Report", "timestamp": "2024-05-01"},
        {"text": "The company made 10,000,000 USD in the first quarter.", # New statement for numeric test (different units/format)
         "source": "Earnings Call", "timestamp": "2024-05-02"},

    ]

    # Initialize ConflictGraph without arguments, it uses CONFIG
    cg = ConflictGraph()
    cg.add_statements(sample_texts)
    cg.compute_edges(verbose=True)
    cg.summary()

    print("\n" + "="*60)
    print("ANCHORING TO LEDGER")
    print("="*60)

    receipt = cg.anchor_to_ledger()
    print("\n--- Ledger Receipt ---")
    print(json.dumps(receipt, indent=2))

    # Visualize if matplotlib available
    try:
        cg.visualize(output_path=Path("data/conflict_graph_demo.png"))
    except Exception as e:
        logger.error(f"Visualization failed: {e}")
        pass # Handle the error gracefully

    cg.ledger.close()
    logger.info("Demo harness finished.")
    print("\n✓ Demo complete")
## Implement advanced graph analysis

### Subtask:
Add functions for community detection, centrality measures, and path analysis to the `ConflictGraph` class.

**Reasoning**:
Add the necessary imports for community detection, centrality measures, and path analysis from networkx.


from networkx.algorithms import community
from networkx.algorithms.centrality import degree_centrality, betweenness_centrality
from networkx.algorithms.shortest_paths.weighted import shortest_path
**Reasoning**:
The import for `shortest_path` failed. I will try importing `shortest_path` directly from `networkx.algorithms.shortest_paths`.


from networkx.algorithms import community
from networkx.algorithms.centrality import degree_centrality, betweenness_centrality
from networkx.algorithms.shortest_paths import shortest_path
**Reasoning**:
Now that the necessary modules are imported, I will add the methods for community detection, centrality measures, and finding contradiction paths to the `ConflictGraph` class, and update the `summary` method to include this information.


import parsedatetime as pdt
from pint import UnitRegistry

# Initialize parsedatetime calendar (already defined)
# cal = pdt.Calendar()

# Initialize Pint Unit Registry (already defined)
# ureg = UnitRegistry()
# Q_ = ureg.Quantity

# Dummy ContradictionLedger (already defined)
# class ContradictionLedger: ...

# Utility: lightweight confidence estimator (already defined)
# def estimate_confidence(text: str) -> float: ...

# Core contradiction metrics (already defined)
# def logical_contradiction(a: str, b: str, sentiment_a: Dict[str, Any], sentiment_b: Dict[str, Any]) -> float: ...
# def numeric_inconsistency(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float: ...
# def temporal_contradiction(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float: ...
# def categorical_mismatch(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float: ...

# Sentiment Analysis and NER Functions (already defined)
# def perform_sentiment_analysis(text: str) -> Dict[str, Any]: ...
# def perform_ner(text: str) -> List[Dict[str, Any]]: ...

# Configuration and Logging (already defined)
# CONFIG = { ... }
# logging.basicConfig(...)
# logger = logging.getLogger(__name__)


# ----------------------------------------------------------
# Contradiction Graph Builder (Modified with new methods and summary update)
# ----------------------------------------------------------
class ConflictGraph:
    def __init__(self):
        """
        Initialize the conflict graph using parameters from CONFIG.
        """
        logger.info("Initializing ConflictGraph...")
        self.model = SentenceTransformer(CONFIG["model_name"])
        self.threshold = CONFIG["threshold"]
        self.graph = nx.Graph()
        self.ledger = ContradictionLedger(Path(CONFIG["ledger_db_path"]))
        self.metadata = {"created": datetime.datetime.utcnow().isoformat()}
        logger.info(f"ConflictGraph initialized with model: {CONFIG['model_name']} and threshold: {self.threshold}")
        # Initialize pipelines here
        self.sentiment_analyzer = pipeline("sentiment-analysis")
        self.ner_analyzer = pipeline("ner", grouped_entities=True)


    # ------------------------------------------------------
    def add_statements(self, statements: List[Dict[str, Any]]):
        """
        Add statements to the graph.
        Each statement dict should contain: {"text": "...", "source": "...", "timestamp": "..."}.
        Confidence is auto-estimated if not provided.
        Sentiment and NER are added.
        """
        logger.info(f"Adding {len(statements)} statements to the graph.")

        for s in statements:
            if "text" not in s:
                logger.warning("Skipping statement due to missing 'text' field.")
                continue

            # Auto-fill missing fields
            s.setdefault("confidence", estimate_confidence(s["text"]))
            s.setdefault("source", "unknown")
            s.setdefault("timestamp", datetime.datetime.utcnow().isoformat())

            # Add sentiment and NER using initialized pipelines
            try:
                s["sentiment"] = self.sentiment_analyzer(s["text"])[0]
                s["named_entities"] = self.ner_analyzer(s["text"])
                logger.debug(f"Processed sentiment and NER for statement: {s['text'][:50]}...")
            except Exception as e:
                logger.error(f"Error processing sentiment/NER for statement: {s['text'][:50]}... - {e}")
                s["sentiment"] = None
                s["named_entities"] = []


            self.graph.add_node(s["text"], **s)
            logger.debug(f"Added node for statement: {s['text'][:50]}...")

        logger.info(f"Finished adding statements. Total nodes: {self.graph.number_of_nodes()}")


    # ------------------------------------------------------
    def compute_edges(self, verbose: bool = False):
        """
        Compute contradiction edges between all statement pairs.
        Uses semantic similarity to filter, then calculates contradiction metrics.
        Passes sentiment and NER data to metric functions.
        """
        texts = list(self.graph.nodes())
        n = len(texts)
        logger.info(f"Computing edges for {n} statements.")

        if n < 2:
            logger.info("Need at least 2 statements to compute contradictions. Skipping edge computation.")
            if verbose:
                print("Need at least 2 statements to compute contradictions")
            return

        if verbose:
            print(f"Computing embeddings for {n} statements...")
        logger.info("Computing sentence embeddings.")

        embeddings = self.model.encode(texts, convert_to_tensor=True, show_progress_bar=verbose)
        cos_sim = util.cos_sim(embeddings, embeddings).cpu().numpy()

        edges_added = 0
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                text_i = texts[i]
                text_j = texts[j]

                # Get node attributes for sentiment and NER
                sentiment_i = self.graph.nodes[text_i].get("sentiment")
                ner_i = self.graph.nodes[text_i].get("named_entities")
                sentiment_j = self.graph.nodes[text_j].get("sentiment")
                ner_j = self.graph.nodes[text_j].get("named_entities")


                sim = float(cos_sim[i][j])

                # Skip pairs that are too dissimilar (unrelated topics)
                if sim < 0.3:
                    logger.debug(f"Skipping pair (sim={sim:.3f}): '{text_i[:30]}...' vs '{text_j[:30]}...'")
                    continue

                # Compute individual contradiction metrics, passing sentiment and NER
                L = logical_contradiction(text_i, text_j, sentiment_i, sentiment_j)
                N = numeric_inconsistency(text_i, text_j, ner_i, ner_j)
                T = temporal_contradiction(text_i, text_j, ner_i, ner_j)
                C = categorical_mismatch(text_i, text_j, ner_i, ner_j)

                # Get confidence scores
                conf_i = self.graph.nodes[text_i]["confidence"]
                conf_j = self.graph.nodes[text_j]["confidence"]
                avg_confidence = (conf_i + conf_j) / 2

                # Weighted contradiction score
                # Higher weights for logical and numeric contradictions
                contradiction_score = (
                    0.4 * L +  # Logical negation
                    0.3 * N +  # Numerical mismatch
                    0.2 * T +  # Temporal inconsistency
                    0.1 * C    # Categorical clash
                ) * sim * avg_confidence

                if contradiction_score >= self.threshold:
                    self.graph.add_edge(
                        text_i,
                        text_j,
                        weight=round(contradiction_score, 3),
                        sim=round(sim, 3),
                        metrics={"L": round(L, 2), "N": round(N, 2),
                                "T": round(T, 2), "C": round(C, 2)}
                    )
                    edges_added += 1
                    logger.info(f"Added edge with score {contradiction_score:.3f}: '{text_i[:50]}...' vs '{text_j[:50]}...'")


        if verbose:
            print(f"Added {edges_added} contradiction edges (threshold={self.threshold})")
        logger.info(f"Finished computing edges. Added {edges_added} edges.")

    # ------------------------------------------------------
    def detect_clusters(self) -> List[List[str]]:
        """
        Find densely connected contradiction clusters (connected components).
        Returns list of clusters, each containing contradicting statements.
        """
        logger.info("Detecting contradiction clusters (connected components).")
        # Find connected components (groups of mutually contradicting statements)
        comps = list(nx.connected_components(self.graph))
        clusters = [list(c) for c in comps if len(c) > 1]
        logger.info(f"Detected {len(clusters)} clusters.")
        return clusters

    # ------------------------------------------------------
    def detect_communities(self) -> List[List[str]]:
        """
        Detect communities within the graph using the Louvain method.
        Returns a list of communities, each being a list of nodes.
        Requires the python-louvain package (community).
        """
        logger.info("Detecting communities using Louvain method.")
        if self.graph.number_of_edges() == 0:
            logger.info("No edges in graph, skipping community detection.")
            return []

        try:
            # Convert graph to undirected if it's not already (Louvain works on undirected)
            ugraph = self.graph.to_undirected()
            # Remove self-loops if any
            ugraph.remove_edges_from(nx.selfloop_edges(ugraph))

            # Use the Louvain method
            communities_generator = community.louvain_communities(ugraph, weight='weight')
            communities_list = [list(c) for c in communities_generator]
            logger.info(f"Detected {len(communities_list)} communities.")
            return communities_list
        except ImportError:
            logger.warning("python-louvain package not installed. Cannot perform community detection.")
            print("⚠ python-louvain package not installed. Cannot perform community detection.")
            return []
        except Exception as e:
            logger.error(f"Error during community detection: {e}")
            print(f"⚠ Error during community detection: {e}")
            return []


    # ------------------------------------------------------
    def calculate_centrality(self) -> Dict[str, Dict[str, float]]:
        """
        Calculate centrality measures (degree and betweenness) for nodes.
        Returns a dictionary with centrality scores for each node.
        """
        logger.info("Calculating centrality measures.")
        if self.graph.number_of_nodes() == 0:
            logger.info("No nodes in graph, skipping centrality calculation.")
            return {}

        try:
            degree_c = degree_centrality(self.graph)
            betweenness_c = betweenness_centrality(self.graph, weight='weight')

            centrality_scores = {}
            for node in self.graph.nodes():
                centrality_scores[node] = {
                    "degree": degree_c.get(node, 0.0),
                    "betweenness": betweenness_c.get(node, 0.0)
                }
            logger.info("Centrality measures calculated.")
            return centrality_scores
        except Exception as e:
            logger.error(f"Error during centrality calculation: {e}")
            print(f"⚠ Error during centrality calculation: {e}")
            return {}

    # ------------------------------------------------------
    def find_contradiction_paths(self, statement_a: str, statement_b: str, weight: str = 'weight') -> List[List[str]]:
        """
        Find shortest paths between two statements in the graph.
        Returns a list of paths (list of nodes).
        Paths represent chains of contradictions linking statement_a to statement_b.
        """
        logger.info(f"Finding paths between '{statement_a[:50]}...' and '{statement_b[:50]}...'.")
        if statement_a not in self.graph or statement_b not in self.graph:
            logger.warning("One or both statements not found in the graph.")
            print("⚠ One or both statements not found in the graph.")
            return []

        try:
            # Find all shortest paths (could be multiple with the same minimum weight)
            # Using all_shortest_paths for potentially multiple paths
            paths = list(shortest_path(self.graph, source=statement_a, target=statement_b, weight=weight))
            logger.info(f"Found {len(paths)} shortest paths.")
            return paths
        except nx.NetworkXNoPath:
            logger.info("No path found between the two statements.")
            print("✓ No path found between the two statements.")
            return []
        except Exception as e:
            logger.error(f"Error finding paths: {e}")
            print(f"⚠ Error finding paths: {e}")
            return []


    # ------------------------------------------------------
    def get_worst_offenders(self, top_k: int = 5) -> List[Tuple[str, int]]:
        """Find statements with the most contradictions (highest degree)."""
        logger.info(f"Finding top {top_k} worst offenders (by degree).")
        degrees = dict(self.graph.degree())
        sorted_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
        worst = sorted_nodes[:top_k]
        logger.info(f"Found {len(worst)} worst offenders.")
        return worst

    # ------------------------------------------------------
    def export_bundle(self) -> Dict[str, Any]:
        """Export the current graph as a contradiction bundle for the ledger."""
        logger.info("Exporting graph bundle.")
        edges = [
            {
                "s_i": u,
                "s_j": v,
                "score": d["weight"],
                "similarity": d["sim"],
                "breakdown": d["metrics"]
            }
            for u, v, d in self.graph.edges(data=True)
        ]

        clusters = self.detect_clusters()
        worst_offenders = self.get_worst_offenders()
        communities = self.detect_communities()
        centrality = self.calculate_centrality()

        bundle = {
            "edges": edges,
            "clusters": clusters,
            "communities": communities,
            "worst_offenders": [{"text": text[:100], "contradiction_count": count}
                               for text, count in worst_offenders],
            "centrality_scores": centrality,
            "stats": {
                "total_statements": self.graph.number_of_nodes(),
                "total_contradictions": self.graph.number_of_edges(),
                "cluster_count": len(clusters),
                "community_count": len(communities)
            },
            "timestamp": datetime.datetime.utcnow().isoformat(),
        }

        bundle_json = json.dumps(bundle, sort_keys=True)
        bundle_hash = hashlib.sha256(bundle_json.encode()).hexdigest()
        bundle["hash"] = "sha256:" + bundle_hash

        logger.info(f"Graph bundle exported with hash: {bundle['hash'][:8]}")
        return {"contradiction_bundle": bundle}

    # ------------------------------------------------------
    def anchor_to_ledger(self) -> Dict[str, Any]:
        """Anchor the contradiction bundle in the Tessrax ContradictionLedger."""
        logger.info("Anchoring bundle to ledger.")
        bundle = self.export_bundle()["contradiction_bundle"]

        receipt = self.ledger.record_contradiction(
            claim_a={"text": "Graph bundle", "id": f"bundle_{bundle['hash'][:8]}"},
            claim_b={"text": f"{bundle['stats']['total_contradictions']} conflicts", "id": "edges"},
            resolution=f"Graph with {bundle['stats']['cluster_count']} contradiction clusters and {bundle['stats']['community_count']} communities",
            metadata=bundle,
        )

        logger.info(f"Bundle anchored. Receipt: {receipt.get('status', 'N/A')}")
        return receipt

    # ------------------------------------------------------
    def summary(self, verbose: bool = True):
        """Print summary stats, including communities and centrality."""
        if not verbose:
            return

        logger.info("Generating summary.")
        print(f"\n{'='*60}")
        print(f"CONFLICT GRAPH SUMMARY")
        print(f"{'='*60}")
        print(f"Nodes (statements): {self.graph.number_of_nodes()}")
        print(f"Edges (contradictions): {self.graph.number_of_edges()}")
        print(f"Contradiction threshold: {self.threshold}")

        if self.graph.number_of_nodes() == 0:
             print("\n✓ No statements in the graph.")
             logger.info("No statements in the graph.")
             return

        if self.graph.number_of_edges() == 0:
            print("\n✓ No contradictions detected above threshold")
            logger.info("No contradictions detected above threshold.")
        else:
            print(f"\n--- Detected Contradictions ---")
            for u, v, d in self.graph.edges(data=True):
                print(f"\n  Score: {d['weight']:.3f} | Similarity: {d['sim']:.3f}")
                print(f"  A: {u[:80]}...")
                print(f"  B: {v[:80]}...")
                print(f"  Metrics: {d['metrics']}")
                logger.debug(f"Contradiction: Score={d['weight']:.3f}, A='{u[:50]}...', B='{v[:50]}...'")


        clusters = self.detect_clusters()
        if clusters:
            print(f"\n--- Contradiction Clusters (Connected Components - {len(clusters)}) ---")
            for i, cluster in enumerate(clusters, 1):
                print(f"\n  Cluster {i}: {len(cluster)} statements")
                for stmt in cluster:
                    print(f"    • {stmt[:70]}...")
                logger.debug(f"Cluster {i}: {len(cluster)} statements")

        communities = self.detect_communities()
        if communities:
            print(f"\n--- Detected Communities ({len(communities)}) ---")
            # Sort communities by size (largest first) for easier reading
            sorted_communities = sorted(communities, key=len, reverse=True)
            for i, comm in enumerate(sorted_communities, 1):
                print(f"\n  Community {i}: {len(comm)} statements")
                # Print first few statements from the community
                for stmt in comm[:5]: # Print only up to the first 5 statements
                     print(f"    • {stmt[:70]}...")
                if len(comm) > 5:
                    print("    ...")
                logger.debug(f"Community {i}: {len(comm)} statements")


        centrality = self.calculate_centrality()
        if centrality:
            print(f"\n--- Node Centrality ---")
            # Sort nodes by degree centrality for display
            sorted_by_degree = sorted(centrality.items(), key=lambda item: item[1]['degree'], reverse=True)
            print("\n  Top 5 by Degree Centrality:")
            for node, scores in sorted_by_degree[:5]:
                 print(f"    • {node[:70]}... : Degree={scores['degree']:.3f}, Betweenness={scores['betweenness']:.3f}")
            logger.debug(f"Top 5 by Degree: {sorted_by_degree[:5]}")

            # Sort nodes by betweenness centrality for display
            sorted_by_betweenness = sorted(centrality.items(), key=lambda item: item[1]['betweenness'], reverse=True)
            print("\n  Top 5 by Betweenness Centrality:")
            for node, scores in sorted_by_betweenness[:5]:
                 print(f"    • {node[:70]}... : Degree={scores['degree']:.3f}, Betweenness={scores['betweenness']:.3f}")
            logger.debug(f"Top 5 by Betweenness: {sorted_by_betweenness[:5]}")


        worst = self.get_worst_offenders(3)
        if worst:
            print(f"\n--- Most Contradictory Statements (by Degree) ---")
            for stmt, count in worst:
                print(f"  • {count} contradictions: {stmt[:70]}...")
            logger.debug(f"Top 3 worst offenders: {worst}")

    # ------------------------------------------------------
    def visualize(self, output_path: Optional[Path] = None):
        """
        Generate a visual representation of the contradiction graph.
        Requires matplotlib. Saves to file if output_path provided.
        """
        logger.info("Gener ating graph visualization.")
        try:
            import matplotlib.pyplot as plt

            pos = nx.spring_layout(self.graph, k=2, iterations=50)

            plt.figure(figsize=(14, 10))

            # Draw nodes
            nx.draw_networkx_nodes(self.graph, pos, node_size=700,
                                  node_color='lightblue', alpha=0.9)

            # Draw edges with width proportional to contradiction score
            edges = self.graph.edges()
            weights = [self.graph[u][v]['weight'] * 3 for u, v in edges]
            nx.draw_networkx_edges(self.graph, pos, width=weights,
                                  alpha=0.6, edge_color='red')

            # Draw labels (truncated)
            labels = {node: node[:30] + "..." if len(node) > 30 else node
                     for node in self.graph.nodes()}
            nx.draw_networkx_labels(self.graph, pos, labels, font_size=8)

            plt.title("Tessrax Contradiction Graph", fontsize=16, fontweight='bold')
            plt.axis('off')
            plt.tight_layout()

            if output_path:
                # Ensure the directory exists
                output_path.parent.mkdir(parents=True, exist_ok=True)
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                logger.info(f"Graph visualization saved to {output_path}")
                print(f"\n✓ Graph visualization saved to {output_path}")
            else:
                plt.show()
                logger.info("Graph visualization displayed.")

        except ImportError:
            print("⚠ matplotlib not installed. Skipping visualization.")
            logger.warning("Matplotlib not installed. Skipping visualization.")
        except Exception as e:
            logger.error(f"Error during visualization: {e}")
            print(f"⚠ Error during visualization: {e}")

# ----------------------------------------------------------
# Demo Harness (Modified to use config and logging)
# ----------------------------------------------------------
if __name__ == "__main__":
    logger.info("Starting demo harness.")
    print("Tessrax Conflict Graph - Demo Harness")
    print("="*60)

    sample_texts = [
        {"text": "In 2020, Acme Corp pledged to cut CO2 emissions 50% by 2030.",
         "source": "Press Release", "timestamp": "2020-01-15"},
        {"text": "In 2024, Acme Corp reported emissions down only 5%.",
         "source": "Annual Report", "timestamp": "2024-03-01"},
        {"text": "Acme Corp achieved record revenue growth of 40% in 2024.",
         "source": "Investor Call", "timestamp": "2024-04-10"},
        {"text": "Acme Corp will eliminate all carbon emissions by 2035.",
         "source": "CEO Interview", "timestamp": "2024-06-01"},
        {"text": "Acme Corp has not reduced carbon emissions significantly.",
         "source": "Third-party Audit", "timestamp": "2024-07-15"},
        {"text": "We completed 90% of the project by December 2022.", # New statement for temporal test
         "source": "Project Report", "timestamp": "2023-01-01"},
        {"text": "The project is only 10% complete as of January 2023.", # New statement for temporal and numeric test
         "source": "Project Update", "timestamp": "2023-01-05"},
         {"text": "Our profit was 10 million dollars in Q1.", # New statement for numeric test
         "source": "Financial Report", "timestamp": "2024-05-01"},
        {"text": "The company made 10,000,000 USD in the first quarter.", # New statement for numeric test (different units/format)
         "source": "Earnings Call", "timestamp": "2024-05-02"},

    ]

    # Initialize ConflictGraph without arguments, it uses CONFIG
    cg = ConflictGraph()
    cg.add_statements(sample_texts)
    cg.compute_edges(verbose=True)
    cg.summary()

    print("\n" + "="*60)
    print("ANCHORING TO LEDGER")
    print("="*60)

    receipt = cg.anchor_to_ledger()
    print("\n--- Ledger Receipt ---")
    print(json.dumps(receipt, indent=2))

    # Demo path finding
    stmt1 = sample_texts[0]["text"] # "In 2020, Acme Corp pledged to cut CO2 emissions 50% by 2030."
    stmt2 = sample_texts[4]["text"] # "Acme Corp has not reduced carbon emissions significantly."
    print(f"\n--- Finding paths between statements ---")
    print(f"Statement A: {stmt1[:70]}...")
    print(f"Statement B: {stmt2[:70]}...")
    paths = cg.find_contradiction_paths(stmt1, stmt2)
    if paths:
        print("\nFound paths:")
        for i, path in enumerate(paths, 1):
            print(f"  Path {i}:")
            for j, node in enumerate(path):
                print(f"    {j+1}. {node[:70]}...")
    else:
        print("\nNo path found between these statements.")


    # Visualize if matplotlib available
    try:
        cg.visualize(output_path=Path("data/conflict_graph_demo.png"))
    except Exception as e:
        logger.error(f"Visualization failed: {e}")
        pass # Handle the error gracefully

    cg.ledger.close()
    logger.info("Demo harness finished.")
    print("\n✓ Demo complete")
## Improve visualization

### Subtask:
Enhance the visualization with interactive features and the display of node/edge attributes.

**Reasoning**:
Implement the `visualize_interactive` function within the `ConflictGraph` class, including the creation of node and edge traces with hover text, and modify the demo harness to call this new visualization method.


import plotly.graph_objects as go
import plotly.io as pio
import networkx as nx
import re
import json
import time
import hashlib
import datetime
import numpy as np
from pathlib import Path
from typing import List, Dict, Any, Tuple, Optional
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
import parsedatetime as pdt
from pint import UnitRegistry
import logging
import sys
from networkx.algorithms import community
from networkx.algorithms.centrality import degree_centrality, betweenness_centrality
from networkx.algorithms.shortest_paths import shortest_path

# Initialize parsedatetime calendar (already defined)
# cal = pdt.Calendar()

# Initialize Pint Unit Registry (already defined)
# ureg = UnitRegistry()
# Q_ = ureg.Quantity

# Dummy ContradictionLedger (already defined)
# class ContradictionLedger: ...

# Utility: lightweight confidence estimator (already defined)
# def estimate_confidence(text: str) -> float: ...

# Core contradiction metrics (already defined)
# def logical_contradiction(a: str, b: str, sentiment_a: Dict[str, Any], sentiment_b: Dict[str, Any]) -> float: ...
# def numeric_inconsistency(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float: ...
# def temporal_contradiction(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float: ...
# def categorical_mismatch(a: str, b: str, ner_a: List[Dict[str, Any]], ner_b: List[Dict[str, Any]]) -> float: ...

# Sentiment Analysis and NER Functions (already defined)
# def perform_sentiment_analysis(text: str) -> Dict[str, Any]: ...
# def perform_ner(text: str) -> List[Dict[str, Any]]: ...

# Configuration and Logging (already defined)
# CONFIG = { ... }
# logging.basicConfig(...)
# logger = logging.getLogger(__name__)


# ----------------------------------------------------------
# Contradiction Graph Builder (Modified with interactive visualization)
# ----------------------------------------------------------
class ConflictGraph:
    def __init__(self):
        """
        Initialize the conflict graph using parameters from CONFIG.
        """
        logger.info("Initializing ConflictGraph...")
        self.model = SentenceTransformer(CONFIG["model_name"])
        self.threshold = CONFIG["threshold"]
        self.graph = nx.Graph()
        self.ledger = ContradictionLedger(Path(CONFIG["ledger_db_path"]))
        self.metadata = {"created": datetime.datetime.utcnow().isoformat()}
        logger.info(f"ConflictGraph initialized with model: {CONFIG['model_name']} and threshold: {self.threshold}")
        # Initialize pipelines here
        self.sentiment_analyzer = pipeline("sentiment-analysis")
        self.ner_analyzer = pipeline("ner", grouped_entities=True)


    # ------------------------------------------------------
    def add_statements(self, statements: List[Dict[str, Any]]):
        """
        Add statements to the graph.
        Each statement dict should contain: {"text": "...", "source": "...", "timestamp": "..."}.
        Confidence is auto-estimated if not provided.
        Sentiment and NER are added.
        """
        logger.info(f"Adding {len(statements)} statements to the graph.")

        for s in statements:
            if "text" not in s:
                logger.warning("Skipping statement due to missing 'text' field.")
                continue

            # Auto-fill missing fields
            s.setdefault("confidence", estimate_confidence(s["text"]))
            s.setdefault("source", "unknown")
            s.setdefault("timestamp", datetime.datetime.utcnow().isoformat())

            # Add sentiment and NER using initialized pipelines
            try:
                s["sentiment"] = self.sentiment_analyzer(s["text"])[0]
                s["named_entities"] = self.ner_analyzer(s["text"])
                logger.debug(f"Processed sentiment and NER for statement: {s['text'][:50]}...")
            except Exception as e:
                logger.error(f"Error processing sentiment/NER for statement: {s['text'][:50]}... - {e}")
                s["sentiment"] = None
                s["named_entities"] = []


            self.graph.add_node(s["text"], **s)
            logger.debug(f"Added node for statement: {s['text'][:50]}...")

        logger.info(f"Finished adding statements. Total nodes: {self.graph.number_of_nodes()}")


    # ------------------------------------------------------
    def compute_edges(self, verbose: bool = False):
        """
        Compute contradiction edges between all statement pairs.
        Uses semantic similarity to filter, then calculates contradiction metrics.
        Passes sentiment and NER data to metric functions.
        """
        texts = list(self.graph.nodes())
        n = len(texts)
        logger.info(f"Computing edges for {n} statements.")

        if n < 2:
            logger.info("Need at least 2 statements to compute contradictions. Skipping edge computation.")
            if verbose:
                print("Need at least 2 statements to compute contradictions")
            return

        if verbose:
            print(f"Computing embeddings for {n} statements...")
        logger.info("Computing sentence embeddings.")

        embeddings = self.model.encode(texts, convert_to_tensor=True, show_progress_bar=verbose)
        cos_sim = util.cos_sim(embeddings, embeddings).cpu().numpy()

        edges_added = 0
        for i in range(len(texts)):
            for j in range(i + 1, len(texts)):
                text_i = texts[i]
                text_j = texts[j]

                # Get node attributes for sentiment and NER
                sentiment_i = self.graph.nodes[text_i].get("sentiment")
                ner_i = self.graph.nodes[text_i].get("named_entities")
                sentiment_j = self.graph.nodes[text_j].get("sentiment")
                ner_j = self.graph.nodes[text_j].get("named_entities")


                sim = float(cos_sim[i][j])

                # Skip pairs that are too dissimilar (unrelated topics)
                if sim < 0.3:
                    logger.debug(f"Skipping pair (sim={sim:.3f}): '{text_i[:30]}...' vs '{text_j[:30]}...'")
                    continue

                # Compute individual contradiction metrics, passing sentiment and NER
                L = logical_contradiction(text_i, text_j, sentiment_i, sentiment_j)
                N = numeric_inconsistency(text_i, text_j, ner_i, ner_j)
                T = temporal_contradiction(text_i, text_j, ner_i, ner_j)
                C = categorical_mismatch(text_i, text_j, ner_i, ner_j)

                # Get confidence scores
                conf_i = self.graph.nodes[text_i]["confidence"]
                conf_j = self.graph.nodes[text_j]["confidence"]
                avg_confidence = (conf_i + conf_j) / 2

                # Weighted contradiction score
                # Higher weights for logical and numeric contradictions
                contradiction_score = (
                    0.4 * L +  # Logical negation
                    0.3 * N +  # Numerical mismatch
                    0.2 * T +  # Temporal inconsistency
                    0.1 * C    # Categorical clash
                ) * sim * avg_confidence

                if contradiction_score >= self.threshold:
                    self.graph.add_edge(
                        text_i,
                        text_j,
                        weight=round(contradiction_score, 3),
                        sim=round(sim, 3),
                        metrics={"L": round(L, 2), "N": round(N, 2),
                                "T": round(T, 2), "C": round(C, 2)}
                    )
                    edges_added += 1
                    logger.info(f"Added edge with score {contradiction_score:.3f}: '{text_i[:50]}...' vs '{text_j[:50]}...'")


        if verbose:
            print(f"Added {edges_added} contradiction edges (threshold={self.threshold})")
        logger.info(f"Finished computing edges. Added {edges_added} edges.")

    # ------------------------------------------------------
    def detect_clusters(self) -> List[List[str]]:
        """
        Find densely connected contradiction clusters (connected components).
        Returns list of clusters, each containing contradicting statements.
        """
        logger.info("Detecting contradiction clusters (connected components).")
        # Find connected components (groups of mutually contradicting statements)
        comps = list(nx.connected_components(self.graph))
        clusters = [list(c) for c in comps if len(c) > 1]
        logger.info(f"Detected {len(clusters)} clusters.")
        return clusters

    # ------------------------------------------------------
    def detect_communities(self) -> List[List[str]]:
        """
        Detect communities within the graph using the Louvain method.
        Returns a list of communities, each being a list of nodes.
        Requires the python-louvain package (community).
        """
        logger.info("Detecting communities using Louvain method.")
        if self.graph.number_of_edges() == 0:
            logger.info("No edges in graph, skipping community detection.")
            return []

        try:
            # Convert graph to undirected if it's not already (Louvain works on undirected)
            ugraph = self.graph.to_undirected()
            # Remove self-loops if any
            ugraph.remove_edges_from(nx.selfloop_edges(ugraph))

            # Use the Louvain method
            communities_generator = community.louvain_communities(ugraph, weight='weight')
            communities_list = [list(c) for c in communities_generator]
            logger.info(f"Detected {len(communities_list)} communities.")
            return communities_list
        except ImportError:
            logger.warning("python-louvain package not installed. Cannot perform community detection.")
            print("⚠ python-louvain package not installed. Cannot perform community detection.")
            return []
        except Exception as e:
            logger.error(f"Error during community detection: {e}")
            print(f"⚠ Error during community detection: {e}")
            return []


    # ------------------------------------------------------
    def calculate_centrality(self) -> Dict[str, Dict[str, float]]:
        """
        Calculate centrality measures (degree and betweenness) for nodes.
        Returns a dictionary with centrality scores for each node.
        """
        logger.info("Calculating centrality measures.")
        if self.graph.number_of_nodes() == 0:
            logger.info("No nodes in graph, skipping centrality calculation.")
            return {}

        try:
            degree_c = degree_centrality(self.graph)
            betweenness_c = betweenness_centrality(self.graph, weight='weight')

            centrality_scores = {}
            for node in self.graph.nodes():
                centrality_scores[node] = {
                    "degree": degree_c.get(node, 0.0),
                    "betweenness": betweenness_c.get(node, 0.0)
                }
            logger.info("Centrality measures calculated.")
            return centrality_scores
        except Exception as e:
            logger.error(f"Error during centrality calculation: {e}")
            print(f"⚠ Error during centrality calculation: {e}")
            return {}

    # ------------------------------------------------------
    def find_contradiction_paths(self, statement_a: str, statement_b: str, weight: str = 'weight') -> List[List[str]]:
        """
        Find shortest paths between two statements in the graph.
        Returns a list of paths (list of nodes).
        Paths represent chains of contradictions linking statement_a to statement_b.
        """
        logger.info(f"Finding paths between '{statement_a[:50]}...' and '{statement_b[:50]}...'.")
        if statement_a not in self.graph or statement_b not in self.graph:
            logger.warning("One or both statements not found in the graph.")
            print("⚠ One or both statements not found in the graph.")
            return []

        try:
            # Find all shortest paths (could be multiple with the same minimum weight)
            # Using all_shortest_paths for potentially multiple paths
            paths = list(shortest_path(self.graph, source=statement_a, target=statement_b, weight=weight))
            logger.info(f"Found {len(paths)} shortest paths.")
            return paths
        except nx.NetworkXNoPath:
            logger.info("No path found between the two statements.")
            print("✓ No path found between the two statements.")
            return []
        except Exception as e:
            logger.error(f"Error finding paths: {e}")
            print(f"⚠ Error finding paths: {e}")
            return []


    # ------------------------------------------------------
    def get_worst_offenders(self, top_k: int = 5) -> List[Tuple[str, int]]:
        """Find statements with the most contradictions (highest degree)."""
        logger.info(f"Finding top {top_k} worst offenders (by degree).")
        degrees = dict(self.graph.degree())
        sorted_nodes = sorted(degrees.items(), key=lambda x: x[1], reverse=True)
        worst = sorted_nodes[:top_k]
        logger.info(f"Found {len(worst)} worst offenders.")
        return worst

    # ------------------------------------------------------
    def export_bundle(self) -> Dict[str, Any]:
        """Export the current graph as a contradiction bundle for the ledger."""
        logger.info("Exporting graph bundle.")
        edges = [
            {
                "s_i": u,
                "s_j": v,
                "score": d["weight"],
                "similarity": d["sim"],
                "breakdown": d["metrics"]
            }
            for u, v, d in self.graph.edges(data=True)
        ]

        clusters = self.detect_clusters()
        worst_offenders = self.get_worst_offenders()
        communities = self.detect_communities()
        centrality = self.calculate_centrality()

        bundle = {
            "edges": edges,
            "clusters": clusters,
            "communities": communities,
            "worst_offenders": [{"text": text[:100], "contradiction_count": count}
                               for text, count in worst_offenders],
            "centrality_scores": centrality,
            "stats": {
                "total_statements": self.graph.number_of_nodes(),
                "total_contradictions": self.graph.number_of_edges(),
                "cluster_count": len(clusters),
                "community_count": len(communities)
            },
            "timestamp": datetime.datetime.utcnow().isoformat(),
        }

        bundle_json = json.dumps(bundle, sort_keys=True)
        bundle_hash = hashlib.sha256(bundle_json.encode()).hexdigest()
        bundle["hash"] = "sha256:" + bundle_hash

        logger.info(f"Graph bundle exported with hash: {bundle['hash'][:8]}")
        return {"contradiction_bundle": bundle}

    # ------------------------------------------------------
    def anchor_to_ledger(self) -> Dict[str, Any]:
        """Anchor the contradiction bundle in the Tessrax ContradictionLedger."""
        logger.info("Anchoring bundle to ledger.")
        bundle = self.export_bundle()["contradiction_bundle"]

        receipt = self.ledger.record_contradiction(
            claim_a={"text": "Graph bundle", "id": f"bundle_{bundle['hash'][:8]}"},
            claim_b={"text": f"{bundle['stats']['total_contradictions']} conflicts", "id": "edges"},
            resolution=f"Graph with {bundle['stats']['cluster_count']} contradiction clusters and {bundle['stats']['community_count']} communities",
            metadata=bundle,
        )

        logger.info(f"Bundle anchored. Receipt: {receipt.get('status', 'N/A')}")
        return receipt

    # ------------------------------------------------------
    def summary(self, verbose: bool = True):
        """Print summary stats, including communities and centrality."""
        if not verbose:
            return

        logger.info("Generating summary.")
        print(f"\n{'='*60}")
        print(f"CONFLICT GRAPH SUMMARY")
        print(f"{'='*60}")
        print(f"Nodes (statements): {self.graph.number_of_nodes()}")
        print(f"Edges (contradictions): {self.graph.number_of_edges()}")
        print(f"Contradiction threshold: {self.threshold}")

        if self.graph.number_of_nodes() == 0:
             print("\n✓ No statements in the graph.")
             logger.info("No statements in the graph.")
             return

        if self.graph.number_of_edges() == 0:
            print("\n✓ No contradictions detected above threshold")
            logger.info("No contradictions detected above threshold.")
        else:
            print(f"\n--- Detected Contradictions ---")
            for u, v, d in self.graph.edges(data=True):
                print(f"\n  Score: {d['weight']:.3f} | Similarity: {d['sim']:.3f}")
                print(f"  A: {u[:80]}...")
                print(f"  B: {v[:80]}...")
                print(f"  Metrics: {d['metrics']}")
                logger.debug(f"Contradiction: Score={d['weight']:.3f}, A='{u[:50]}...', B='{v[:50]}...'")


        clusters = self.detect_clusters()
        if clusters:
            print(f"\n--- Contradiction Clusters (Connected Components - {len(clusters)}) ---")
            for i, cluster in enumerate(clusters, 1):
                print(f"\n  Cluster {i}: {len(cluster)} statements")
                for stmt in cluster:
                    print(f"    • {stmt[:70]}...")
                logger.debug(f"Cluster {i}: {len(cluster)} statements")

        communities = self.detect_communities()
        if communities:
            print(f"\n--- Detected Communities ({len(communities)}) ---")
            # Sort communities by size (largest first) for easier reading
            sorted_communities = sorted(communities, key=len, reverse=True)
            for i, comm in enumerate(sorted_communities, 1):
                print(f"\n  Community {i}: {len(comm)} statements")
                # Print first few statements from the community
                for stmt in comm[:5]: # Print only up to the first 5 statements
                     print(f"    • {stmt[:70]}...")
                if len(comm) > 5:
                    print("    ...")
                logger.debug(f"Community {i}: {len(comm)} statements")


        centrality = self.calculate_centrality()
        if centrality:
            print(f"\n--- Node Centrality ---")
            # Sort nodes by degree centrality for display
            sorted_by_degree = sorted(centrality.items(), key=lambda item: item[1]['degree'], reverse=True)
            print("\n  Top 5 by Degree Centrality:")
            for node, scores in sorted_by_degree[:5]:
                 print(f"    • {node[:70]}... : Degree={scores['degree']:.3f}, Betweenness={scores['betweenness']:.3f}")
            logger.debug(f"Top 5 by Degree: {sorted_by_degree[:5]}")

            # Sort nodes by betweenness centrality for display
            sorted_by_betweenness = sorted(centrality.items(), key=lambda item: item[1]['betweenness'], reverse=True)
            print("\n  Top 5 by Betweenness Centrality:")
            for node, scores in sorted_by_betweenness[:5]:
                 print(f"    • {node[:70]}... : Degree={scores['degree']:.3f}, Betweenness={scores['betweenness']:.3f}")
            logger.debug(f"Top 5 by Betweenness: {sorted_by_betweenness[:5]}")


        worst = self.get_worst_offenders(3)
        if worst:
            print(f"\n--- Most Contradictory Statements (by Degree) ---")
            for stmt, count in worst:
                print(f"  • {count} contradictions: {stmt[:70]}...")
            logger.debug(f"Top 3 worst offenders: {worst}")


    # ------------------------------------------------------
    def visualize(self, output_path: Optional[Path] = None):
        """
        Generate a visual representation of the contradiction graph.
        Requires matplotlib. Saves to file if output_path provided.
        """
        logger.info("Generating graph visualization (matplotlib).")
        try:
            import matplotlib.pyplot as plt

            pos = nx.spring_layout(self.graph, k=2, iterations=50)

            plt.figure(figsize=(14, 10))

            # Draw nodes
            nx.draw_networkx_nodes(self.graph, pos, node_size=700,
                                  node_color='lightblue', alpha=0.9)

            # Draw edges with width proportional to contradiction score
            edges = self.graph.edges()
            weights = [self.graph[u][v]['weight'] * 3 for u, v in edges]
            nx.draw_networkx_edges(self.graph, pos, width=weights,
                                  alpha=0.6, edge_color='red')

            # Draw labels (truncated)
            labels = {node: node[:30] + "..." if len(node) > 30 else node
                     for node in self.graph.nodes()}
            nx.draw_networkx_labels(self.graph, pos, labels, font_size=8)

            plt.title("Tessrax Contradiction Graph (matplotlib)", fontsize=16, fontweight='bold')
            plt.axis('off')
            plt.tight_layout()

            if output_path:
                # Ensure the directory exists
                output_path.parent.mkdir(parents=True, exist_ok=True)
                plt.savefig(output_path, dpi=300, bbox_inches='tight')
                logger.info(f"Matplotlib graph visualization saved to {output_path}")
                print(f"\n✓ Matplotlib graph visualization saved to {output_path}")
            else:
                plt.show()
                logger.info("Matplotlib graph visualization displayed.")

        except ImportError:
            print("⚠ matplotlib not installed. Skipping matplotlib visualization.")
            logger.warning("Matplotlib not installed. Skipping matplotlib visualization.")
        except Exception as e:
            logger.error(f"Error during matplotlib visualization: {e}")
            print(f"⚠ Error during matplotlib visualization: {e}")

    # ------------------------------------------------------
    def visualize_interactive(self, output_path: Optional[Path] = None):
        """
        Generate an interactive visual representation of the contradiction graph using Plotly.
        Displays node and edge attributes on hover. Saves to HTML if output_path provided.
        """
        logger.info("Generating interactive graph visualization (Plotly).")
        if not self.graph.number_of_nodes():
            logger.info("No nodes in graph, skipping interactive visualization.")
            print("✓ No statements in the graph to visualize interactively.")
            return

        try:
            # Use spring_layout for node positions
            pos = nx.spring_layout(self.graph, k=2, iterations=50)

            # Create Edge Trace
            edge_x = []
            edge_y = []
            edge_hover_text = []
            for edge in self.graph.edges(data=True):
                x0, y0 = pos[edge[0]]
                x1, y1 = pos[edge[1]]
                edge_x.extend([x0, x1, None])
                edge_y.extend([y0, y1, None])
                # Edge hover text
                u, v, data = edge
                hover_text = f"<b>Statements:</b><br>{u[:100]}...<br>{v[:100]}...<br>" \
                             f"<b>Contradiction Score:</b> {data['weight']:.3f}<br>" \
                             f"<b>Similarity:</b> {data['sim']:.3f}<br>" \
                             f"<b>Metrics:</b> L={data['metrics']['L']:.2f}, N={data['metrics']['N']:.2f}, T={data['metrics']['T']:.2f}, C={data['metrics']['C']:.2f}"
                edge_hover_text.append(hover_text) # Add text for the first point of the edge, it will show on hover

            edge_trace = go.Scattergl(
                x=edge_x, y=edge_y,
                line=dict(width=1.5, color='#888'),
                hoverinfo='text',
                text=edge_hover_text,
                mode='lines')

            # Create Node Trace
            node_x = []
            node_y = []
            node_text = [] # Labels on the graph
            node_hover_text = [] # Detailed info on hover
            node_size = [] # Node size based on degree
            node_color = [] # Node color based on sentiment or community

            centrality = self.calculate_centrality() # Calculate centrality for node size
            communities = self.detect_communities() # Detect communities for node color

            # Assign color based on community (simple example)
            # Create a mapping from node to community index
            node_community_map = {}
            for i, comm in enumerate(communities):
                for node in comm:
                    node_community_map[node] = i

            # Assign color based on sentiment if no communities or not in a community
            sentiment_colors = {'POSITIVE': 'green', 'NEGATIVE': 'red', 'NEUTRAL': 'gray'}
            default_color = 'lightblue'


            for node in self.graph.nodes():
                x, y = pos[node]
                node_x.append(x)
                node_y.append(y)

                # Node label (truncated statement text)
                node_text.append(node[:30] + "..." if len(node) > 30 else node)

                # Node hover text (detailed attributes)
                attrs = self.graph.nodes[node]
                hover_info = f"<b>Statement:</b> {node}<br>"
                hover_info += f"<b>Source:</b> {attrs.get('source', 'N/A')}<br>"
                hover_info += f"<b>Timestamp:</b> {attrs.get('timestamp', 'N/A')}<br>"
                hover_info += f"<b>Confidence:</b> {attrs.get('confidence', 'N/A'):.3f}<br>"

                sentiment_data = attrs.get('sentiment')
                if sentiment_data:
                    hover_info += f"<b>Sentiment:</b> {sentiment_data['label']} ({sentiment_data['score']:.3f})<br>"

                ner_data = attrs.get('named_entities')
                if ner_data:
                    hover_info += "<b>Named Entities:</b><br>" + "<br>".join([f"- {ent['word']} ({ent['entity_group']})" for ent in ner_data]) + "<br>"

                centrality_data = centrality.get(node)
                if centrality_data:
                     hover_info += f"<b>Centrality:</b> Degree={centrality_data['degree']:.3f}, Betweenness={centrality_data['betweenness']:.3f}<br>"

                # Add community info to hover text
                comm_index = node_community_map.get(node)
                if comm_index is not None:
                    hover_info += f"<b>Community:</b> {comm_index + 1}<br>"


                node_hover_text.append(hover_info)

                # Node size based on degree centrality (scaled)
                size = 5 + centrality.get(node, {}).get('degree', 0) * 50 # Base size 5 + scaled degree
                node_size.append(size)

                # Node color based on community or sentiment
                if comm_index is not None:
                     # Use a predefined or generated color scale for communities
                     color = pio.colors.sequential.Plasma[comm_index % len(pio.colors.sequential.Plasma)]
                elif sentiment_data and sentiment_data['label'] in sentiment_colors:
                    color = sentiment_colors[sentiment_data['label']]
                else:
                    color = default_color
                node_color.append(color)


            node_trace = go.Scattergl(
                x=node_x, y=node_y,
                mode='markers+text',
                hoverinfo='text',
                text=node_text,
                textposition='bottom center',
                textfont=dict(size=8),
                marker=dict(
                    showscale=False,
                    colorscale='YlGnBu', # Not strictly needed if using custom colors, but kept for scale definition
                    reversescale=True,
                    color=node_color,
                    size=node_size,
                    opacity=0.9,
                    line_width=1))

            # Add hover text separately for detailed info on hover
            node_trace.text = node_hover_text


            # Create Figure
            fig = go.Figure(data=[edge_trace, node_trace],
                            layout=go.Layout(
                                title='Tessrax Contradiction Graph (Interactive)',
                                titlefont_size=16,
                                showlegend=False,
                                hovermode='closest',
                                margin=dict(b=20,l=5,r=5,t=40),
                                annotations=[ dict(
                                    text="Interactive Contradiction Graph powered by Plotly",
                                    showarrow=False,
                                    xref="paper", yref="paper",
                                    x=0.005, y=-0.002 ) ],
                                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))
                           )

            if output_path:
                # Ensure the directory exists
                output_path.parent.mkdir(parents=True, exist_ok=True)
                pio.write_html(fig, str(output_path), auto_open=False)
                logger.info(f"Interactive graph visualization saved to {output_path}")
                print(f"\n✓ Interactive graph visualization saved to {output_path}")
            else:
                fig.show()
                logger.info("Interactive graph visualization displayed.")

        except ImportError:
            print("⚠ plotly or plotly.io not installed. Skipping interactive visualization.")
            logger.warning("Plotly or plotly.io not installed. Skipping interactive visualization.")
        except Exception as e:
            logger.error(f"Error during interactive visualization: {e}")
            print(f"⚠ Error during interactive visualization: {e}")


# ----------------------------------------------------------
# Demo Harness (Modified to use visualize_interactive)
# ----------------------------------------------------------
if __name__ == "__main__":
    logger.info("Starting demo harness.")
    print("Tessrax Conflict Graph - Demo Harness")
    print("="*60)

    sample_texts = [
        {"text": "In 2020, Acme Corp pledged to cut CO2 emissions 50% by 2030.",
         "source": "Press Release", "timestamp": "2020-01-15"},
        {"text": "In 2024, Acme Corp reported emissions down only 5%.",
         "source": "Annual Report", "timestamp": "2024-03-01"},
        {"text": "Acme Corp achieved record revenue growth of 40% in 2024.",
         "source": "Investor Call", "timestamp": "2024-04-10"},
        {"text": "Acme Corp will eliminate all carbon emissions by 2035.",
         "source": "CEO Interview", "timestamp": "2024-06-01"},
        {"text": "Acme Corp has not reduced carbon emissions significantly.",
         "source": "Third-party Audit", "timestamp": "2024-07-15"},
        {"text": "We completed 90% of the project by December 2022.", # New statement for temporal test
         "source": "Project Report", "timestamp": "2023-01-01"},
        {"text": "The project is only 10% complete as of January 2023.", # New statement for temporal and numeric test
         "source": "Project Update", "timestamp": "2023-01-05"},
         {"text": "Our profit was 10 million dollars in Q1.", # New statement for numeric test
         "source": "Financial Report", "timestamp": "2024-05-01"},
        {"text": "The company made 10,000,000 USD in the first quarter.", # New statement for numeric test (different units/format)
         "source": "Earnings Call", "timestamp": "2024-05-02"},

    ]

    # Initialize ConflictGraph without arguments, it uses CONFIG
    cg = ConflictGraph()
    cg.add_statements(sample_texts)
    cg.compute_edges(verbose=True)
    cg.summary()

    print("\n" + "="*60)
    print("ANCHORING TO LEDGER")
    print("="*60)

    receipt = cg.anchor_to_ledger()
    print("\n--- Ledger Receipt ---")
    print(json.dumps(receipt, indent=2))

    # Demo path finding
    stmt1 = sample_texts[0]["text"] # "In 2020, Acme Corp pledged to cut CO2 emissions 50% by 2030."
    stmt2 = sample_texts[4]["text"] # "Acme Corp has not reduced carbon emissions significantly."
    print(f"\n--- Finding paths between statements ---")
    print(f"Statement A: {stmt1[:70]}...")
    print(f"Statement B: {stmt2[:70]}...")
    paths = cg.find_contradiction_paths(stmt1, stmt2)
    if paths:
        print("\nFound paths:")
        for i, path in enumerate(paths, 1):
            print(f"  Path {i}:")
            for j, node in enumerate(path):
                print(f"    {j+1}. {node[:70]}...")
    else:
        print("\nNo path found between these statements.")


    # Visualize using the new interactive method
    try:
        cg.visualize_interactive(output_path=Path("data/conflict_graph_demo_interactive.html"))
        # Optionally, still visualize with matplotlib for comparison
        # cg.visualize(output_path=Path("data/conflict_graph_demo_matplotlib.png"))

    except Exception as e:
        logger.error(f"Visualization failed: {e}")
        pass # Handle the error gracefully

    cg.ledger.close()
    logger.info("Demo harness finished.")
    print("\n✓ Demo complete")
